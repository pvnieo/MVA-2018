{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attaiki_Souhaib.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ig-OSYUENd",
        "colab_type": "text"
      },
      "source": [
        "# Small data and deep learning\n",
        "This mini-project proposes to study several techniques for improving challenging context, in which few data and resources are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJiGeuujUENf",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
        "\n",
        "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   XXX  | XXX | XXX | XXX |\n",
        "\n",
        "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
        "\n",
        "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
        "\n",
        "__The total file sizes should not exceed 2MB. Please name your notebook (LASTNAME)\\_(FIRSTNAME).ipynb, zip/tar it with any necessary files required to run your notebook, in a compressed file named (LASTNAME)\\_(FIRSTNAME).X where X is the corresponding extension. Zip/tar files exceeding 2MB will not be considered for grading. Submit the compressed file via the submission link provided on the website of the class.__\n",
        "\n",
        "You can use https://colab.research.google.com/ to run your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAXqUYlZB_eq",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WCmrQ90gEM",
        "colab_type": "code",
        "outputId": "8d503e9f-76a3-4c76-ffe6-b320d1d87b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip uninstall scikit-cuda\n",
        "!rm -r scikit_cuda\n",
        "!rm -r scikit-cuda\n",
        "!git clone https://github.com/lebedov/scikit-cuda.git\n",
        "!mv scikit-cuda scikit_cuda\n",
        "%cd scikit_cuda\n",
        "!python setup.py install\n",
        "from scikit_cuda.skcuda import cublas, cufft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mSkipping scikit-cuda as it is not installed.\u001b[0m\n",
            "Cloning into 'scikit-cuda'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 5708 (delta 12), reused 16 (delta 6), pack-reused 5678\u001b[K\n",
            "Receiving objects: 100% (5708/5708), 2.40 MiB | 21.05 MiB/s, done.\n",
            "Resolving deltas: 100% (3404/3404), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4wPrbBtUfQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/edouardoyallon/pyscatwave.git\n",
        "from pyscatwave.scatwave.scattering import Scattering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA3NyisIB7tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import os.path\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models.resnet import *\n",
        "from torchvision.models.vgg import *\n",
        "from sklearn import svm\n",
        "\n",
        "device = \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4_p-ybHffdw",
        "colab_type": "code",
        "outputId": "75e4c46b-f9e2-49ff-d378-24adec6b4ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = \"drive/My Drive/Colab Notebooks/discriminator.pt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeLm2kEWUENh",
        "colab_type": "text"
      },
      "source": [
        "## Disclaimer\n",
        "\n",
        "* It may be that the results I give in the table are a little different with the cell outputs. This is because the output changes when I change the hyperparameters, and the results obtained are worse than the previous ones, and when I change the model (for example, Resnet 18 and 50), the results of the previous model are lost.\n",
        "* I could not send the pre-trained discriminator for the \"DCGan features\" question, because its size is very large (40 MB), but the link for the downloaded one is as follows: https://github.com/donand/GAN_pytorch/blob/master/DCGAN/results_celeba/checkpoint_ep10/discriminator.pt\n",
        "\n",
        "## Training set creation\n",
        "__Question 1:__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. \n",
        "\n",
        "__Answer__: To obtain a dataloader with only 100 samples, we simply take only 100 samples in the attributes `self.data` of the CIFAR10 class, after shuffling both the data and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFipVgKCUENi",
        "colab_type": "code",
        "outputId": "c4690212-dbc1-42d1-d5f5-6b66fb0c2fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./root', train=True,\n",
        "                                       download=True)\n",
        "p = np.random.permutation(len(testset.data))\n",
        "testset.data = testset.data[p][:100,:,:,:]\n",
        "testset.targets = np.array(testset.targets)[p][:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR75zvEtUENn",
        "colab_type": "text"
      },
      "source": [
        "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRZNHtj_UENo",
        "colab_type": "text"
      },
      "source": [
        "## Testing procedure\n",
        "__Question 2:__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OG8hcy5UENp",
        "colab_type": "text"
      },
      "source": [
        "__Answer__: The problems that can be encountered are that when using a small dataset, the network can easily overfit and therefore will not have a great power of generalization. To remedy this, we can show the network different images during training using data augumentation, use transfer learning, and in this case, we won't need many images to train the network, and finally, we can use weak supervision, to benefit from our unlabeled part of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STEMUkZrUENq",
        "colab_type": "text"
      },
      "source": [
        "# Raw approach: the baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIu4OmxjUENr",
        "colab_type": "text"
      },
      "source": [
        "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performances with reported number from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
        "\n",
        "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
        "\n",
        "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context, as those researchers had access to GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GREBuZ28UENs",
        "colab_type": "text"
      },
      "source": [
        "## ResNet architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2p39A2wUENs",
        "colab_type": "text"
      },
      "source": [
        "__Question 3:__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1409.1556 ). If possible, please report the accuracy obtained on the whole dataset, as well as the reference paper/GitHub link you might have used.\n",
        "\n",
        "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (~5 minutes).\n",
        "\n",
        "__Answer__: I tried resnet18 and resnet50 by resing the code provided. I also used data augumentation (random crop, random flip), and learning rate decay (i've done this manually) as regularizers for the network. After cross validation, I find:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on all the dataset | source\n",
        "|------|------|------|------|------|------|\n",
        "|   Resnet18  | 60 | 83 % | 27 %|93.02 % | https://github.com/kuangliu/pytorch-cifar|\n",
        "|   Resnet50  | 60 | 22 % | 14 %|93.62 % | https://github.com/kuangliu/pytorch-cifar|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMkiaLQqUENt",
        "colab_type": "code",
        "outputId": "45991d9b-3982-4544-8ea5-1720a23f59e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62693
        }
      },
      "source": [
        "# hyperparameters\n",
        "lr = 0.01\n",
        "batch_size = 10\n",
        "\n",
        "# Data augumentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# Load train\n",
        "trainset = torchvision.datasets.CIFAR10(root='./root', train=True, download=True, transform=transform_train)\n",
        "# Keep 100 examples\n",
        "p = np.random.permutation(len(trainset.data))\n",
        "trainset.data = trainset.data[p][:100,:,:,:]\n",
        "trainset.targets = np.array(trainset.targets)[p][:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load test\n",
        "testset = torchvision.datasets.CIFAR10(root='./root', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Load architecture: Resnet 18 / 50\n",
        "net = resnet50()\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Train network\n",
        "def train(epoch):\n",
        "  \n",
        "    print('\\n===Train===\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print( 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n===Test===\\nEpoch: %d' % epoch)\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "start_epoch = 0\n",
        "epochs = 30\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "===Train===\n",
            "Epoch: 0\n",
            "Loss: 7.160 | Acc: 0.000% (0/10)\n",
            "Loss: 5.916 | Acc: 5.000% (1/20)\n",
            "Loss: 5.732 | Acc: 10.000% (3/30)\n",
            "Loss: 5.327 | Acc: 10.000% (4/40)\n",
            "Loss: 6.158 | Acc: 10.000% (5/50)\n",
            "Loss: 7.854 | Acc: 10.000% (6/60)\n",
            "Loss: 8.463 | Acc: 8.571% (6/70)\n",
            "Loss: 8.969 | Acc: 8.750% (7/80)\n",
            "Loss: 8.976 | Acc: 10.000% (9/90)\n",
            "Loss: 8.962 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 0\n",
            "Loss: 7333762.500 | Acc: 11.000% (11/100)\n",
            "Loss: 7701061.500 | Acc: 10.500% (21/200)\n",
            "Loss: 7546623.000 | Acc: 11.667% (35/300)\n",
            "Loss: 7555053.750 | Acc: 11.250% (45/400)\n",
            "Loss: 7508350.300 | Acc: 11.200% (56/500)\n",
            "Loss: 7299439.583 | Acc: 11.833% (71/600)\n",
            "Loss: 7270857.714 | Acc: 11.857% (83/700)\n",
            "Loss: 7273020.188 | Acc: 11.500% (92/800)\n",
            "Loss: 7341365.778 | Acc: 10.778% (97/900)\n",
            "Loss: 7272753.000 | Acc: 10.900% (109/1000)\n",
            "Loss: 7270631.273 | Acc: 10.455% (115/1100)\n",
            "Loss: 7322141.708 | Acc: 10.333% (124/1200)\n",
            "Loss: 7371650.346 | Acc: 10.462% (136/1300)\n",
            "Loss: 7385498.536 | Acc: 10.500% (147/1400)\n",
            "Loss: 7407920.700 | Acc: 10.467% (157/1500)\n",
            "Loss: 7403265.094 | Acc: 9.938% (159/1600)\n",
            "Loss: 7369561.059 | Acc: 10.176% (173/1700)\n",
            "Loss: 7382950.306 | Acc: 9.944% (179/1800)\n",
            "Loss: 7392363.105 | Acc: 10.158% (193/1900)\n",
            "Loss: 7399218.400 | Acc: 10.150% (203/2000)\n",
            "Loss: 7414594.952 | Acc: 9.952% (209/2100)\n",
            "Loss: 7402831.000 | Acc: 10.227% (225/2200)\n",
            "Loss: 7385946.043 | Acc: 10.261% (236/2300)\n",
            "Loss: 7412663.917 | Acc: 10.125% (243/2400)\n",
            "Loss: 7363993.520 | Acc: 10.320% (258/2500)\n",
            "Loss: 7380562.904 | Acc: 10.269% (267/2600)\n",
            "Loss: 7402311.778 | Acc: 10.000% (270/2700)\n",
            "Loss: 7412854.214 | Acc: 10.036% (281/2800)\n",
            "Loss: 7387990.621 | Acc: 10.069% (292/2900)\n",
            "Loss: 7407789.700 | Acc: 10.100% (303/3000)\n",
            "Loss: 7396974.710 | Acc: 9.968% (309/3100)\n",
            "Loss: 7391441.766 | Acc: 10.219% (327/3200)\n",
            "Loss: 7375922.894 | Acc: 10.242% (338/3300)\n",
            "Loss: 7376897.588 | Acc: 10.294% (350/3400)\n",
            "Loss: 7388511.529 | Acc: 10.200% (357/3500)\n",
            "Loss: 7402889.958 | Acc: 10.167% (366/3600)\n",
            "Loss: 7402675.986 | Acc: 10.189% (377/3700)\n",
            "Loss: 7421332.289 | Acc: 10.211% (388/3800)\n",
            "Loss: 7418226.231 | Acc: 10.282% (401/3900)\n",
            "Loss: 7408852.638 | Acc: 10.375% (415/4000)\n",
            "Loss: 7397435.927 | Acc: 10.268% (421/4100)\n",
            "Loss: 7404847.774 | Acc: 10.119% (425/4200)\n",
            "Loss: 7394940.814 | Acc: 10.163% (437/4300)\n",
            "Loss: 7402279.420 | Acc: 10.159% (447/4400)\n",
            "Loss: 7419662.467 | Acc: 10.156% (457/4500)\n",
            "Loss: 7402584.033 | Acc: 10.217% (470/4600)\n",
            "Loss: 7394083.638 | Acc: 10.106% (475/4700)\n",
            "Loss: 7387358.219 | Acc: 10.250% (492/4800)\n",
            "Loss: 7390739.582 | Acc: 10.265% (503/4900)\n",
            "Loss: 7389157.870 | Acc: 10.260% (513/5000)\n",
            "Loss: 7385583.451 | Acc: 10.255% (523/5100)\n",
            "Loss: 7365400.337 | Acc: 10.288% (535/5200)\n",
            "Loss: 7378504.274 | Acc: 10.189% (540/5300)\n",
            "Loss: 7368736.259 | Acc: 10.185% (550/5400)\n",
            "Loss: 7359751.700 | Acc: 10.200% (561/5500)\n",
            "Loss: 7367960.268 | Acc: 10.161% (569/5600)\n",
            "Loss: 7381187.061 | Acc: 10.053% (573/5700)\n",
            "Loss: 7379865.397 | Acc: 10.017% (581/5800)\n",
            "Loss: 7370011.831 | Acc: 9.949% (587/5900)\n",
            "Loss: 7342648.633 | Acc: 10.083% (605/6000)\n",
            "Loss: 7343430.238 | Acc: 9.984% (609/6100)\n",
            "Loss: 7348505.016 | Acc: 10.081% (625/6200)\n",
            "Loss: 7356989.254 | Acc: 10.063% (634/6300)\n",
            "Loss: 7357206.273 | Acc: 10.031% (642/6400)\n",
            "Loss: 7361958.731 | Acc: 9.969% (648/6500)\n",
            "Loss: 7364728.598 | Acc: 9.955% (657/6600)\n",
            "Loss: 7355447.590 | Acc: 9.910% (664/6700)\n",
            "Loss: 7356172.985 | Acc: 9.882% (672/6800)\n",
            "Loss: 7344375.551 | Acc: 9.913% (684/6900)\n",
            "Loss: 7352696.171 | Acc: 9.900% (693/7000)\n",
            "Loss: 7365826.415 | Acc: 9.887% (702/7100)\n",
            "Loss: 7366134.611 | Acc: 9.861% (710/7200)\n",
            "Loss: 7362198.486 | Acc: 9.822% (717/7300)\n",
            "Loss: 7372697.878 | Acc: 9.743% (721/7400)\n",
            "Loss: 7360772.753 | Acc: 9.773% (733/7500)\n",
            "Loss: 7354808.546 | Acc: 9.816% (746/7600)\n",
            "Loss: 7350474.286 | Acc: 9.870% (760/7700)\n",
            "Loss: 7358338.103 | Acc: 9.872% (770/7800)\n",
            "Loss: 7363180.399 | Acc: 9.861% (779/7900)\n",
            "Loss: 7362233.125 | Acc: 9.887% (791/8000)\n",
            "Loss: 7357512.272 | Acc: 9.914% (803/8100)\n",
            "Loss: 7349598.299 | Acc: 9.902% (812/8200)\n",
            "Loss: 7357339.982 | Acc: 9.880% (820/8300)\n",
            "Loss: 7353563.833 | Acc: 9.857% (828/8400)\n",
            "Loss: 7352468.594 | Acc: 9.882% (840/8500)\n",
            "Loss: 7350003.471 | Acc: 9.849% (847/8600)\n",
            "Loss: 7354225.621 | Acc: 9.816% (854/8700)\n",
            "Loss: 7353320.909 | Acc: 9.807% (863/8800)\n",
            "Loss: 7357457.056 | Acc: 9.798% (872/8900)\n",
            "Loss: 7357386.089 | Acc: 9.856% (887/9000)\n",
            "Loss: 7353841.044 | Acc: 9.879% (899/9100)\n",
            "Loss: 7350808.804 | Acc: 9.880% (909/9200)\n",
            "Loss: 7345480.124 | Acc: 9.946% (925/9300)\n",
            "Loss: 7343652.117 | Acc: 9.979% (938/9400)\n",
            "Loss: 7336770.505 | Acc: 9.968% (947/9500)\n",
            "Loss: 7330383.120 | Acc: 10.021% (962/9600)\n",
            "Loss: 7329576.345 | Acc: 10.010% (971/9700)\n",
            "Loss: 7323174.959 | Acc: 10.061% (986/9800)\n",
            "Loss: 7327162.798 | Acc: 10.061% (996/9900)\n",
            "Loss: 7334789.860 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 1\n",
            "Loss: 14.574 | Acc: 10.000% (1/10)\n",
            "Loss: 12.717 | Acc: 5.000% (1/20)\n",
            "Loss: 13.062 | Acc: 3.333% (1/30)\n",
            "Loss: 10.721 | Acc: 7.500% (3/40)\n",
            "Loss: 10.645 | Acc: 14.000% (7/50)\n",
            "Loss: 11.526 | Acc: 16.667% (10/60)\n",
            "Loss: 11.305 | Acc: 15.714% (11/70)\n",
            "Loss: 12.200 | Acc: 15.000% (12/80)\n",
            "Loss: 12.412 | Acc: 16.667% (15/90)\n",
            "Loss: 11.704 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 1\n",
            "Loss: 1383872.000 | Acc: 6.000% (6/100)\n",
            "Loss: 1407945.188 | Acc: 7.000% (14/200)\n",
            "Loss: 1416224.375 | Acc: 8.000% (24/300)\n",
            "Loss: 1461261.969 | Acc: 8.250% (33/400)\n",
            "Loss: 1431720.600 | Acc: 8.200% (41/500)\n",
            "Loss: 1383812.167 | Acc: 7.667% (46/600)\n",
            "Loss: 1370446.482 | Acc: 8.000% (56/700)\n",
            "Loss: 1357850.375 | Acc: 8.250% (66/800)\n",
            "Loss: 1372159.847 | Acc: 8.444% (76/900)\n",
            "Loss: 1358900.938 | Acc: 8.900% (89/1000)\n",
            "Loss: 1363058.011 | Acc: 9.091% (100/1100)\n",
            "Loss: 1364411.448 | Acc: 9.500% (114/1200)\n",
            "Loss: 1365057.385 | Acc: 9.538% (124/1300)\n",
            "Loss: 1366276.875 | Acc: 9.714% (136/1400)\n",
            "Loss: 1365765.158 | Acc: 10.133% (152/1500)\n",
            "Loss: 1356020.383 | Acc: 10.188% (163/1600)\n",
            "Loss: 1350822.699 | Acc: 10.000% (170/1700)\n",
            "Loss: 1349182.903 | Acc: 10.111% (182/1800)\n",
            "Loss: 1347293.684 | Acc: 9.947% (189/1900)\n",
            "Loss: 1349056.206 | Acc: 9.900% (198/2000)\n",
            "Loss: 1346204.054 | Acc: 9.905% (208/2100)\n",
            "Loss: 1350793.097 | Acc: 9.773% (215/2200)\n",
            "Loss: 1346933.375 | Acc: 9.739% (224/2300)\n",
            "Loss: 1352798.130 | Acc: 9.833% (236/2400)\n",
            "Loss: 1350114.390 | Acc: 9.720% (243/2500)\n",
            "Loss: 1355261.928 | Acc: 9.538% (248/2600)\n",
            "Loss: 1354802.639 | Acc: 9.593% (259/2700)\n",
            "Loss: 1355523.509 | Acc: 9.571% (268/2800)\n",
            "Loss: 1349591.836 | Acc: 9.517% (276/2900)\n",
            "Loss: 1352664.958 | Acc: 9.567% (287/3000)\n",
            "Loss: 1344714.831 | Acc: 9.742% (302/3100)\n",
            "Loss: 1343949.770 | Acc: 9.750% (312/3200)\n",
            "Loss: 1337429.795 | Acc: 9.667% (319/3300)\n",
            "Loss: 1341854.533 | Acc: 9.647% (328/3400)\n",
            "Loss: 1351298.154 | Acc: 9.571% (335/3500)\n",
            "Loss: 1351035.003 | Acc: 9.667% (348/3600)\n",
            "Loss: 1345466.071 | Acc: 9.730% (360/3700)\n",
            "Loss: 1351739.135 | Acc: 9.763% (371/3800)\n",
            "Loss: 1351502.689 | Acc: 9.769% (381/3900)\n",
            "Loss: 1351931.584 | Acc: 9.800% (392/4000)\n",
            "Loss: 1348337.482 | Acc: 9.756% (400/4100)\n",
            "Loss: 1352337.351 | Acc: 9.762% (410/4200)\n",
            "Loss: 1346889.727 | Acc: 9.884% (425/4300)\n",
            "Loss: 1348366.480 | Acc: 9.886% (435/4400)\n",
            "Loss: 1351332.867 | Acc: 9.911% (446/4500)\n",
            "Loss: 1349482.264 | Acc: 9.891% (455/4600)\n",
            "Loss: 1345408.168 | Acc: 9.894% (465/4700)\n",
            "Loss: 1347373.583 | Acc: 9.917% (476/4800)\n",
            "Loss: 1346778.865 | Acc: 10.020% (491/4900)\n",
            "Loss: 1343639.390 | Acc: 10.100% (505/5000)\n",
            "Loss: 1343742.294 | Acc: 10.098% (515/5100)\n",
            "Loss: 1339395.050 | Acc: 10.038% (522/5200)\n",
            "Loss: 1341964.285 | Acc: 10.057% (533/5300)\n",
            "Loss: 1339646.625 | Acc: 10.167% (549/5400)\n",
            "Loss: 1340235.832 | Acc: 10.182% (560/5500)\n",
            "Loss: 1342382.663 | Acc: 10.089% (565/5600)\n",
            "Loss: 1343440.189 | Acc: 10.035% (572/5700)\n",
            "Loss: 1344391.416 | Acc: 9.966% (578/5800)\n",
            "Loss: 1341816.653 | Acc: 9.932% (586/5900)\n",
            "Loss: 1336800.146 | Acc: 9.883% (593/6000)\n",
            "Loss: 1338076.070 | Acc: 9.836% (600/6100)\n",
            "Loss: 1338176.480 | Acc: 9.903% (614/6200)\n",
            "Loss: 1339560.925 | Acc: 9.841% (620/6300)\n",
            "Loss: 1342410.568 | Acc: 9.781% (626/6400)\n",
            "Loss: 1344467.829 | Acc: 9.846% (640/6500)\n",
            "Loss: 1345619.407 | Acc: 9.848% (650/6600)\n",
            "Loss: 1346068.539 | Acc: 9.806% (657/6700)\n",
            "Loss: 1343066.638 | Acc: 9.809% (667/6800)\n",
            "Loss: 1341945.716 | Acc: 9.870% (681/6900)\n",
            "Loss: 1340347.236 | Acc: 9.843% (689/7000)\n",
            "Loss: 1341104.789 | Acc: 9.873% (701/7100)\n",
            "Loss: 1344046.118 | Acc: 9.875% (711/7200)\n",
            "Loss: 1340447.767 | Acc: 9.904% (723/7300)\n",
            "Loss: 1340501.796 | Acc: 9.905% (733/7400)\n",
            "Loss: 1338022.780 | Acc: 9.920% (744/7500)\n",
            "Loss: 1338309.903 | Acc: 9.947% (756/7600)\n",
            "Loss: 1340365.153 | Acc: 9.922% (764/7700)\n",
            "Loss: 1339762.944 | Acc: 9.885% (771/7800)\n",
            "Loss: 1342128.658 | Acc: 9.911% (783/7900)\n",
            "Loss: 1343660.355 | Acc: 9.963% (797/8000)\n",
            "Loss: 1344006.008 | Acc: 10.012% (811/8100)\n",
            "Loss: 1339336.806 | Acc: 9.988% (819/8200)\n",
            "Loss: 1336652.187 | Acc: 10.000% (830/8300)\n",
            "Loss: 1334342.284 | Acc: 10.024% (842/8400)\n",
            "Loss: 1334205.721 | Acc: 10.035% (853/8500)\n",
            "Loss: 1334783.231 | Acc: 10.116% (870/8600)\n",
            "Loss: 1334669.668 | Acc: 10.126% (881/8700)\n",
            "Loss: 1335250.338 | Acc: 10.114% (890/8800)\n",
            "Loss: 1337514.902 | Acc: 10.079% (897/8900)\n",
            "Loss: 1339310.258 | Acc: 10.044% (904/9000)\n",
            "Loss: 1341335.460 | Acc: 10.000% (910/9100)\n",
            "Loss: 1338923.689 | Acc: 10.011% (921/9200)\n",
            "Loss: 1337998.656 | Acc: 9.989% (929/9300)\n",
            "Loss: 1336923.061 | Acc: 9.979% (938/9400)\n",
            "Loss: 1337430.301 | Acc: 9.958% (946/9500)\n",
            "Loss: 1337638.285 | Acc: 9.969% (957/9600)\n",
            "Loss: 1337861.585 | Acc: 10.000% (970/9700)\n",
            "Loss: 1338379.217 | Acc: 9.969% (977/9800)\n",
            "Loss: 1339832.864 | Acc: 10.010% (991/9900)\n",
            "Loss: 1341576.570 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 2\n",
            "Loss: 15.943 | Acc: 0.000% (0/10)\n",
            "Loss: 13.022 | Acc: 5.000% (1/20)\n",
            "Loss: 15.574 | Acc: 13.333% (4/30)\n",
            "Loss: 14.411 | Acc: 12.500% (5/40)\n",
            "Loss: 13.595 | Acc: 10.000% (5/50)\n",
            "Loss: 17.528 | Acc: 11.667% (7/60)\n",
            "Loss: 15.874 | Acc: 11.429% (8/70)\n",
            "Loss: 16.253 | Acc: 13.750% (11/80)\n",
            "Loss: 14.895 | Acc: 16.667% (15/90)\n",
            "Loss: 14.670 | Acc: 17.000% (17/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 2\n",
            "Loss: 395405.906 | Acc: 10.000% (10/100)\n",
            "Loss: 395005.375 | Acc: 10.000% (20/200)\n",
            "Loss: 394228.625 | Acc: 12.000% (36/300)\n",
            "Loss: 388187.289 | Acc: 11.250% (45/400)\n",
            "Loss: 380103.181 | Acc: 11.600% (58/500)\n",
            "Loss: 380019.365 | Acc: 11.167% (67/600)\n",
            "Loss: 377630.768 | Acc: 10.571% (74/700)\n",
            "Loss: 372571.996 | Acc: 10.125% (81/800)\n",
            "Loss: 369771.854 | Acc: 10.333% (93/900)\n",
            "Loss: 367734.106 | Acc: 10.400% (104/1000)\n",
            "Loss: 366295.034 | Acc: 11.000% (121/1100)\n",
            "Loss: 364524.398 | Acc: 11.000% (132/1200)\n",
            "Loss: 371234.618 | Acc: 10.615% (138/1300)\n",
            "Loss: 371353.127 | Acc: 10.357% (145/1400)\n",
            "Loss: 374560.335 | Acc: 10.133% (152/1500)\n",
            "Loss: 372805.227 | Acc: 9.938% (159/1600)\n",
            "Loss: 373689.934 | Acc: 9.765% (166/1700)\n",
            "Loss: 372124.722 | Acc: 10.111% (182/1800)\n",
            "Loss: 374324.826 | Acc: 9.947% (189/1900)\n",
            "Loss: 376350.925 | Acc: 9.900% (198/2000)\n",
            "Loss: 375903.528 | Acc: 9.905% (208/2100)\n",
            "Loss: 375793.121 | Acc: 10.000% (220/2200)\n",
            "Loss: 374907.810 | Acc: 10.000% (230/2300)\n",
            "Loss: 374876.509 | Acc: 10.083% (242/2400)\n",
            "Loss: 374630.031 | Acc: 10.080% (252/2500)\n",
            "Loss: 374524.663 | Acc: 10.000% (260/2600)\n",
            "Loss: 372083.757 | Acc: 10.074% (272/2700)\n",
            "Loss: 373886.443 | Acc: 10.071% (282/2800)\n",
            "Loss: 373649.055 | Acc: 10.000% (290/2900)\n",
            "Loss: 374587.963 | Acc: 10.067% (302/3000)\n",
            "Loss: 373968.725 | Acc: 10.032% (311/3100)\n",
            "Loss: 374075.034 | Acc: 10.094% (323/3200)\n",
            "Loss: 374953.566 | Acc: 10.061% (332/3300)\n",
            "Loss: 376053.407 | Acc: 10.000% (340/3400)\n",
            "Loss: 376974.944 | Acc: 10.171% (356/3500)\n",
            "Loss: 376553.453 | Acc: 10.111% (364/3600)\n",
            "Loss: 375731.591 | Acc: 10.108% (374/3700)\n",
            "Loss: 377465.947 | Acc: 10.000% (380/3800)\n",
            "Loss: 378443.638 | Acc: 9.949% (388/3900)\n",
            "Loss: 378704.985 | Acc: 9.875% (395/4000)\n",
            "Loss: 378862.459 | Acc: 9.829% (403/4100)\n",
            "Loss: 379323.646 | Acc: 9.952% (418/4200)\n",
            "Loss: 378701.567 | Acc: 10.000% (430/4300)\n",
            "Loss: 379176.313 | Acc: 10.045% (442/4400)\n",
            "Loss: 378900.001 | Acc: 10.089% (454/4500)\n",
            "Loss: 378176.371 | Acc: 10.065% (463/4600)\n",
            "Loss: 377858.745 | Acc: 9.979% (469/4700)\n",
            "Loss: 378934.247 | Acc: 9.958% (478/4800)\n",
            "Loss: 378328.534 | Acc: 9.939% (487/4900)\n",
            "Loss: 379129.715 | Acc: 9.900% (495/5000)\n",
            "Loss: 378811.488 | Acc: 9.961% (508/5100)\n",
            "Loss: 378554.753 | Acc: 9.942% (517/5200)\n",
            "Loss: 378567.308 | Acc: 10.057% (533/5300)\n",
            "Loss: 378226.986 | Acc: 10.111% (546/5400)\n",
            "Loss: 378540.371 | Acc: 10.073% (554/5500)\n",
            "Loss: 378611.740 | Acc: 10.071% (564/5600)\n",
            "Loss: 380038.735 | Acc: 9.982% (569/5700)\n",
            "Loss: 380347.351 | Acc: 10.000% (580/5800)\n",
            "Loss: 380411.633 | Acc: 9.983% (589/5900)\n",
            "Loss: 380558.185 | Acc: 9.917% (595/6000)\n",
            "Loss: 379485.941 | Acc: 10.016% (611/6100)\n",
            "Loss: 379532.110 | Acc: 10.016% (621/6200)\n",
            "Loss: 380082.948 | Acc: 9.984% (629/6300)\n",
            "Loss: 380316.300 | Acc: 10.062% (644/6400)\n",
            "Loss: 379578.508 | Acc: 10.108% (657/6500)\n",
            "Loss: 380330.560 | Acc: 10.061% (664/6600)\n",
            "Loss: 380146.897 | Acc: 10.149% (680/6700)\n",
            "Loss: 379997.731 | Acc: 10.162% (691/6800)\n",
            "Loss: 379120.881 | Acc: 10.203% (704/6900)\n",
            "Loss: 379291.762 | Acc: 10.200% (714/7000)\n",
            "Loss: 378829.842 | Acc: 10.169% (722/7100)\n",
            "Loss: 379442.702 | Acc: 10.250% (738/7200)\n",
            "Loss: 378706.117 | Acc: 10.192% (744/7300)\n",
            "Loss: 379128.745 | Acc: 10.122% (749/7400)\n",
            "Loss: 378828.767 | Acc: 10.120% (759/7500)\n",
            "Loss: 378835.945 | Acc: 10.092% (767/7600)\n",
            "Loss: 378317.722 | Acc: 10.143% (781/7700)\n",
            "Loss: 378671.013 | Acc: 10.128% (790/7800)\n",
            "Loss: 377749.116 | Acc: 10.228% (808/7900)\n",
            "Loss: 377398.396 | Acc: 10.200% (816/8000)\n",
            "Loss: 376927.740 | Acc: 10.235% (829/8100)\n",
            "Loss: 376844.712 | Acc: 10.183% (835/8200)\n",
            "Loss: 376878.638 | Acc: 10.133% (841/8300)\n",
            "Loss: 376878.922 | Acc: 10.071% (846/8400)\n",
            "Loss: 376618.138 | Acc: 10.082% (857/8500)\n",
            "Loss: 376604.445 | Acc: 10.058% (865/8600)\n",
            "Loss: 376896.516 | Acc: 9.989% (869/8700)\n",
            "Loss: 377226.432 | Acc: 9.955% (876/8800)\n",
            "Loss: 376676.407 | Acc: 10.000% (890/8900)\n",
            "Loss: 376932.618 | Acc: 10.033% (903/9000)\n",
            "Loss: 376810.748 | Acc: 10.066% (916/9100)\n",
            "Loss: 376230.159 | Acc: 10.065% (926/9200)\n",
            "Loss: 376803.846 | Acc: 10.043% (934/9300)\n",
            "Loss: 377299.375 | Acc: 9.989% (939/9400)\n",
            "Loss: 377153.300 | Acc: 10.011% (951/9500)\n",
            "Loss: 377258.472 | Acc: 10.031% (963/9600)\n",
            "Loss: 377453.411 | Acc: 10.031% (973/9700)\n",
            "Loss: 377660.459 | Acc: 10.010% (981/9800)\n",
            "Loss: 377535.928 | Acc: 10.061% (996/9900)\n",
            "Loss: 377690.064 | Acc: 10.100% (1010/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 3\n",
            "Loss: 17.186 | Acc: 0.000% (0/10)\n",
            "Loss: 11.323 | Acc: 0.000% (0/20)\n",
            "Loss: 13.377 | Acc: 3.333% (1/30)\n",
            "Loss: 11.173 | Acc: 5.000% (2/40)\n",
            "Loss: 15.272 | Acc: 4.000% (2/50)\n",
            "Loss: 18.747 | Acc: 3.333% (2/60)\n",
            "Loss: 16.410 | Acc: 8.571% (6/70)\n",
            "Loss: 15.014 | Acc: 10.000% (8/80)\n",
            "Loss: 13.777 | Acc: 10.000% (9/90)\n",
            "Loss: 15.337 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 3\n",
            "Loss: 20995.189 | Acc: 7.000% (7/100)\n",
            "Loss: 20793.377 | Acc: 10.000% (20/200)\n",
            "Loss: 21776.035 | Acc: 9.000% (27/300)\n",
            "Loss: 20413.957 | Acc: 10.250% (41/400)\n",
            "Loss: 20018.044 | Acc: 10.200% (51/500)\n",
            "Loss: 19778.555 | Acc: 9.833% (59/600)\n",
            "Loss: 19183.005 | Acc: 9.857% (69/700)\n",
            "Loss: 18697.902 | Acc: 9.750% (78/800)\n",
            "Loss: 18847.701 | Acc: 10.111% (91/900)\n",
            "Loss: 18617.672 | Acc: 10.200% (102/1000)\n",
            "Loss: 18522.737 | Acc: 10.182% (112/1100)\n",
            "Loss: 18516.825 | Acc: 10.083% (121/1200)\n",
            "Loss: 18754.339 | Acc: 10.154% (132/1300)\n",
            "Loss: 18716.139 | Acc: 10.357% (145/1400)\n",
            "Loss: 18946.730 | Acc: 10.600% (159/1500)\n",
            "Loss: 18740.497 | Acc: 10.812% (173/1600)\n",
            "Loss: 18812.577 | Acc: 10.647% (181/1700)\n",
            "Loss: 18882.168 | Acc: 10.722% (193/1800)\n",
            "Loss: 18878.816 | Acc: 10.842% (206/1900)\n",
            "Loss: 18836.971 | Acc: 10.950% (219/2000)\n",
            "Loss: 18910.379 | Acc: 11.000% (231/2100)\n",
            "Loss: 18768.286 | Acc: 11.000% (242/2200)\n",
            "Loss: 18665.645 | Acc: 11.087% (255/2300)\n",
            "Loss: 18793.640 | Acc: 11.083% (266/2400)\n",
            "Loss: 18699.264 | Acc: 11.040% (276/2500)\n",
            "Loss: 18581.407 | Acc: 11.462% (298/2600)\n",
            "Loss: 18616.625 | Acc: 11.444% (309/2700)\n",
            "Loss: 18811.954 | Acc: 11.357% (318/2800)\n",
            "Loss: 18744.704 | Acc: 11.379% (330/2900)\n",
            "Loss: 18742.837 | Acc: 11.467% (344/3000)\n",
            "Loss: 18803.346 | Acc: 11.355% (352/3100)\n",
            "Loss: 18811.826 | Acc: 11.281% (361/3200)\n",
            "Loss: 18904.964 | Acc: 11.152% (368/3300)\n",
            "Loss: 18921.444 | Acc: 11.147% (379/3400)\n",
            "Loss: 18974.563 | Acc: 11.229% (393/3500)\n",
            "Loss: 18985.855 | Acc: 11.444% (412/3600)\n",
            "Loss: 18982.103 | Acc: 11.378% (421/3700)\n",
            "Loss: 19054.034 | Acc: 11.632% (442/3800)\n",
            "Loss: 19037.750 | Acc: 11.590% (452/3900)\n",
            "Loss: 18951.961 | Acc: 11.650% (466/4000)\n",
            "Loss: 18962.546 | Acc: 11.634% (477/4100)\n",
            "Loss: 19032.130 | Acc: 11.619% (488/4200)\n",
            "Loss: 19057.625 | Acc: 11.512% (495/4300)\n",
            "Loss: 19084.593 | Acc: 11.386% (501/4400)\n",
            "Loss: 19076.901 | Acc: 11.444% (515/4500)\n",
            "Loss: 19018.640 | Acc: 11.457% (527/4600)\n",
            "Loss: 19022.426 | Acc: 11.553% (543/4700)\n",
            "Loss: 19056.819 | Acc: 11.479% (551/4800)\n",
            "Loss: 19021.059 | Acc: 11.490% (563/4900)\n",
            "Loss: 19068.480 | Acc: 11.440% (572/5000)\n",
            "Loss: 19071.625 | Acc: 11.412% (582/5100)\n",
            "Loss: 18976.714 | Acc: 11.404% (593/5200)\n",
            "Loss: 19079.274 | Acc: 11.396% (604/5300)\n",
            "Loss: 19087.635 | Acc: 11.352% (613/5400)\n",
            "Loss: 19047.562 | Acc: 11.291% (621/5500)\n",
            "Loss: 19033.532 | Acc: 11.268% (631/5600)\n",
            "Loss: 19106.302 | Acc: 11.281% (643/5700)\n",
            "Loss: 19053.593 | Acc: 11.379% (660/5800)\n",
            "Loss: 19040.843 | Acc: 11.390% (672/5900)\n",
            "Loss: 18987.034 | Acc: 11.367% (682/6000)\n",
            "Loss: 18924.751 | Acc: 11.410% (696/6100)\n",
            "Loss: 18925.439 | Acc: 11.403% (707/6200)\n",
            "Loss: 18957.666 | Acc: 11.381% (717/6300)\n",
            "Loss: 19008.167 | Acc: 11.344% (726/6400)\n",
            "Loss: 19047.783 | Acc: 11.354% (738/6500)\n",
            "Loss: 18999.548 | Acc: 11.348% (749/6600)\n",
            "Loss: 18982.719 | Acc: 11.373% (762/6700)\n",
            "Loss: 19009.842 | Acc: 11.324% (770/6800)\n",
            "Loss: 18959.671 | Acc: 11.304% (780/6900)\n",
            "Loss: 18971.789 | Acc: 11.300% (791/7000)\n",
            "Loss: 18918.767 | Acc: 11.338% (805/7100)\n",
            "Loss: 19006.282 | Acc: 11.306% (814/7200)\n",
            "Loss: 18959.858 | Acc: 11.356% (829/7300)\n",
            "Loss: 18963.598 | Acc: 11.432% (846/7400)\n",
            "Loss: 18970.529 | Acc: 11.400% (855/7500)\n",
            "Loss: 18951.327 | Acc: 11.408% (867/7600)\n",
            "Loss: 18947.736 | Acc: 11.390% (877/7700)\n",
            "Loss: 18947.224 | Acc: 11.385% (888/7800)\n",
            "Loss: 18938.374 | Acc: 11.405% (901/7900)\n",
            "Loss: 18870.048 | Acc: 11.425% (914/8000)\n",
            "Loss: 18876.419 | Acc: 11.383% (922/8100)\n",
            "Loss: 18873.232 | Acc: 11.280% (925/8200)\n",
            "Loss: 18885.096 | Acc: 11.193% (929/8300)\n",
            "Loss: 18855.947 | Acc: 11.202% (941/8400)\n",
            "Loss: 18806.219 | Acc: 11.235% (955/8500)\n",
            "Loss: 18821.660 | Acc: 11.174% (961/8600)\n",
            "Loss: 18822.745 | Acc: 11.230% (977/8700)\n",
            "Loss: 18769.175 | Acc: 11.273% (992/8800)\n",
            "Loss: 18759.476 | Acc: 11.292% (1005/8900)\n",
            "Loss: 18776.249 | Acc: 11.256% (1013/9000)\n",
            "Loss: 18781.111 | Acc: 11.253% (1024/9100)\n",
            "Loss: 18751.982 | Acc: 11.283% (1038/9200)\n",
            "Loss: 18747.487 | Acc: 11.226% (1044/9300)\n",
            "Loss: 18757.632 | Acc: 11.234% (1056/9400)\n",
            "Loss: 18745.108 | Acc: 11.232% (1067/9500)\n",
            "Loss: 18746.749 | Acc: 11.188% (1074/9600)\n",
            "Loss: 18748.490 | Acc: 11.175% (1084/9700)\n",
            "Loss: 18730.444 | Acc: 11.214% (1099/9800)\n",
            "Loss: 18748.262 | Acc: 11.202% (1109/9900)\n",
            "Loss: 18806.019 | Acc: 11.180% (1118/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 4\n",
            "Loss: 19.066 | Acc: 10.000% (1/10)\n",
            "Loss: 26.362 | Acc: 10.000% (2/20)\n",
            "Loss: 24.000 | Acc: 13.333% (4/30)\n",
            "Loss: 24.480 | Acc: 15.000% (6/40)\n",
            "Loss: 24.910 | Acc: 12.000% (6/50)\n",
            "Loss: 25.287 | Acc: 11.667% (7/60)\n",
            "Loss: 22.304 | Acc: 12.857% (9/70)\n",
            "Loss: 21.379 | Acc: 11.250% (9/80)\n",
            "Loss: 20.631 | Acc: 11.111% (10/90)\n",
            "Loss: 19.302 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 4\n",
            "Loss: 6834.314 | Acc: 21.000% (21/100)\n",
            "Loss: 6535.764 | Acc: 17.500% (35/200)\n",
            "Loss: 6362.247 | Acc: 17.333% (52/300)\n",
            "Loss: 6150.650 | Acc: 17.500% (70/400)\n",
            "Loss: 5939.763 | Acc: 16.600% (83/500)\n",
            "Loss: 5811.312 | Acc: 17.833% (107/600)\n",
            "Loss: 5644.791 | Acc: 18.000% (126/700)\n",
            "Loss: 5569.945 | Acc: 17.500% (140/800)\n",
            "Loss: 5677.786 | Acc: 16.222% (146/900)\n",
            "Loss: 5604.005 | Acc: 16.300% (163/1000)\n",
            "Loss: 5540.740 | Acc: 15.818% (174/1100)\n",
            "Loss: 5615.735 | Acc: 15.583% (187/1200)\n",
            "Loss: 5780.964 | Acc: 15.615% (203/1300)\n",
            "Loss: 5817.251 | Acc: 15.500% (217/1400)\n",
            "Loss: 5948.030 | Acc: 15.400% (231/1500)\n",
            "Loss: 5887.721 | Acc: 14.688% (235/1600)\n",
            "Loss: 5908.624 | Acc: 15.176% (258/1700)\n",
            "Loss: 5907.961 | Acc: 14.944% (269/1800)\n",
            "Loss: 5942.113 | Acc: 15.053% (286/1900)\n",
            "Loss: 5890.103 | Acc: 14.950% (299/2000)\n",
            "Loss: 5915.904 | Acc: 14.714% (309/2100)\n",
            "Loss: 5863.763 | Acc: 14.818% (326/2200)\n",
            "Loss: 5828.347 | Acc: 14.826% (341/2300)\n",
            "Loss: 5872.990 | Acc: 14.792% (355/2400)\n",
            "Loss: 5840.083 | Acc: 15.120% (378/2500)\n",
            "Loss: 5837.637 | Acc: 15.000% (390/2600)\n",
            "Loss: 5843.021 | Acc: 14.778% (399/2700)\n",
            "Loss: 5894.312 | Acc: 14.786% (414/2800)\n",
            "Loss: 5840.896 | Acc: 14.828% (430/2900)\n",
            "Loss: 5840.542 | Acc: 14.700% (441/3000)\n",
            "Loss: 5831.621 | Acc: 14.548% (451/3100)\n",
            "Loss: 5821.427 | Acc: 14.656% (469/3200)\n",
            "Loss: 5820.938 | Acc: 14.636% (483/3300)\n",
            "Loss: 5842.912 | Acc: 14.618% (497/3400)\n",
            "Loss: 5868.571 | Acc: 14.543% (509/3500)\n",
            "Loss: 5867.897 | Acc: 14.528% (523/3600)\n",
            "Loss: 5841.556 | Acc: 14.541% (538/3700)\n",
            "Loss: 5866.217 | Acc: 14.526% (552/3800)\n",
            "Loss: 5886.522 | Acc: 14.564% (568/3900)\n",
            "Loss: 5880.732 | Acc: 14.650% (586/4000)\n",
            "Loss: 5887.909 | Acc: 14.512% (595/4100)\n",
            "Loss: 5907.934 | Acc: 14.357% (603/4200)\n",
            "Loss: 5907.632 | Acc: 14.279% (614/4300)\n",
            "Loss: 5907.603 | Acc: 14.295% (629/4400)\n",
            "Loss: 5921.099 | Acc: 14.244% (641/4500)\n",
            "Loss: 5901.290 | Acc: 14.261% (656/4600)\n",
            "Loss: 5901.171 | Acc: 14.213% (668/4700)\n",
            "Loss: 5907.097 | Acc: 14.396% (691/4800)\n",
            "Loss: 5896.845 | Acc: 14.429% (707/4900)\n",
            "Loss: 5882.556 | Acc: 14.440% (722/5000)\n",
            "Loss: 5878.436 | Acc: 14.510% (740/5100)\n",
            "Loss: 5830.712 | Acc: 14.577% (758/5200)\n",
            "Loss: 5853.923 | Acc: 14.453% (766/5300)\n",
            "Loss: 5851.944 | Acc: 14.407% (778/5400)\n",
            "Loss: 5830.530 | Acc: 14.455% (795/5500)\n",
            "Loss: 5831.931 | Acc: 14.500% (812/5600)\n",
            "Loss: 5850.007 | Acc: 14.421% (822/5700)\n",
            "Loss: 5827.829 | Acc: 14.431% (837/5800)\n",
            "Loss: 5814.380 | Acc: 14.407% (850/5900)\n",
            "Loss: 5783.215 | Acc: 14.567% (874/6000)\n",
            "Loss: 5772.413 | Acc: 14.492% (884/6100)\n",
            "Loss: 5771.994 | Acc: 14.548% (902/6200)\n",
            "Loss: 5784.127 | Acc: 14.556% (917/6300)\n",
            "Loss: 5807.417 | Acc: 14.500% (928/6400)\n",
            "Loss: 5838.092 | Acc: 14.462% (940/6500)\n",
            "Loss: 5816.234 | Acc: 14.470% (955/6600)\n",
            "Loss: 5809.285 | Acc: 14.418% (966/6700)\n",
            "Loss: 5799.597 | Acc: 14.338% (975/6800)\n",
            "Loss: 5775.808 | Acc: 14.406% (994/6900)\n",
            "Loss: 5773.384 | Acc: 14.443% (1011/7000)\n",
            "Loss: 5774.689 | Acc: 14.366% (1020/7100)\n",
            "Loss: 5800.180 | Acc: 14.333% (1032/7200)\n",
            "Loss: 5785.596 | Acc: 14.274% (1042/7300)\n",
            "Loss: 5787.163 | Acc: 14.203% (1051/7400)\n",
            "Loss: 5778.319 | Acc: 14.227% (1067/7500)\n",
            "Loss: 5775.772 | Acc: 14.224% (1081/7600)\n",
            "Loss: 5773.213 | Acc: 14.260% (1098/7700)\n",
            "Loss: 5770.358 | Acc: 14.244% (1111/7800)\n",
            "Loss: 5764.666 | Acc: 14.241% (1125/7900)\n",
            "Loss: 5758.375 | Acc: 14.275% (1142/8000)\n",
            "Loss: 5753.265 | Acc: 14.284% (1157/8100)\n",
            "Loss: 5741.712 | Acc: 14.293% (1172/8200)\n",
            "Loss: 5739.477 | Acc: 14.241% (1182/8300)\n",
            "Loss: 5729.755 | Acc: 14.238% (1196/8400)\n",
            "Loss: 5723.020 | Acc: 14.259% (1212/8500)\n",
            "Loss: 5730.101 | Acc: 14.267% (1227/8600)\n",
            "Loss: 5739.040 | Acc: 14.241% (1239/8700)\n",
            "Loss: 5721.779 | Acc: 14.250% (1254/8800)\n",
            "Loss: 5719.273 | Acc: 14.213% (1265/8900)\n",
            "Loss: 5724.550 | Acc: 14.333% (1290/9000)\n",
            "Loss: 5725.926 | Acc: 14.352% (1306/9100)\n",
            "Loss: 5713.123 | Acc: 14.370% (1322/9200)\n",
            "Loss: 5710.720 | Acc: 14.430% (1342/9300)\n",
            "Loss: 5709.580 | Acc: 14.457% (1359/9400)\n",
            "Loss: 5706.588 | Acc: 14.463% (1374/9500)\n",
            "Loss: 5701.606 | Acc: 14.521% (1394/9600)\n",
            "Loss: 5701.817 | Acc: 14.515% (1408/9700)\n",
            "Loss: 5702.880 | Acc: 14.561% (1427/9800)\n",
            "Loss: 5710.335 | Acc: 14.545% (1440/9900)\n",
            "Loss: 5732.951 | Acc: 14.460% (1446/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 5\n",
            "Loss: 17.270 | Acc: 30.000% (3/10)\n",
            "Loss: 22.018 | Acc: 20.000% (4/20)\n",
            "Loss: 26.405 | Acc: 16.667% (5/30)\n",
            "Loss: 20.602 | Acc: 15.000% (6/40)\n",
            "Loss: 18.122 | Acc: 14.000% (7/50)\n",
            "Loss: 17.464 | Acc: 13.333% (8/60)\n",
            "Loss: 15.546 | Acc: 14.286% (10/70)\n",
            "Loss: 14.770 | Acc: 13.750% (11/80)\n",
            "Loss: 15.317 | Acc: 12.222% (11/90)\n",
            "Loss: 15.631 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 5\n",
            "Loss: 169.802 | Acc: 11.000% (11/100)\n",
            "Loss: 184.052 | Acc: 10.500% (21/200)\n",
            "Loss: 214.386 | Acc: 11.000% (33/300)\n",
            "Loss: 213.037 | Acc: 10.500% (42/400)\n",
            "Loss: 200.412 | Acc: 10.800% (54/500)\n",
            "Loss: 195.817 | Acc: 10.833% (65/600)\n",
            "Loss: 186.853 | Acc: 11.714% (82/700)\n",
            "Loss: 179.407 | Acc: 11.500% (92/800)\n",
            "Loss: 174.268 | Acc: 11.889% (107/900)\n",
            "Loss: 172.204 | Acc: 12.000% (120/1000)\n",
            "Loss: 165.883 | Acc: 11.818% (130/1100)\n",
            "Loss: 168.303 | Acc: 11.917% (143/1200)\n",
            "Loss: 178.981 | Acc: 11.923% (155/1300)\n",
            "Loss: 183.336 | Acc: 11.929% (167/1400)\n",
            "Loss: 187.632 | Acc: 11.733% (176/1500)\n",
            "Loss: 187.103 | Acc: 11.625% (186/1600)\n",
            "Loss: 191.842 | Acc: 11.529% (196/1700)\n",
            "Loss: 191.798 | Acc: 11.500% (207/1800)\n",
            "Loss: 195.008 | Acc: 11.474% (218/1900)\n",
            "Loss: 194.114 | Acc: 11.550% (231/2000)\n",
            "Loss: 197.562 | Acc: 11.524% (242/2100)\n",
            "Loss: 193.412 | Acc: 11.364% (250/2200)\n",
            "Loss: 189.985 | Acc: 11.391% (262/2300)\n",
            "Loss: 188.701 | Acc: 11.250% (270/2400)\n",
            "Loss: 189.902 | Acc: 11.280% (282/2500)\n",
            "Loss: 187.556 | Acc: 11.192% (291/2600)\n",
            "Loss: 186.180 | Acc: 11.148% (301/2700)\n",
            "Loss: 190.245 | Acc: 11.000% (308/2800)\n",
            "Loss: 191.059 | Acc: 10.931% (317/2900)\n",
            "Loss: 189.903 | Acc: 10.900% (327/3000)\n",
            "Loss: 188.714 | Acc: 11.000% (341/3100)\n",
            "Loss: 186.748 | Acc: 11.000% (352/3200)\n",
            "Loss: 186.811 | Acc: 11.030% (364/3300)\n",
            "Loss: 186.413 | Acc: 11.000% (374/3400)\n",
            "Loss: 186.469 | Acc: 10.971% (384/3500)\n",
            "Loss: 186.483 | Acc: 11.056% (398/3600)\n",
            "Loss: 186.025 | Acc: 11.162% (413/3700)\n",
            "Loss: 186.571 | Acc: 11.237% (427/3800)\n",
            "Loss: 188.521 | Acc: 11.128% (434/3900)\n",
            "Loss: 189.007 | Acc: 11.050% (442/4000)\n",
            "Loss: 189.480 | Acc: 10.951% (449/4100)\n",
            "Loss: 189.885 | Acc: 10.952% (460/4200)\n",
            "Loss: 188.263 | Acc: 10.977% (472/4300)\n",
            "Loss: 188.916 | Acc: 11.023% (485/4400)\n",
            "Loss: 190.093 | Acc: 11.222% (505/4500)\n",
            "Loss: 190.143 | Acc: 11.283% (519/4600)\n",
            "Loss: 190.474 | Acc: 11.298% (531/4700)\n",
            "Loss: 191.695 | Acc: 11.250% (540/4800)\n",
            "Loss: 190.494 | Acc: 11.327% (555/4900)\n",
            "Loss: 189.243 | Acc: 11.380% (569/5000)\n",
            "Loss: 188.314 | Acc: 11.314% (577/5100)\n",
            "Loss: 187.260 | Acc: 11.269% (586/5200)\n",
            "Loss: 187.285 | Acc: 11.340% (601/5300)\n",
            "Loss: 187.321 | Acc: 11.296% (610/5400)\n",
            "Loss: 187.260 | Acc: 11.291% (621/5500)\n",
            "Loss: 186.648 | Acc: 11.411% (639/5600)\n",
            "Loss: 187.322 | Acc: 11.386% (649/5700)\n",
            "Loss: 187.262 | Acc: 11.328% (657/5800)\n",
            "Loss: 186.501 | Acc: 11.322% (668/5900)\n",
            "Loss: 186.064 | Acc: 11.233% (674/6000)\n",
            "Loss: 185.458 | Acc: 11.197% (683/6100)\n",
            "Loss: 185.868 | Acc: 11.242% (697/6200)\n",
            "Loss: 186.588 | Acc: 11.254% (709/6300)\n",
            "Loss: 186.944 | Acc: 11.188% (716/6400)\n",
            "Loss: 187.040 | Acc: 11.154% (725/6500)\n",
            "Loss: 186.711 | Acc: 11.106% (733/6600)\n",
            "Loss: 186.589 | Acc: 11.119% (745/6700)\n",
            "Loss: 187.894 | Acc: 11.191% (761/6800)\n",
            "Loss: 186.119 | Acc: 11.246% (776/6900)\n",
            "Loss: 186.116 | Acc: 11.214% (785/7000)\n",
            "Loss: 185.225 | Acc: 11.211% (796/7100)\n",
            "Loss: 186.902 | Acc: 11.194% (806/7200)\n",
            "Loss: 185.912 | Acc: 11.260% (822/7300)\n",
            "Loss: 186.416 | Acc: 11.257% (833/7400)\n",
            "Loss: 186.467 | Acc: 11.307% (848/7500)\n",
            "Loss: 186.193 | Acc: 11.276% (857/7600)\n",
            "Loss: 185.665 | Acc: 11.234% (865/7700)\n",
            "Loss: 186.282 | Acc: 11.269% (879/7800)\n",
            "Loss: 185.227 | Acc: 11.316% (894/7900)\n",
            "Loss: 185.021 | Acc: 11.375% (910/8000)\n",
            "Loss: 184.483 | Acc: 11.432% (926/8100)\n",
            "Loss: 184.181 | Acc: 11.476% (941/8200)\n",
            "Loss: 184.014 | Acc: 11.602% (963/8300)\n",
            "Loss: 183.201 | Acc: 11.631% (977/8400)\n",
            "Loss: 183.163 | Acc: 11.624% (988/8500)\n",
            "Loss: 182.629 | Acc: 11.663% (1003/8600)\n",
            "Loss: 182.488 | Acc: 11.713% (1019/8700)\n",
            "Loss: 181.583 | Acc: 11.693% (1029/8800)\n",
            "Loss: 181.033 | Acc: 11.685% (1040/8900)\n",
            "Loss: 181.266 | Acc: 11.633% (1047/9000)\n",
            "Loss: 182.044 | Acc: 11.571% (1053/9100)\n",
            "Loss: 181.480 | Acc: 11.576% (1065/9200)\n",
            "Loss: 181.429 | Acc: 11.548% (1074/9300)\n",
            "Loss: 180.926 | Acc: 11.532% (1084/9400)\n",
            "Loss: 180.527 | Acc: 11.516% (1094/9500)\n",
            "Loss: 180.916 | Acc: 11.490% (1103/9600)\n",
            "Loss: 181.022 | Acc: 11.464% (1112/9700)\n",
            "Loss: 181.618 | Acc: 11.418% (1119/9800)\n",
            "Loss: 182.241 | Acc: 11.384% (1127/9900)\n",
            "Loss: 183.014 | Acc: 11.410% (1141/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 6\n",
            "Loss: 12.003 | Acc: 0.000% (0/10)\n",
            "Loss: 13.173 | Acc: 5.000% (1/20)\n",
            "Loss: 13.572 | Acc: 3.333% (1/30)\n",
            "Loss: 12.714 | Acc: 2.500% (1/40)\n",
            "Loss: 10.840 | Acc: 10.000% (5/50)\n",
            "Loss: 13.201 | Acc: 10.000% (6/60)\n",
            "Loss: 14.722 | Acc: 8.571% (6/70)\n",
            "Loss: 14.170 | Acc: 10.000% (8/80)\n",
            "Loss: 15.235 | Acc: 8.889% (8/90)\n",
            "Loss: 18.830 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 6\n",
            "Loss: 23.699 | Acc: 8.000% (8/100)\n",
            "Loss: 27.989 | Acc: 8.000% (16/200)\n",
            "Loss: 33.920 | Acc: 9.000% (27/300)\n",
            "Loss: 37.686 | Acc: 9.000% (36/400)\n",
            "Loss: 32.473 | Acc: 9.000% (45/500)\n",
            "Loss: 28.459 | Acc: 9.500% (57/600)\n",
            "Loss: 27.145 | Acc: 9.571% (67/700)\n",
            "Loss: 25.572 | Acc: 10.000% (80/800)\n",
            "Loss: 24.920 | Acc: 10.556% (95/900)\n",
            "Loss: 23.891 | Acc: 10.200% (102/1000)\n",
            "Loss: 23.602 | Acc: 9.636% (106/1100)\n",
            "Loss: 25.422 | Acc: 9.583% (115/1200)\n",
            "Loss: 28.109 | Acc: 9.615% (125/1300)\n",
            "Loss: 29.427 | Acc: 9.714% (136/1400)\n",
            "Loss: 29.122 | Acc: 10.000% (150/1500)\n",
            "Loss: 28.353 | Acc: 9.750% (156/1600)\n",
            "Loss: 29.923 | Acc: 9.706% (165/1700)\n",
            "Loss: 29.876 | Acc: 9.611% (173/1800)\n",
            "Loss: 29.774 | Acc: 9.632% (183/1900)\n",
            "Loss: 29.404 | Acc: 9.600% (192/2000)\n",
            "Loss: 31.419 | Acc: 9.524% (200/2100)\n",
            "Loss: 30.666 | Acc: 9.545% (210/2200)\n",
            "Loss: 29.744 | Acc: 9.391% (216/2300)\n",
            "Loss: 29.164 | Acc: 9.542% (229/2400)\n",
            "Loss: 29.680 | Acc: 9.400% (235/2500)\n",
            "Loss: 28.914 | Acc: 9.192% (239/2600)\n",
            "Loss: 28.498 | Acc: 9.259% (250/2700)\n",
            "Loss: 29.169 | Acc: 9.357% (262/2800)\n",
            "Loss: 29.682 | Acc: 9.448% (274/2900)\n",
            "Loss: 29.018 | Acc: 9.533% (286/3000)\n",
            "Loss: 28.748 | Acc: 9.581% (297/3100)\n",
            "Loss: 28.230 | Acc: 9.812% (314/3200)\n",
            "Loss: 27.898 | Acc: 9.879% (326/3300)\n",
            "Loss: 27.506 | Acc: 9.853% (335/3400)\n",
            "Loss: 27.814 | Acc: 9.771% (342/3500)\n",
            "Loss: 28.018 | Acc: 9.861% (355/3600)\n",
            "Loss: 28.264 | Acc: 9.730% (360/3700)\n",
            "Loss: 28.285 | Acc: 9.789% (372/3800)\n",
            "Loss: 28.519 | Acc: 9.872% (385/3900)\n",
            "Loss: 28.537 | Acc: 9.900% (396/4000)\n",
            "Loss: 28.265 | Acc: 9.976% (409/4100)\n",
            "Loss: 28.542 | Acc: 9.905% (416/4200)\n",
            "Loss: 28.309 | Acc: 9.814% (422/4300)\n",
            "Loss: 28.512 | Acc: 9.841% (433/4400)\n",
            "Loss: 28.692 | Acc: 9.800% (441/4500)\n",
            "Loss: 28.531 | Acc: 9.783% (450/4600)\n",
            "Loss: 28.277 | Acc: 9.851% (463/4700)\n",
            "Loss: 29.089 | Acc: 9.771% (469/4800)\n",
            "Loss: 28.865 | Acc: 9.816% (481/4900)\n",
            "Loss: 28.714 | Acc: 9.900% (495/5000)\n",
            "Loss: 28.350 | Acc: 9.902% (505/5100)\n",
            "Loss: 28.066 | Acc: 9.923% (516/5200)\n",
            "Loss: 28.018 | Acc: 9.943% (527/5300)\n",
            "Loss: 28.066 | Acc: 10.093% (545/5400)\n",
            "Loss: 28.157 | Acc: 10.127% (557/5500)\n",
            "Loss: 28.190 | Acc: 10.143% (568/5600)\n",
            "Loss: 28.440 | Acc: 10.175% (580/5700)\n",
            "Loss: 28.399 | Acc: 10.138% (588/5800)\n",
            "Loss: 28.121 | Acc: 10.153% (599/5900)\n",
            "Loss: 27.888 | Acc: 10.117% (607/6000)\n",
            "Loss: 27.713 | Acc: 10.082% (615/6100)\n",
            "Loss: 27.758 | Acc: 10.113% (627/6200)\n",
            "Loss: 27.798 | Acc: 10.079% (635/6300)\n",
            "Loss: 27.865 | Acc: 10.094% (646/6400)\n",
            "Loss: 27.966 | Acc: 10.092% (656/6500)\n",
            "Loss: 27.809 | Acc: 10.030% (662/6600)\n",
            "Loss: 27.858 | Acc: 10.030% (672/6700)\n",
            "Loss: 28.400 | Acc: 9.985% (679/6800)\n",
            "Loss: 28.109 | Acc: 9.957% (687/6900)\n",
            "Loss: 27.915 | Acc: 9.943% (696/7000)\n",
            "Loss: 27.618 | Acc: 10.014% (711/7100)\n",
            "Loss: 28.430 | Acc: 9.958% (717/7200)\n",
            "Loss: 28.156 | Acc: 9.973% (728/7300)\n",
            "Loss: 28.019 | Acc: 9.959% (737/7400)\n",
            "Loss: 28.010 | Acc: 9.960% (747/7500)\n",
            "Loss: 28.033 | Acc: 9.934% (755/7600)\n",
            "Loss: 28.052 | Acc: 9.935% (765/7700)\n",
            "Loss: 27.862 | Acc: 9.923% (774/7800)\n",
            "Loss: 27.820 | Acc: 9.949% (786/7900)\n",
            "Loss: 27.862 | Acc: 9.963% (797/8000)\n",
            "Loss: 27.680 | Acc: 10.062% (815/8100)\n",
            "Loss: 27.710 | Acc: 10.061% (825/8200)\n",
            "Loss: 27.590 | Acc: 10.048% (834/8300)\n",
            "Loss: 27.469 | Acc: 10.083% (847/8400)\n",
            "Loss: 27.396 | Acc: 10.012% (851/8500)\n",
            "Loss: 27.312 | Acc: 10.058% (865/8600)\n",
            "Loss: 27.343 | Acc: 10.046% (874/8700)\n",
            "Loss: 27.147 | Acc: 10.045% (884/8800)\n",
            "Loss: 27.275 | Acc: 10.045% (894/8900)\n",
            "Loss: 27.235 | Acc: 10.044% (904/9000)\n",
            "Loss: 27.446 | Acc: 10.022% (912/9100)\n",
            "Loss: 27.441 | Acc: 10.022% (922/9200)\n",
            "Loss: 27.366 | Acc: 10.129% (942/9300)\n",
            "Loss: 27.160 | Acc: 10.170% (956/9400)\n",
            "Loss: 26.997 | Acc: 10.189% (968/9500)\n",
            "Loss: 27.014 | Acc: 10.188% (978/9600)\n",
            "Loss: 26.893 | Acc: 10.206% (990/9700)\n",
            "Loss: 27.029 | Acc: 10.204% (1000/9800)\n",
            "Loss: 27.174 | Acc: 10.192% (1009/9900)\n",
            "Loss: 27.412 | Acc: 10.200% (1020/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 7\n",
            "Loss: 26.874 | Acc: 20.000% (2/10)\n",
            "Loss: 21.958 | Acc: 20.000% (4/20)\n",
            "Loss: 19.893 | Acc: 13.333% (4/30)\n",
            "Loss: 22.715 | Acc: 10.000% (4/40)\n",
            "Loss: 19.100 | Acc: 10.000% (5/50)\n",
            "Loss: 16.787 | Acc: 8.333% (5/60)\n",
            "Loss: 17.331 | Acc: 8.571% (6/70)\n",
            "Loss: 18.981 | Acc: 7.500% (6/80)\n",
            "Loss: 18.253 | Acc: 8.889% (8/90)\n",
            "Loss: 20.711 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 7\n",
            "Loss: 44.892 | Acc: 13.000% (13/100)\n",
            "Loss: 35.039 | Acc: 13.500% (27/200)\n",
            "Loss: 35.568 | Acc: 12.000% (36/300)\n",
            "Loss: 32.800 | Acc: 11.250% (45/400)\n",
            "Loss: 27.300 | Acc: 11.400% (57/500)\n",
            "Loss: 23.487 | Acc: 10.833% (65/600)\n",
            "Loss: 22.212 | Acc: 11.000% (77/700)\n",
            "Loss: 21.464 | Acc: 10.875% (87/800)\n",
            "Loss: 22.698 | Acc: 10.667% (96/900)\n",
            "Loss: 21.237 | Acc: 10.700% (107/1000)\n",
            "Loss: 21.657 | Acc: 10.364% (114/1100)\n",
            "Loss: 20.713 | Acc: 10.583% (127/1200)\n",
            "Loss: 21.532 | Acc: 11.000% (143/1300)\n",
            "Loss: 22.621 | Acc: 10.714% (150/1400)\n",
            "Loss: 22.002 | Acc: 10.800% (162/1500)\n",
            "Loss: 21.030 | Acc: 10.625% (170/1600)\n",
            "Loss: 22.978 | Acc: 10.529% (179/1700)\n",
            "Loss: 24.955 | Acc: 10.389% (187/1800)\n",
            "Loss: 24.404 | Acc: 10.474% (199/1900)\n",
            "Loss: 25.076 | Acc: 10.300% (206/2000)\n",
            "Loss: 26.826 | Acc: 10.286% (216/2100)\n",
            "Loss: 26.213 | Acc: 10.182% (224/2200)\n",
            "Loss: 25.802 | Acc: 10.174% (234/2300)\n",
            "Loss: 25.695 | Acc: 10.000% (240/2400)\n",
            "Loss: 25.912 | Acc: 10.080% (252/2500)\n",
            "Loss: 25.250 | Acc: 10.077% (262/2600)\n",
            "Loss: 24.867 | Acc: 9.963% (269/2700)\n",
            "Loss: 25.844 | Acc: 9.857% (276/2800)\n",
            "Loss: 26.329 | Acc: 9.828% (285/2900)\n",
            "Loss: 25.857 | Acc: 9.967% (299/3000)\n",
            "Loss: 25.587 | Acc: 9.935% (308/3100)\n",
            "Loss: 25.452 | Acc: 9.906% (317/3200)\n",
            "Loss: 25.087 | Acc: 9.909% (327/3300)\n",
            "Loss: 25.513 | Acc: 9.971% (339/3400)\n",
            "Loss: 25.836 | Acc: 9.829% (344/3500)\n",
            "Loss: 25.482 | Acc: 9.917% (357/3600)\n",
            "Loss: 25.498 | Acc: 9.892% (366/3700)\n",
            "Loss: 25.734 | Acc: 9.789% (372/3800)\n",
            "Loss: 25.812 | Acc: 9.974% (389/3900)\n",
            "Loss: 25.516 | Acc: 9.950% (398/4000)\n",
            "Loss: 25.293 | Acc: 9.976% (409/4100)\n",
            "Loss: 25.741 | Acc: 10.000% (420/4200)\n",
            "Loss: 25.374 | Acc: 10.047% (432/4300)\n",
            "Loss: 25.189 | Acc: 10.045% (442/4400)\n",
            "Loss: 25.065 | Acc: 10.089% (454/4500)\n",
            "Loss: 24.754 | Acc: 10.043% (462/4600)\n",
            "Loss: 24.388 | Acc: 9.957% (468/4700)\n",
            "Loss: 24.653 | Acc: 9.917% (476/4800)\n",
            "Loss: 24.571 | Acc: 9.837% (482/4900)\n",
            "Loss: 24.313 | Acc: 9.820% (491/5000)\n",
            "Loss: 24.300 | Acc: 9.765% (498/5100)\n",
            "Loss: 23.953 | Acc: 9.750% (507/5200)\n",
            "Loss: 23.966 | Acc: 9.698% (514/5300)\n",
            "Loss: 23.861 | Acc: 9.685% (523/5400)\n",
            "Loss: 24.072 | Acc: 9.636% (530/5500)\n",
            "Loss: 24.041 | Acc: 9.589% (537/5600)\n",
            "Loss: 24.212 | Acc: 9.632% (549/5700)\n",
            "Loss: 24.023 | Acc: 9.603% (557/5800)\n",
            "Loss: 23.724 | Acc: 9.542% (563/5900)\n",
            "Loss: 23.444 | Acc: 9.467% (568/6000)\n",
            "Loss: 23.223 | Acc: 9.475% (578/6100)\n",
            "Loss: 23.328 | Acc: 9.452% (586/6200)\n",
            "Loss: 23.075 | Acc: 9.492% (598/6300)\n",
            "Loss: 23.165 | Acc: 9.469% (606/6400)\n",
            "Loss: 23.329 | Acc: 9.554% (621/6500)\n",
            "Loss: 23.073 | Acc: 9.530% (629/6600)\n",
            "Loss: 22.804 | Acc: 9.507% (637/6700)\n",
            "Loss: 22.978 | Acc: 9.588% (652/6800)\n",
            "Loss: 22.748 | Acc: 9.536% (658/6900)\n",
            "Loss: 22.595 | Acc: 9.586% (671/7000)\n",
            "Loss: 22.402 | Acc: 9.577% (680/7100)\n",
            "Loss: 23.121 | Acc: 9.556% (688/7200)\n",
            "Loss: 22.893 | Acc: 9.507% (694/7300)\n",
            "Loss: 23.187 | Acc: 9.514% (704/7400)\n",
            "Loss: 23.125 | Acc: 9.520% (714/7500)\n",
            "Loss: 23.291 | Acc: 9.553% (726/7600)\n",
            "Loss: 23.204 | Acc: 9.558% (736/7700)\n",
            "Loss: 22.993 | Acc: 9.577% (747/7800)\n",
            "Loss: 23.543 | Acc: 9.544% (754/7900)\n",
            "Loss: 23.333 | Acc: 9.537% (763/8000)\n",
            "Loss: 23.148 | Acc: 9.543% (773/8100)\n",
            "Loss: 23.071 | Acc: 9.634% (790/8200)\n",
            "Loss: 22.901 | Acc: 9.651% (801/8300)\n",
            "Loss: 22.772 | Acc: 9.643% (810/8400)\n",
            "Loss: 22.659 | Acc: 9.624% (818/8500)\n",
            "Loss: 22.647 | Acc: 9.674% (832/8600)\n",
            "Loss: 22.997 | Acc: 9.713% (845/8700)\n",
            "Loss: 22.799 | Acc: 9.682% (852/8800)\n",
            "Loss: 22.774 | Acc: 9.719% (865/8900)\n",
            "Loss: 22.667 | Acc: 9.722% (875/9000)\n",
            "Loss: 22.880 | Acc: 9.747% (887/9100)\n",
            "Loss: 22.832 | Acc: 9.750% (897/9200)\n",
            "Loss: 23.065 | Acc: 9.774% (909/9300)\n",
            "Loss: 22.917 | Acc: 9.745% (916/9400)\n",
            "Loss: 22.943 | Acc: 9.758% (927/9500)\n",
            "Loss: 22.879 | Acc: 9.719% (933/9600)\n",
            "Loss: 22.716 | Acc: 9.691% (940/9700)\n",
            "Loss: 22.632 | Acc: 9.714% (952/9800)\n",
            "Loss: 22.666 | Acc: 9.727% (963/9900)\n",
            "Loss: 22.912 | Acc: 9.740% (974/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 8\n",
            "Loss: 28.523 | Acc: 0.000% (0/10)\n",
            "Loss: 36.766 | Acc: 5.000% (1/20)\n",
            "Loss: 26.653 | Acc: 3.333% (1/30)\n",
            "Loss: 20.874 | Acc: 10.000% (4/40)\n",
            "Loss: 21.203 | Acc: 10.000% (5/50)\n",
            "Loss: 25.150 | Acc: 11.667% (7/60)\n",
            "Loss: 22.278 | Acc: 14.286% (10/70)\n",
            "Loss: 21.339 | Acc: 13.750% (11/80)\n",
            "Loss: 19.820 | Acc: 14.444% (13/90)\n",
            "Loss: 24.725 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 8\n",
            "Loss: 84.112 | Acc: 14.000% (14/100)\n",
            "Loss: 74.136 | Acc: 10.500% (21/200)\n",
            "Loss: 95.067 | Acc: 10.333% (31/300)\n",
            "Loss: 93.458 | Acc: 11.750% (47/400)\n",
            "Loss: 79.035 | Acc: 11.600% (58/500)\n",
            "Loss: 67.696 | Acc: 13.167% (79/600)\n",
            "Loss: 64.291 | Acc: 12.571% (88/700)\n",
            "Loss: 61.284 | Acc: 13.000% (104/800)\n",
            "Loss: 61.421 | Acc: 12.556% (113/900)\n",
            "Loss: 57.981 | Acc: 12.500% (125/1000)\n",
            "Loss: 58.606 | Acc: 12.545% (138/1100)\n",
            "Loss: 60.768 | Acc: 12.667% (152/1200)\n",
            "Loss: 64.231 | Acc: 12.385% (161/1300)\n",
            "Loss: 67.168 | Acc: 12.357% (173/1400)\n",
            "Loss: 65.913 | Acc: 12.200% (183/1500)\n",
            "Loss: 64.166 | Acc: 12.000% (192/1600)\n",
            "Loss: 67.381 | Acc: 12.118% (206/1700)\n",
            "Loss: 68.729 | Acc: 11.889% (214/1800)\n",
            "Loss: 67.956 | Acc: 11.684% (222/1900)\n",
            "Loss: 67.870 | Acc: 11.650% (233/2000)\n",
            "Loss: 70.804 | Acc: 11.429% (240/2100)\n",
            "Loss: 69.612 | Acc: 11.273% (248/2200)\n",
            "Loss: 68.300 | Acc: 11.174% (257/2300)\n",
            "Loss: 68.608 | Acc: 11.042% (265/2400)\n",
            "Loss: 69.984 | Acc: 11.120% (278/2500)\n",
            "Loss: 68.322 | Acc: 11.192% (291/2600)\n",
            "Loss: 68.026 | Acc: 11.296% (305/2700)\n",
            "Loss: 70.662 | Acc: 11.179% (313/2800)\n",
            "Loss: 71.859 | Acc: 11.241% (326/2900)\n",
            "Loss: 70.494 | Acc: 11.167% (335/3000)\n",
            "Loss: 70.151 | Acc: 11.065% (343/3100)\n",
            "Loss: 69.221 | Acc: 11.062% (354/3200)\n",
            "Loss: 68.869 | Acc: 10.909% (360/3300)\n",
            "Loss: 68.504 | Acc: 10.853% (369/3400)\n",
            "Loss: 69.354 | Acc: 10.886% (381/3500)\n",
            "Loss: 70.113 | Acc: 10.944% (394/3600)\n",
            "Loss: 70.308 | Acc: 10.919% (404/3700)\n",
            "Loss: 71.612 | Acc: 10.789% (410/3800)\n",
            "Loss: 72.541 | Acc: 10.718% (418/3900)\n",
            "Loss: 71.818 | Acc: 10.675% (427/4000)\n",
            "Loss: 71.050 | Acc: 10.634% (436/4100)\n",
            "Loss: 71.388 | Acc: 10.643% (447/4200)\n",
            "Loss: 70.247 | Acc: 10.581% (455/4300)\n",
            "Loss: 70.691 | Acc: 10.545% (464/4400)\n",
            "Loss: 71.101 | Acc: 10.400% (468/4500)\n",
            "Loss: 70.784 | Acc: 10.457% (481/4600)\n",
            "Loss: 70.003 | Acc: 10.511% (494/4700)\n",
            "Loss: 70.972 | Acc: 10.667% (512/4800)\n",
            "Loss: 70.563 | Acc: 10.592% (519/4900)\n",
            "Loss: 70.682 | Acc: 10.540% (527/5000)\n",
            "Loss: 70.059 | Acc: 10.569% (539/5100)\n",
            "Loss: 69.065 | Acc: 10.615% (552/5200)\n",
            "Loss: 69.050 | Acc: 10.585% (561/5300)\n",
            "Loss: 69.048 | Acc: 10.574% (571/5400)\n",
            "Loss: 68.678 | Acc: 10.691% (588/5500)\n",
            "Loss: 69.026 | Acc: 10.821% (606/5600)\n",
            "Loss: 69.731 | Acc: 10.825% (617/5700)\n",
            "Loss: 69.356 | Acc: 10.793% (626/5800)\n",
            "Loss: 68.344 | Acc: 10.814% (638/5900)\n",
            "Loss: 67.473 | Acc: 10.833% (650/6000)\n",
            "Loss: 67.245 | Acc: 10.803% (659/6100)\n",
            "Loss: 67.446 | Acc: 10.774% (668/6200)\n",
            "Loss: 67.336 | Acc: 10.810% (681/6300)\n",
            "Loss: 67.312 | Acc: 10.766% (689/6400)\n",
            "Loss: 67.463 | Acc: 10.754% (699/6500)\n",
            "Loss: 66.866 | Acc: 10.803% (713/6600)\n",
            "Loss: 66.761 | Acc: 10.836% (726/6700)\n",
            "Loss: 68.221 | Acc: 10.765% (732/6800)\n",
            "Loss: 67.529 | Acc: 10.841% (748/6900)\n",
            "Loss: 67.268 | Acc: 10.857% (760/7000)\n",
            "Loss: 66.748 | Acc: 10.845% (770/7100)\n",
            "Loss: 68.007 | Acc: 10.903% (785/7200)\n",
            "Loss: 67.460 | Acc: 10.849% (792/7300)\n",
            "Loss: 67.843 | Acc: 10.851% (803/7400)\n",
            "Loss: 67.585 | Acc: 10.840% (813/7500)\n",
            "Loss: 67.733 | Acc: 10.829% (823/7600)\n",
            "Loss: 67.836 | Acc: 10.805% (832/7700)\n",
            "Loss: 67.304 | Acc: 10.808% (843/7800)\n",
            "Loss: 67.906 | Acc: 10.785% (852/7900)\n",
            "Loss: 67.971 | Acc: 10.787% (863/8000)\n",
            "Loss: 67.433 | Acc: 10.778% (873/8100)\n",
            "Loss: 67.180 | Acc: 10.866% (891/8200)\n",
            "Loss: 66.698 | Acc: 10.867% (902/8300)\n",
            "Loss: 66.271 | Acc: 10.893% (915/8400)\n",
            "Loss: 66.107 | Acc: 10.918% (928/8500)\n",
            "Loss: 65.961 | Acc: 10.930% (940/8600)\n",
            "Loss: 66.215 | Acc: 10.897% (948/8700)\n",
            "Loss: 65.739 | Acc: 10.875% (957/8800)\n",
            "Loss: 66.155 | Acc: 10.899% (970/8900)\n",
            "Loss: 66.014 | Acc: 10.944% (985/9000)\n",
            "Loss: 66.553 | Acc: 10.945% (996/9100)\n",
            "Loss: 66.450 | Acc: 10.957% (1008/9200)\n",
            "Loss: 66.216 | Acc: 10.914% (1015/9300)\n",
            "Loss: 65.901 | Acc: 10.904% (1025/9400)\n",
            "Loss: 65.691 | Acc: 10.884% (1034/9500)\n",
            "Loss: 65.529 | Acc: 10.906% (1047/9600)\n",
            "Loss: 65.235 | Acc: 10.918% (1059/9700)\n",
            "Loss: 65.308 | Acc: 10.898% (1068/9800)\n",
            "Loss: 65.672 | Acc: 10.869% (1076/9900)\n",
            "Loss: 66.378 | Acc: 10.820% (1082/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 9\n",
            "Loss: 65.988 | Acc: 10.000% (1/10)\n",
            "Loss: 46.224 | Acc: 20.000% (4/20)\n",
            "Loss: 40.004 | Acc: 13.333% (4/30)\n",
            "Loss: 34.551 | Acc: 10.000% (4/40)\n",
            "Loss: 37.034 | Acc: 12.000% (6/50)\n",
            "Loss: 32.808 | Acc: 13.333% (8/60)\n",
            "Loss: 32.486 | Acc: 14.286% (10/70)\n",
            "Loss: 29.174 | Acc: 13.750% (11/80)\n",
            "Loss: 27.864 | Acc: 13.333% (12/90)\n",
            "Loss: 28.267 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 9\n",
            "Loss: 11.590 | Acc: 9.000% (9/100)\n",
            "Loss: 10.373 | Acc: 14.000% (28/200)\n",
            "Loss: 16.790 | Acc: 14.000% (42/300)\n",
            "Loss: 21.120 | Acc: 15.250% (61/400)\n",
            "Loss: 18.499 | Acc: 15.200% (76/500)\n",
            "Loss: 16.199 | Acc: 15.167% (91/600)\n",
            "Loss: 16.354 | Acc: 14.571% (102/700)\n",
            "Loss: 15.915 | Acc: 14.875% (119/800)\n",
            "Loss: 15.773 | Acc: 14.556% (131/900)\n",
            "Loss: 14.844 | Acc: 14.400% (144/1000)\n",
            "Loss: 15.721 | Acc: 14.182% (156/1100)\n",
            "Loss: 19.004 | Acc: 14.250% (171/1200)\n",
            "Loss: 20.538 | Acc: 14.077% (183/1300)\n",
            "Loss: 21.195 | Acc: 14.143% (198/1400)\n",
            "Loss: 20.579 | Acc: 13.867% (208/1500)\n",
            "Loss: 20.367 | Acc: 13.625% (218/1600)\n",
            "Loss: 20.849 | Acc: 13.765% (234/1700)\n",
            "Loss: 21.672 | Acc: 13.556% (244/1800)\n",
            "Loss: 21.858 | Acc: 13.632% (259/1900)\n",
            "Loss: 21.711 | Acc: 13.600% (272/2000)\n",
            "Loss: 24.525 | Acc: 13.762% (289/2100)\n",
            "Loss: 23.789 | Acc: 14.091% (310/2200)\n",
            "Loss: 23.597 | Acc: 14.130% (325/2300)\n",
            "Loss: 23.216 | Acc: 14.250% (342/2400)\n",
            "Loss: 23.006 | Acc: 14.160% (354/2500)\n",
            "Loss: 22.621 | Acc: 14.231% (370/2600)\n",
            "Loss: 22.228 | Acc: 14.259% (385/2700)\n",
            "Loss: 21.998 | Acc: 14.286% (400/2800)\n",
            "Loss: 22.122 | Acc: 14.345% (416/2900)\n",
            "Loss: 21.569 | Acc: 14.500% (435/3000)\n",
            "Loss: 21.238 | Acc: 14.355% (445/3100)\n",
            "Loss: 20.768 | Acc: 14.375% (460/3200)\n",
            "Loss: 20.523 | Acc: 14.212% (469/3300)\n",
            "Loss: 20.364 | Acc: 14.265% (485/3400)\n",
            "Loss: 21.302 | Acc: 14.200% (497/3500)\n",
            "Loss: 21.656 | Acc: 14.333% (516/3600)\n",
            "Loss: 22.059 | Acc: 14.459% (535/3700)\n",
            "Loss: 22.852 | Acc: 14.579% (554/3800)\n",
            "Loss: 23.057 | Acc: 14.538% (567/3900)\n",
            "Loss: 23.092 | Acc: 14.625% (585/4000)\n",
            "Loss: 22.676 | Acc: 14.488% (594/4100)\n",
            "Loss: 22.756 | Acc: 14.429% (606/4200)\n",
            "Loss: 22.667 | Acc: 14.442% (621/4300)\n",
            "Loss: 23.026 | Acc: 14.432% (635/4400)\n",
            "Loss: 23.104 | Acc: 14.311% (644/4500)\n",
            "Loss: 22.920 | Acc: 14.283% (657/4600)\n",
            "Loss: 22.639 | Acc: 14.191% (667/4700)\n",
            "Loss: 22.785 | Acc: 14.271% (685/4800)\n",
            "Loss: 22.796 | Acc: 14.286% (700/4900)\n",
            "Loss: 22.656 | Acc: 14.240% (712/5000)\n",
            "Loss: 22.355 | Acc: 14.255% (727/5100)\n",
            "Loss: 22.146 | Acc: 14.269% (742/5200)\n",
            "Loss: 21.978 | Acc: 14.208% (753/5300)\n",
            "Loss: 22.295 | Acc: 14.111% (762/5400)\n",
            "Loss: 22.097 | Acc: 14.109% (776/5500)\n",
            "Loss: 22.082 | Acc: 14.071% (788/5600)\n",
            "Loss: 22.194 | Acc: 13.947% (795/5700)\n",
            "Loss: 22.092 | Acc: 14.034% (814/5800)\n",
            "Loss: 21.807 | Acc: 13.983% (825/5900)\n",
            "Loss: 21.537 | Acc: 14.000% (840/6000)\n",
            "Loss: 21.320 | Acc: 13.984% (853/6100)\n",
            "Loss: 21.114 | Acc: 14.081% (873/6200)\n",
            "Loss: 20.960 | Acc: 14.079% (887/6300)\n",
            "Loss: 20.875 | Acc: 14.047% (899/6400)\n",
            "Loss: 20.838 | Acc: 13.969% (908/6500)\n",
            "Loss: 20.680 | Acc: 14.000% (924/6600)\n",
            "Loss: 20.890 | Acc: 13.955% (935/6700)\n",
            "Loss: 21.234 | Acc: 14.044% (955/6800)\n",
            "Loss: 21.054 | Acc: 13.986% (965/6900)\n",
            "Loss: 21.059 | Acc: 13.914% (974/7000)\n",
            "Loss: 20.851 | Acc: 13.930% (989/7100)\n",
            "Loss: 20.965 | Acc: 13.847% (997/7200)\n",
            "Loss: 20.747 | Acc: 13.822% (1009/7300)\n",
            "Loss: 20.699 | Acc: 13.770% (1019/7400)\n",
            "Loss: 20.564 | Acc: 13.760% (1032/7500)\n",
            "Loss: 20.574 | Acc: 13.829% (1051/7600)\n",
            "Loss: 20.587 | Acc: 13.909% (1071/7700)\n",
            "Loss: 20.384 | Acc: 13.936% (1087/7800)\n",
            "Loss: 20.350 | Acc: 13.949% (1102/7900)\n",
            "Loss: 20.451 | Acc: 13.963% (1117/8000)\n",
            "Loss: 20.268 | Acc: 13.988% (1133/8100)\n",
            "Loss: 20.263 | Acc: 13.915% (1141/8200)\n",
            "Loss: 20.086 | Acc: 13.843% (1149/8300)\n",
            "Loss: 19.982 | Acc: 13.774% (1157/8400)\n",
            "Loss: 19.976 | Acc: 13.776% (1171/8500)\n",
            "Loss: 19.867 | Acc: 13.709% (1179/8600)\n",
            "Loss: 19.858 | Acc: 13.655% (1188/8700)\n",
            "Loss: 19.702 | Acc: 13.693% (1205/8800)\n",
            "Loss: 20.182 | Acc: 13.685% (1218/8900)\n",
            "Loss: 20.060 | Acc: 13.733% (1236/9000)\n",
            "Loss: 19.976 | Acc: 13.813% (1257/9100)\n",
            "Loss: 19.954 | Acc: 13.804% (1270/9200)\n",
            "Loss: 19.819 | Acc: 13.774% (1281/9300)\n",
            "Loss: 19.731 | Acc: 13.777% (1295/9400)\n",
            "Loss: 19.593 | Acc: 13.758% (1307/9500)\n",
            "Loss: 19.689 | Acc: 13.750% (1320/9600)\n",
            "Loss: 19.569 | Acc: 13.732% (1332/9700)\n",
            "Loss: 19.590 | Acc: 13.776% (1350/9800)\n",
            "Loss: 19.743 | Acc: 13.788% (1365/9900)\n",
            "Loss: 19.817 | Acc: 13.770% (1377/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 10\n",
            "Loss: 21.629 | Acc: 10.000% (1/10)\n",
            "Loss: 12.711 | Acc: 20.000% (4/20)\n",
            "Loss: 11.108 | Acc: 20.000% (6/30)\n",
            "Loss: 11.666 | Acc: 17.500% (7/40)\n",
            "Loss: 12.539 | Acc: 14.000% (7/50)\n",
            "Loss: 11.699 | Acc: 11.667% (7/60)\n",
            "Loss: 10.908 | Acc: 12.857% (9/70)\n",
            "Loss: 11.070 | Acc: 12.500% (10/80)\n",
            "Loss: 14.032 | Acc: 12.222% (11/90)\n",
            "Loss: 13.343 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 10\n",
            "Loss: 4.469 | Acc: 12.000% (12/100)\n",
            "Loss: 4.640 | Acc: 10.500% (21/200)\n",
            "Loss: 5.895 | Acc: 10.000% (30/300)\n",
            "Loss: 5.496 | Acc: 9.750% (39/400)\n",
            "Loss: 5.391 | Acc: 10.400% (52/500)\n",
            "Loss: 5.195 | Acc: 10.167% (61/600)\n",
            "Loss: 5.386 | Acc: 11.000% (77/700)\n",
            "Loss: 5.621 | Acc: 10.625% (85/800)\n",
            "Loss: 5.462 | Acc: 11.222% (101/900)\n",
            "Loss: 5.408 | Acc: 11.100% (111/1000)\n",
            "Loss: 5.454 | Acc: 11.182% (123/1100)\n",
            "Loss: 5.424 | Acc: 11.333% (136/1200)\n",
            "Loss: 5.458 | Acc: 11.231% (146/1300)\n",
            "Loss: 5.472 | Acc: 11.000% (154/1400)\n",
            "Loss: 5.473 | Acc: 10.600% (159/1500)\n",
            "Loss: 5.545 | Acc: 10.562% (169/1600)\n",
            "Loss: 5.583 | Acc: 10.529% (179/1700)\n",
            "Loss: 5.996 | Acc: 10.444% (188/1800)\n",
            "Loss: 5.951 | Acc: 10.474% (199/1900)\n",
            "Loss: 5.957 | Acc: 10.400% (208/2000)\n",
            "Loss: 6.578 | Acc: 10.429% (219/2100)\n",
            "Loss: 6.555 | Acc: 10.318% (227/2200)\n",
            "Loss: 6.465 | Acc: 10.348% (238/2300)\n",
            "Loss: 6.374 | Acc: 10.208% (245/2400)\n",
            "Loss: 6.307 | Acc: 10.200% (255/2500)\n",
            "Loss: 6.245 | Acc: 10.115% (263/2600)\n",
            "Loss: 6.163 | Acc: 10.148% (274/2700)\n",
            "Loss: 6.110 | Acc: 10.143% (284/2800)\n",
            "Loss: 6.278 | Acc: 10.207% (296/2900)\n",
            "Loss: 6.231 | Acc: 10.200% (306/3000)\n",
            "Loss: 6.180 | Acc: 10.194% (316/3100)\n",
            "Loss: 6.146 | Acc: 10.188% (326/3200)\n",
            "Loss: 6.101 | Acc: 10.242% (338/3300)\n",
            "Loss: 6.140 | Acc: 10.206% (347/3400)\n",
            "Loss: 6.410 | Acc: 10.171% (356/3500)\n",
            "Loss: 6.357 | Acc: 10.139% (365/3600)\n",
            "Loss: 6.306 | Acc: 10.216% (378/3700)\n",
            "Loss: 6.555 | Acc: 10.211% (388/3800)\n",
            "Loss: 6.560 | Acc: 10.154% (396/3900)\n",
            "Loss: 6.537 | Acc: 10.150% (406/4000)\n",
            "Loss: 6.499 | Acc: 10.195% (418/4100)\n",
            "Loss: 6.585 | Acc: 10.262% (431/4200)\n",
            "Loss: 6.579 | Acc: 10.163% (437/4300)\n",
            "Loss: 6.592 | Acc: 10.136% (446/4400)\n",
            "Loss: 6.547 | Acc: 10.244% (461/4500)\n",
            "Loss: 6.509 | Acc: 10.348% (476/4600)\n",
            "Loss: 6.519 | Acc: 10.340% (486/4700)\n",
            "Loss: 6.519 | Acc: 10.333% (496/4800)\n",
            "Loss: 6.567 | Acc: 10.408% (510/4900)\n",
            "Loss: 6.548 | Acc: 10.480% (524/5000)\n",
            "Loss: 6.507 | Acc: 10.490% (535/5100)\n",
            "Loss: 6.462 | Acc: 10.462% (544/5200)\n",
            "Loss: 6.455 | Acc: 10.528% (558/5300)\n",
            "Loss: 6.500 | Acc: 10.537% (569/5400)\n",
            "Loss: 6.582 | Acc: 10.545% (580/5500)\n",
            "Loss: 6.531 | Acc: 10.643% (596/5600)\n",
            "Loss: 6.527 | Acc: 10.632% (606/5700)\n",
            "Loss: 6.567 | Acc: 10.603% (615/5800)\n",
            "Loss: 6.529 | Acc: 10.610% (626/5900)\n",
            "Loss: 6.501 | Acc: 10.567% (634/6000)\n",
            "Loss: 6.458 | Acc: 10.590% (646/6100)\n",
            "Loss: 6.432 | Acc: 10.629% (659/6200)\n",
            "Loss: 6.402 | Acc: 10.619% (669/6300)\n",
            "Loss: 6.369 | Acc: 10.656% (682/6400)\n",
            "Loss: 6.353 | Acc: 10.631% (691/6500)\n",
            "Loss: 6.320 | Acc: 10.591% (699/6600)\n",
            "Loss: 6.381 | Acc: 10.522% (705/6700)\n",
            "Loss: 6.388 | Acc: 10.574% (719/6800)\n",
            "Loss: 6.359 | Acc: 10.565% (729/6900)\n",
            "Loss: 6.411 | Acc: 10.557% (739/7000)\n",
            "Loss: 6.386 | Acc: 10.563% (750/7100)\n",
            "Loss: 6.358 | Acc: 10.528% (758/7200)\n",
            "Loss: 6.346 | Acc: 10.548% (770/7300)\n",
            "Loss: 6.317 | Acc: 10.568% (782/7400)\n",
            "Loss: 6.331 | Acc: 10.587% (794/7500)\n",
            "Loss: 6.411 | Acc: 10.592% (805/7600)\n",
            "Loss: 6.435 | Acc: 10.506% (809/7700)\n",
            "Loss: 6.423 | Acc: 10.538% (822/7800)\n",
            "Loss: 6.389 | Acc: 10.582% (836/7900)\n",
            "Loss: 6.368 | Acc: 10.588% (847/8000)\n",
            "Loss: 6.352 | Acc: 10.568% (856/8100)\n",
            "Loss: 6.335 | Acc: 10.598% (869/8200)\n",
            "Loss: 6.316 | Acc: 10.711% (889/8300)\n",
            "Loss: 6.325 | Acc: 10.738% (902/8400)\n",
            "Loss: 6.338 | Acc: 10.729% (912/8500)\n",
            "Loss: 6.325 | Acc: 10.733% (923/8600)\n",
            "Loss: 6.354 | Acc: 10.770% (937/8700)\n",
            "Loss: 6.332 | Acc: 10.750% (946/8800)\n",
            "Loss: 6.362 | Acc: 10.742% (956/8900)\n",
            "Loss: 6.347 | Acc: 10.689% (962/9000)\n",
            "Loss: 6.331 | Acc: 10.681% (972/9100)\n",
            "Loss: 6.313 | Acc: 10.685% (983/9200)\n",
            "Loss: 6.305 | Acc: 10.667% (992/9300)\n",
            "Loss: 6.321 | Acc: 10.670% (1003/9400)\n",
            "Loss: 6.300 | Acc: 10.695% (1016/9500)\n",
            "Loss: 6.303 | Acc: 10.667% (1024/9600)\n",
            "Loss: 6.323 | Acc: 10.629% (1031/9700)\n",
            "Loss: 6.306 | Acc: 10.602% (1039/9800)\n",
            "Loss: 6.290 | Acc: 10.556% (1045/9900)\n",
            "Loss: 6.279 | Acc: 10.610% (1061/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 11\n",
            "Loss: 21.944 | Acc: 10.000% (1/10)\n",
            "Loss: 24.572 | Acc: 10.000% (2/20)\n",
            "Loss: 18.770 | Acc: 13.333% (4/30)\n",
            "Loss: 15.486 | Acc: 10.000% (4/40)\n",
            "Loss: 13.768 | Acc: 10.000% (5/50)\n",
            "Loss: 13.125 | Acc: 10.000% (6/60)\n",
            "Loss: 11.981 | Acc: 10.000% (7/70)\n",
            "Loss: 13.245 | Acc: 12.500% (10/80)\n",
            "Loss: 13.386 | Acc: 11.111% (10/90)\n",
            "Loss: 16.990 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 11\n",
            "Loss: 3.198 | Acc: 16.000% (16/100)\n",
            "Loss: 3.234 | Acc: 13.000% (26/200)\n",
            "Loss: 3.477 | Acc: 11.333% (34/300)\n",
            "Loss: 3.396 | Acc: 12.000% (48/400)\n",
            "Loss: 3.429 | Acc: 10.800% (54/500)\n",
            "Loss: 3.477 | Acc: 11.667% (70/600)\n",
            "Loss: 3.511 | Acc: 11.857% (83/700)\n",
            "Loss: 3.495 | Acc: 11.875% (95/800)\n",
            "Loss: 3.492 | Acc: 11.111% (100/900)\n",
            "Loss: 3.490 | Acc: 11.200% (112/1000)\n",
            "Loss: 3.436 | Acc: 11.909% (131/1100)\n",
            "Loss: 3.406 | Acc: 11.750% (141/1200)\n",
            "Loss: 3.402 | Acc: 11.385% (148/1300)\n",
            "Loss: 3.450 | Acc: 11.214% (157/1400)\n",
            "Loss: 3.442 | Acc: 11.200% (168/1500)\n",
            "Loss: 3.493 | Acc: 11.062% (177/1600)\n",
            "Loss: 3.484 | Acc: 11.118% (189/1700)\n",
            "Loss: 3.536 | Acc: 10.889% (196/1800)\n",
            "Loss: 3.575 | Acc: 10.684% (203/1900)\n",
            "Loss: 3.564 | Acc: 10.800% (216/2000)\n",
            "Loss: 3.691 | Acc: 10.619% (223/2100)\n",
            "Loss: 3.718 | Acc: 10.500% (231/2200)\n",
            "Loss: 3.702 | Acc: 10.391% (239/2300)\n",
            "Loss: 3.692 | Acc: 10.417% (250/2400)\n",
            "Loss: 3.664 | Acc: 10.640% (266/2500)\n",
            "Loss: 3.668 | Acc: 10.577% (275/2600)\n",
            "Loss: 3.655 | Acc: 10.556% (285/2700)\n",
            "Loss: 3.653 | Acc: 10.429% (292/2800)\n",
            "Loss: 3.680 | Acc: 10.483% (304/2900)\n",
            "Loss: 3.668 | Acc: 10.233% (307/3000)\n",
            "Loss: 3.660 | Acc: 10.194% (316/3100)\n",
            "Loss: 3.688 | Acc: 10.125% (324/3200)\n",
            "Loss: 3.693 | Acc: 10.000% (330/3300)\n",
            "Loss: 3.738 | Acc: 9.912% (337/3400)\n",
            "Loss: 3.851 | Acc: 9.971% (349/3500)\n",
            "Loss: 3.829 | Acc: 10.028% (361/3600)\n",
            "Loss: 3.807 | Acc: 9.973% (369/3700)\n",
            "Loss: 3.874 | Acc: 10.000% (380/3800)\n",
            "Loss: 3.887 | Acc: 9.949% (388/3900)\n",
            "Loss: 3.876 | Acc: 9.925% (397/4000)\n",
            "Loss: 3.871 | Acc: 9.902% (406/4100)\n",
            "Loss: 3.897 | Acc: 9.929% (417/4200)\n",
            "Loss: 3.882 | Acc: 9.884% (425/4300)\n",
            "Loss: 3.890 | Acc: 9.886% (435/4400)\n",
            "Loss: 3.887 | Acc: 9.822% (442/4500)\n",
            "Loss: 3.881 | Acc: 9.739% (448/4600)\n",
            "Loss: 3.919 | Acc: 9.766% (459/4700)\n",
            "Loss: 3.923 | Acc: 9.938% (477/4800)\n",
            "Loss: 3.914 | Acc: 9.918% (486/4900)\n",
            "Loss: 3.910 | Acc: 9.860% (493/5000)\n",
            "Loss: 3.903 | Acc: 9.863% (503/5100)\n",
            "Loss: 3.892 | Acc: 9.962% (518/5200)\n",
            "Loss: 3.893 | Acc: 9.906% (525/5300)\n",
            "Loss: 3.918 | Acc: 9.852% (532/5400)\n",
            "Loss: 3.957 | Acc: 9.945% (547/5500)\n",
            "Loss: 3.944 | Acc: 10.036% (562/5600)\n",
            "Loss: 3.943 | Acc: 10.035% (572/5700)\n",
            "Loss: 3.937 | Acc: 10.069% (584/5800)\n",
            "Loss: 3.929 | Acc: 10.085% (595/5900)\n",
            "Loss: 3.922 | Acc: 10.067% (604/6000)\n",
            "Loss: 3.913 | Acc: 10.049% (613/6100)\n",
            "Loss: 3.904 | Acc: 9.952% (617/6200)\n",
            "Loss: 3.897 | Acc: 9.952% (627/6300)\n",
            "Loss: 3.892 | Acc: 9.953% (637/6400)\n",
            "Loss: 3.879 | Acc: 9.938% (646/6500)\n",
            "Loss: 3.871 | Acc: 9.985% (659/6600)\n",
            "Loss: 3.866 | Acc: 9.985% (669/6700)\n",
            "Loss: 3.856 | Acc: 9.926% (675/6800)\n",
            "Loss: 3.845 | Acc: 9.986% (689/6900)\n",
            "Loss: 3.845 | Acc: 10.000% (700/7000)\n",
            "Loss: 3.837 | Acc: 9.972% (708/7100)\n",
            "Loss: 3.833 | Acc: 9.986% (719/7200)\n",
            "Loss: 3.832 | Acc: 9.904% (723/7300)\n",
            "Loss: 3.823 | Acc: 9.932% (735/7400)\n",
            "Loss: 3.832 | Acc: 9.907% (743/7500)\n",
            "Loss: 3.840 | Acc: 9.908% (753/7600)\n",
            "Loss: 3.835 | Acc: 9.909% (763/7700)\n",
            "Loss: 3.853 | Acc: 9.885% (771/7800)\n",
            "Loss: 3.844 | Acc: 9.899% (782/7900)\n",
            "Loss: 3.836 | Acc: 9.912% (793/8000)\n",
            "Loss: 3.837 | Acc: 9.877% (800/8100)\n",
            "Loss: 3.828 | Acc: 9.927% (814/8200)\n",
            "Loss: 3.820 | Acc: 9.904% (822/8300)\n",
            "Loss: 3.813 | Acc: 9.940% (835/8400)\n",
            "Loss: 3.806 | Acc: 9.929% (844/8500)\n",
            "Loss: 3.805 | Acc: 10.012% (861/8600)\n",
            "Loss: 3.825 | Acc: 10.034% (873/8700)\n",
            "Loss: 3.820 | Acc: 10.080% (887/8800)\n",
            "Loss: 3.811 | Acc: 10.101% (899/8900)\n",
            "Loss: 3.812 | Acc: 10.156% (914/9000)\n",
            "Loss: 3.807 | Acc: 10.121% (921/9100)\n",
            "Loss: 3.801 | Acc: 10.098% (929/9200)\n",
            "Loss: 3.801 | Acc: 10.065% (936/9300)\n",
            "Loss: 3.817 | Acc: 10.053% (945/9400)\n",
            "Loss: 3.811 | Acc: 10.074% (957/9500)\n",
            "Loss: 3.806 | Acc: 10.115% (971/9600)\n",
            "Loss: 3.811 | Acc: 10.144% (984/9700)\n",
            "Loss: 3.805 | Acc: 10.153% (995/9800)\n",
            "Loss: 3.802 | Acc: 10.121% (1002/9900)\n",
            "Loss: 3.799 | Acc: 10.070% (1007/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 12\n",
            "Loss: 45.042 | Acc: 30.000% (3/10)\n",
            "Loss: 23.951 | Acc: 25.000% (5/20)\n",
            "Loss: 17.881 | Acc: 20.000% (6/30)\n",
            "Loss: 17.885 | Acc: 20.000% (8/40)\n",
            "Loss: 16.757 | Acc: 16.000% (8/50)\n",
            "Loss: 14.456 | Acc: 16.667% (10/60)\n",
            "Loss: 13.631 | Acc: 14.286% (10/70)\n",
            "Loss: 12.967 | Acc: 13.750% (11/80)\n",
            "Loss: 12.434 | Acc: 13.333% (12/90)\n",
            "Loss: 11.552 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 12\n",
            "Loss: 3.108 | Acc: 10.000% (10/100)\n",
            "Loss: 3.226 | Acc: 9.000% (18/200)\n",
            "Loss: 3.328 | Acc: 9.000% (27/300)\n",
            "Loss: 3.247 | Acc: 9.000% (36/400)\n",
            "Loss: 3.210 | Acc: 9.400% (47/500)\n",
            "Loss: 3.329 | Acc: 9.167% (55/600)\n",
            "Loss: 3.445 | Acc: 9.857% (69/700)\n",
            "Loss: 3.417 | Acc: 9.625% (77/800)\n",
            "Loss: 3.364 | Acc: 10.222% (92/900)\n",
            "Loss: 3.493 | Acc: 10.200% (102/1000)\n",
            "Loss: 3.441 | Acc: 10.364% (114/1100)\n",
            "Loss: 3.424 | Acc: 10.500% (126/1200)\n",
            "Loss: 3.438 | Acc: 10.385% (135/1300)\n",
            "Loss: 3.551 | Acc: 10.214% (143/1400)\n",
            "Loss: 3.540 | Acc: 10.000% (150/1500)\n",
            "Loss: 3.601 | Acc: 10.000% (160/1600)\n",
            "Loss: 3.573 | Acc: 9.941% (169/1700)\n",
            "Loss: 3.585 | Acc: 9.889% (178/1800)\n",
            "Loss: 3.582 | Acc: 9.842% (187/1900)\n",
            "Loss: 3.588 | Acc: 9.900% (198/2000)\n",
            "Loss: 3.818 | Acc: 9.905% (208/2100)\n",
            "Loss: 3.838 | Acc: 9.773% (215/2200)\n",
            "Loss: 3.803 | Acc: 9.870% (227/2300)\n",
            "Loss: 3.775 | Acc: 9.667% (232/2400)\n",
            "Loss: 3.754 | Acc: 9.600% (240/2500)\n",
            "Loss: 3.784 | Acc: 9.538% (248/2600)\n",
            "Loss: 3.751 | Acc: 9.593% (259/2700)\n",
            "Loss: 3.723 | Acc: 9.643% (270/2800)\n",
            "Loss: 3.730 | Acc: 9.724% (282/2900)\n",
            "Loss: 3.715 | Acc: 9.700% (291/3000)\n",
            "Loss: 3.698 | Acc: 9.774% (303/3100)\n",
            "Loss: 3.756 | Acc: 9.719% (311/3200)\n",
            "Loss: 3.740 | Acc: 9.788% (323/3300)\n",
            "Loss: 3.789 | Acc: 9.765% (332/3400)\n",
            "Loss: 3.898 | Acc: 9.686% (339/3500)\n",
            "Loss: 3.875 | Acc: 9.667% (348/3600)\n",
            "Loss: 3.850 | Acc: 9.757% (361/3700)\n",
            "Loss: 3.953 | Acc: 9.737% (370/3800)\n",
            "Loss: 3.984 | Acc: 9.667% (377/3900)\n",
            "Loss: 3.962 | Acc: 9.625% (385/4000)\n",
            "Loss: 3.944 | Acc: 9.610% (394/4100)\n",
            "Loss: 3.981 | Acc: 9.667% (406/4200)\n",
            "Loss: 3.965 | Acc: 9.558% (411/4300)\n",
            "Loss: 3.963 | Acc: 9.545% (420/4400)\n",
            "Loss: 3.980 | Acc: 9.644% (434/4500)\n",
            "Loss: 3.972 | Acc: 9.739% (448/4600)\n",
            "Loss: 4.028 | Acc: 9.766% (459/4700)\n",
            "Loss: 4.058 | Acc: 9.750% (468/4800)\n",
            "Loss: 4.042 | Acc: 9.796% (480/4900)\n",
            "Loss: 4.021 | Acc: 9.880% (494/5000)\n",
            "Loss: 4.000 | Acc: 9.824% (501/5100)\n",
            "Loss: 3.977 | Acc: 9.827% (511/5200)\n",
            "Loss: 3.975 | Acc: 9.868% (523/5300)\n",
            "Loss: 4.003 | Acc: 9.889% (534/5400)\n",
            "Loss: 4.059 | Acc: 9.891% (544/5500)\n",
            "Loss: 4.034 | Acc: 10.000% (560/5600)\n",
            "Loss: 4.020 | Acc: 9.982% (569/5700)\n",
            "Loss: 4.010 | Acc: 9.966% (578/5800)\n",
            "Loss: 3.992 | Acc: 9.983% (589/5900)\n",
            "Loss: 3.974 | Acc: 9.950% (597/6000)\n",
            "Loss: 3.957 | Acc: 9.951% (607/6100)\n",
            "Loss: 3.942 | Acc: 10.000% (620/6200)\n",
            "Loss: 3.929 | Acc: 10.000% (630/6300)\n",
            "Loss: 3.914 | Acc: 10.016% (641/6400)\n",
            "Loss: 3.911 | Acc: 9.969% (648/6500)\n",
            "Loss: 3.896 | Acc: 9.939% (656/6600)\n",
            "Loss: 3.881 | Acc: 9.881% (662/6700)\n",
            "Loss: 3.875 | Acc: 9.941% (676/6800)\n",
            "Loss: 3.861 | Acc: 9.942% (686/6900)\n",
            "Loss: 3.853 | Acc: 9.929% (695/7000)\n",
            "Loss: 3.842 | Acc: 9.930% (705/7100)\n",
            "Loss: 3.830 | Acc: 9.875% (711/7200)\n",
            "Loss: 3.819 | Acc: 9.932% (725/7300)\n",
            "Loss: 3.805 | Acc: 9.986% (739/7400)\n",
            "Loss: 3.817 | Acc: 10.000% (750/7500)\n",
            "Loss: 3.832 | Acc: 10.000% (760/7600)\n",
            "Loss: 3.824 | Acc: 9.922% (764/7700)\n",
            "Loss: 3.839 | Acc: 9.949% (776/7800)\n",
            "Loss: 3.826 | Acc: 9.975% (788/7900)\n",
            "Loss: 3.816 | Acc: 9.988% (799/8000)\n",
            "Loss: 3.808 | Acc: 9.975% (808/8100)\n",
            "Loss: 3.804 | Acc: 10.012% (821/8200)\n",
            "Loss: 3.798 | Acc: 10.133% (841/8300)\n",
            "Loss: 3.789 | Acc: 10.179% (855/8400)\n",
            "Loss: 3.780 | Acc: 10.176% (865/8500)\n",
            "Loss: 3.798 | Acc: 10.174% (875/8600)\n",
            "Loss: 3.836 | Acc: 10.218% (889/8700)\n",
            "Loss: 3.826 | Acc: 10.205% (898/8800)\n",
            "Loss: 3.819 | Acc: 10.180% (906/8900)\n",
            "Loss: 3.831 | Acc: 10.133% (912/9000)\n",
            "Loss: 3.823 | Acc: 10.088% (918/9100)\n",
            "Loss: 3.815 | Acc: 10.098% (929/9200)\n",
            "Loss: 3.811 | Acc: 10.097% (939/9300)\n",
            "Loss: 3.826 | Acc: 10.085% (948/9400)\n",
            "Loss: 3.818 | Acc: 10.084% (958/9500)\n",
            "Loss: 3.809 | Acc: 10.042% (964/9600)\n",
            "Loss: 3.818 | Acc: 10.000% (970/9700)\n",
            "Loss: 3.811 | Acc: 9.980% (978/9800)\n",
            "Loss: 3.805 | Acc: 9.929% (983/9900)\n",
            "Loss: 3.799 | Acc: 9.960% (996/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 13\n",
            "Loss: 2.291 | Acc: 20.000% (2/10)\n",
            "Loss: 8.966 | Acc: 10.000% (2/20)\n",
            "Loss: 7.212 | Acc: 6.667% (2/30)\n",
            "Loss: 12.144 | Acc: 7.500% (3/40)\n",
            "Loss: 13.111 | Acc: 6.000% (3/50)\n",
            "Loss: 17.000 | Acc: 6.667% (4/60)\n",
            "Loss: 15.712 | Acc: 7.143% (5/70)\n",
            "Loss: 15.461 | Acc: 7.500% (6/80)\n",
            "Loss: 15.790 | Acc: 6.667% (6/90)\n",
            "Loss: 14.912 | Acc: 6.000% (6/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 13\n",
            "Loss: 3.286 | Acc: 8.000% (8/100)\n",
            "Loss: 3.312 | Acc: 11.000% (22/200)\n",
            "Loss: 3.379 | Acc: 10.000% (30/300)\n",
            "Loss: 3.331 | Acc: 11.250% (45/400)\n",
            "Loss: 3.359 | Acc: 11.000% (55/500)\n",
            "Loss: 3.363 | Acc: 10.333% (62/600)\n",
            "Loss: 3.444 | Acc: 10.143% (71/700)\n",
            "Loss: 3.427 | Acc: 10.625% (85/800)\n",
            "Loss: 3.432 | Acc: 10.667% (96/900)\n",
            "Loss: 3.472 | Acc: 10.400% (104/1000)\n",
            "Loss: 3.474 | Acc: 10.182% (112/1100)\n",
            "Loss: 3.480 | Acc: 10.083% (121/1200)\n",
            "Loss: 3.505 | Acc: 10.000% (130/1300)\n",
            "Loss: 3.597 | Acc: 10.143% (142/1400)\n",
            "Loss: 3.593 | Acc: 9.933% (149/1500)\n",
            "Loss: 3.609 | Acc: 10.000% (160/1600)\n",
            "Loss: 3.593 | Acc: 9.824% (167/1700)\n",
            "Loss: 3.643 | Acc: 9.889% (178/1800)\n",
            "Loss: 3.627 | Acc: 9.947% (189/1900)\n",
            "Loss: 3.626 | Acc: 9.850% (197/2000)\n",
            "Loss: 3.668 | Acc: 10.048% (211/2100)\n",
            "Loss: 3.665 | Acc: 10.182% (224/2200)\n",
            "Loss: 3.659 | Acc: 10.174% (234/2300)\n",
            "Loss: 3.670 | Acc: 10.375% (249/2400)\n",
            "Loss: 3.658 | Acc: 10.320% (258/2500)\n",
            "Loss: 3.648 | Acc: 10.577% (275/2600)\n",
            "Loss: 3.644 | Acc: 10.593% (286/2700)\n",
            "Loss: 3.627 | Acc: 10.643% (298/2800)\n",
            "Loss: 3.631 | Acc: 10.621% (308/2900)\n",
            "Loss: 3.631 | Acc: 10.700% (321/3000)\n",
            "Loss: 3.630 | Acc: 10.645% (330/3100)\n",
            "Loss: 3.626 | Acc: 10.562% (338/3200)\n",
            "Loss: 3.625 | Acc: 10.364% (342/3300)\n",
            "Loss: 3.625 | Acc: 10.441% (355/3400)\n",
            "Loss: 3.629 | Acc: 10.514% (368/3500)\n",
            "Loss: 3.623 | Acc: 10.583% (381/3600)\n",
            "Loss: 3.619 | Acc: 10.568% (391/3700)\n",
            "Loss: 3.638 | Acc: 10.684% (406/3800)\n",
            "Loss: 3.637 | Acc: 10.641% (415/3900)\n",
            "Loss: 3.627 | Acc: 10.700% (428/4000)\n",
            "Loss: 3.622 | Acc: 10.610% (435/4100)\n",
            "Loss: 3.630 | Acc: 10.595% (445/4200)\n",
            "Loss: 3.629 | Acc: 10.605% (456/4300)\n",
            "Loss: 3.634 | Acc: 10.523% (463/4400)\n",
            "Loss: 3.646 | Acc: 10.533% (474/4500)\n",
            "Loss: 3.648 | Acc: 10.478% (482/4600)\n",
            "Loss: 3.660 | Acc: 10.447% (491/4700)\n",
            "Loss: 3.666 | Acc: 10.396% (499/4800)\n",
            "Loss: 3.665 | Acc: 10.367% (508/4900)\n",
            "Loss: 3.662 | Acc: 10.260% (513/5000)\n",
            "Loss: 3.654 | Acc: 10.235% (522/5100)\n",
            "Loss: 3.645 | Acc: 10.231% (532/5200)\n",
            "Loss: 3.645 | Acc: 10.208% (541/5300)\n",
            "Loss: 3.655 | Acc: 10.130% (547/5400)\n",
            "Loss: 3.662 | Acc: 10.145% (558/5500)\n",
            "Loss: 3.656 | Acc: 10.107% (566/5600)\n",
            "Loss: 3.652 | Acc: 10.070% (574/5700)\n",
            "Loss: 3.676 | Acc: 10.155% (589/5800)\n",
            "Loss: 3.672 | Acc: 10.119% (597/5900)\n",
            "Loss: 3.667 | Acc: 10.017% (601/6000)\n",
            "Loss: 3.661 | Acc: 10.049% (613/6100)\n",
            "Loss: 3.661 | Acc: 10.065% (624/6200)\n",
            "Loss: 3.656 | Acc: 10.095% (636/6300)\n",
            "Loss: 3.649 | Acc: 10.078% (645/6400)\n",
            "Loss: 3.661 | Acc: 10.046% (653/6500)\n",
            "Loss: 3.655 | Acc: 10.091% (666/6600)\n",
            "Loss: 3.648 | Acc: 10.090% (676/6700)\n",
            "Loss: 3.647 | Acc: 10.147% (690/6800)\n",
            "Loss: 3.644 | Acc: 10.101% (697/6900)\n",
            "Loss: 3.641 | Acc: 10.086% (706/7000)\n",
            "Loss: 3.638 | Acc: 10.141% (720/7100)\n",
            "Loss: 3.636 | Acc: 10.139% (730/7200)\n",
            "Loss: 3.644 | Acc: 10.123% (739/7300)\n",
            "Loss: 3.642 | Acc: 10.149% (751/7400)\n",
            "Loss: 3.641 | Acc: 10.133% (760/7500)\n",
            "Loss: 3.644 | Acc: 10.158% (772/7600)\n",
            "Loss: 3.637 | Acc: 10.208% (786/7700)\n",
            "Loss: 3.638 | Acc: 10.192% (795/7800)\n",
            "Loss: 3.633 | Acc: 10.241% (809/7900)\n",
            "Loss: 3.630 | Acc: 10.287% (823/8000)\n",
            "Loss: 3.627 | Acc: 10.284% (833/8100)\n",
            "Loss: 3.628 | Acc: 10.195% (836/8200)\n",
            "Loss: 3.630 | Acc: 10.157% (843/8300)\n",
            "Loss: 3.630 | Acc: 10.119% (850/8400)\n",
            "Loss: 3.626 | Acc: 10.153% (863/8500)\n",
            "Loss: 3.631 | Acc: 10.105% (869/8600)\n",
            "Loss: 3.639 | Acc: 10.092% (878/8700)\n",
            "Loss: 3.634 | Acc: 10.125% (891/8800)\n",
            "Loss: 3.633 | Acc: 10.135% (902/8900)\n",
            "Loss: 3.634 | Acc: 10.133% (912/9000)\n",
            "Loss: 3.628 | Acc: 10.209% (929/9100)\n",
            "Loss: 3.627 | Acc: 10.196% (938/9200)\n",
            "Loss: 3.626 | Acc: 10.151% (944/9300)\n",
            "Loss: 3.625 | Acc: 10.128% (952/9400)\n",
            "Loss: 3.622 | Acc: 10.137% (963/9500)\n",
            "Loss: 3.618 | Acc: 10.104% (970/9600)\n",
            "Loss: 3.639 | Acc: 10.113% (981/9700)\n",
            "Loss: 3.635 | Acc: 10.133% (993/9800)\n",
            "Loss: 3.632 | Acc: 10.152% (1005/9900)\n",
            "Loss: 3.631 | Acc: 10.140% (1014/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 14\n",
            "Loss: 11.027 | Acc: 0.000% (0/10)\n",
            "Loss: 20.814 | Acc: 0.000% (0/20)\n",
            "Loss: 15.241 | Acc: 3.333% (1/30)\n",
            "Loss: 15.312 | Acc: 7.500% (3/40)\n",
            "Loss: 15.472 | Acc: 6.000% (3/50)\n",
            "Loss: 15.417 | Acc: 5.000% (3/60)\n",
            "Loss: 14.484 | Acc: 4.286% (3/70)\n",
            "Loss: 14.274 | Acc: 5.000% (4/80)\n",
            "Loss: 15.773 | Acc: 8.889% (8/90)\n",
            "Loss: 15.476 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 14\n",
            "Loss: 75.822 | Acc: 8.000% (8/100)\n",
            "Loss: 91.627 | Acc: 9.000% (18/200)\n",
            "Loss: 178.536 | Acc: 9.333% (28/300)\n",
            "Loss: 146.309 | Acc: 8.750% (35/400)\n",
            "Loss: 141.813 | Acc: 9.000% (45/500)\n",
            "Loss: 138.976 | Acc: 9.333% (56/600)\n",
            "Loss: 123.863 | Acc: 9.286% (65/700)\n",
            "Loss: 114.777 | Acc: 10.125% (81/800)\n",
            "Loss: 114.745 | Acc: 10.444% (94/900)\n",
            "Loss: 113.578 | Acc: 10.400% (104/1000)\n",
            "Loss: 115.114 | Acc: 10.182% (112/1100)\n",
            "Loss: 112.639 | Acc: 10.000% (120/1200)\n",
            "Loss: 122.934 | Acc: 10.308% (134/1300)\n",
            "Loss: 134.831 | Acc: 10.643% (149/1400)\n",
            "Loss: 134.945 | Acc: 10.867% (163/1500)\n",
            "Loss: 131.412 | Acc: 11.750% (188/1600)\n",
            "Loss: 139.628 | Acc: 11.706% (199/1700)\n",
            "Loss: 148.534 | Acc: 11.889% (214/1800)\n",
            "Loss: 151.329 | Acc: 11.842% (225/1900)\n",
            "Loss: 150.926 | Acc: 11.950% (239/2000)\n",
            "Loss: 154.328 | Acc: 11.857% (249/2100)\n",
            "Loss: 151.628 | Acc: 11.864% (261/2200)\n",
            "Loss: 147.174 | Acc: 12.000% (276/2300)\n",
            "Loss: 149.829 | Acc: 11.917% (286/2400)\n",
            "Loss: 149.114 | Acc: 11.880% (297/2500)\n",
            "Loss: 145.365 | Acc: 12.077% (314/2600)\n",
            "Loss: 143.919 | Acc: 12.296% (332/2700)\n",
            "Loss: 153.595 | Acc: 12.250% (343/2800)\n",
            "Loss: 157.826 | Acc: 12.276% (356/2900)\n",
            "Loss: 155.566 | Acc: 12.367% (371/3000)\n",
            "Loss: 159.173 | Acc: 12.419% (385/3100)\n",
            "Loss: 157.337 | Acc: 12.375% (396/3200)\n",
            "Loss: 155.246 | Acc: 12.424% (410/3300)\n",
            "Loss: 153.476 | Acc: 12.412% (422/3400)\n",
            "Loss: 152.471 | Acc: 12.486% (437/3500)\n",
            "Loss: 151.102 | Acc: 12.583% (453/3600)\n",
            "Loss: 153.558 | Acc: 12.649% (468/3700)\n",
            "Loss: 155.912 | Acc: 12.711% (483/3800)\n",
            "Loss: 154.586 | Acc: 12.564% (490/3900)\n",
            "Loss: 157.539 | Acc: 12.550% (502/4000)\n",
            "Loss: 157.818 | Acc: 12.561% (515/4100)\n",
            "Loss: 158.809 | Acc: 12.548% (527/4200)\n",
            "Loss: 162.704 | Acc: 12.581% (541/4300)\n",
            "Loss: 163.937 | Acc: 12.614% (555/4400)\n",
            "Loss: 163.163 | Acc: 12.578% (566/4500)\n",
            "Loss: 162.626 | Acc: 12.652% (582/4600)\n",
            "Loss: 163.706 | Acc: 12.766% (600/4700)\n",
            "Loss: 166.492 | Acc: 12.750% (612/4800)\n",
            "Loss: 164.679 | Acc: 12.694% (622/4900)\n",
            "Loss: 167.105 | Acc: 12.660% (633/5000)\n",
            "Loss: 166.930 | Acc: 12.627% (644/5100)\n",
            "Loss: 168.512 | Acc: 12.654% (658/5200)\n",
            "Loss: 169.173 | Acc: 12.736% (675/5300)\n",
            "Loss: 169.630 | Acc: 12.704% (686/5400)\n",
            "Loss: 171.946 | Acc: 12.673% (697/5500)\n",
            "Loss: 171.639 | Acc: 12.643% (708/5600)\n",
            "Loss: 175.735 | Acc: 12.702% (724/5700)\n",
            "Loss: 175.751 | Acc: 12.655% (734/5800)\n",
            "Loss: 173.992 | Acc: 12.661% (747/5900)\n",
            "Loss: 172.067 | Acc: 12.750% (765/6000)\n",
            "Loss: 170.968 | Acc: 12.738% (777/6100)\n",
            "Loss: 172.163 | Acc: 12.694% (787/6200)\n",
            "Loss: 173.714 | Acc: 12.635% (796/6300)\n",
            "Loss: 174.596 | Acc: 12.562% (804/6400)\n",
            "Loss: 177.483 | Acc: 12.600% (819/6500)\n",
            "Loss: 177.504 | Acc: 12.561% (829/6600)\n",
            "Loss: 176.655 | Acc: 12.642% (847/6700)\n",
            "Loss: 175.854 | Acc: 12.574% (855/6800)\n",
            "Loss: 174.597 | Acc: 12.565% (867/6900)\n",
            "Loss: 173.830 | Acc: 12.586% (881/7000)\n",
            "Loss: 172.099 | Acc: 12.620% (896/7100)\n",
            "Loss: 172.652 | Acc: 12.639% (910/7200)\n",
            "Loss: 172.094 | Acc: 12.671% (925/7300)\n",
            "Loss: 172.941 | Acc: 12.797% (947/7400)\n",
            "Loss: 173.987 | Acc: 12.787% (959/7500)\n",
            "Loss: 174.552 | Acc: 12.763% (970/7600)\n",
            "Loss: 176.421 | Acc: 12.675% (976/7700)\n",
            "Loss: 176.494 | Acc: 12.679% (989/7800)\n",
            "Loss: 175.991 | Acc: 12.658% (1000/7900)\n",
            "Loss: 175.184 | Acc: 12.575% (1006/8000)\n",
            "Loss: 173.499 | Acc: 12.617% (1022/8100)\n",
            "Loss: 173.674 | Acc: 12.585% (1032/8200)\n",
            "Loss: 172.299 | Acc: 12.530% (1040/8300)\n",
            "Loss: 172.799 | Acc: 12.595% (1058/8400)\n",
            "Loss: 172.170 | Acc: 12.588% (1070/8500)\n",
            "Loss: 172.708 | Acc: 12.581% (1082/8600)\n",
            "Loss: 172.646 | Acc: 12.586% (1095/8700)\n",
            "Loss: 172.724 | Acc: 12.568% (1106/8800)\n",
            "Loss: 173.682 | Acc: 12.596% (1121/8900)\n",
            "Loss: 173.385 | Acc: 12.589% (1133/9000)\n",
            "Loss: 173.006 | Acc: 12.549% (1142/9100)\n",
            "Loss: 172.232 | Acc: 12.598% (1159/9200)\n",
            "Loss: 172.333 | Acc: 12.495% (1162/9300)\n",
            "Loss: 171.287 | Acc: 12.479% (1173/9400)\n",
            "Loss: 170.977 | Acc: 12.463% (1184/9500)\n",
            "Loss: 170.861 | Acc: 12.490% (1199/9600)\n",
            "Loss: 170.286 | Acc: 12.464% (1209/9700)\n",
            "Loss: 169.644 | Acc: 12.469% (1222/9800)\n",
            "Loss: 168.719 | Acc: 12.455% (1233/9900)\n",
            "Loss: 170.725 | Acc: 12.470% (1247/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 15\n",
            "Loss: 23.630 | Acc: 0.000% (0/10)\n",
            "Loss: 13.510 | Acc: 10.000% (2/20)\n",
            "Loss: 10.222 | Acc: 10.000% (3/30)\n",
            "Loss: 12.661 | Acc: 7.500% (3/40)\n",
            "Loss: 12.567 | Acc: 10.000% (5/50)\n",
            "Loss: 15.275 | Acc: 11.667% (7/60)\n",
            "Loss: 13.571 | Acc: 10.000% (7/70)\n",
            "Loss: 13.186 | Acc: 10.000% (8/80)\n",
            "Loss: 12.670 | Acc: 10.000% (9/90)\n",
            "Loss: 11.703 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 15\n",
            "Loss: 256.516 | Acc: 10.000% (10/100)\n",
            "Loss: 349.474 | Acc: 11.500% (23/200)\n",
            "Loss: 446.324 | Acc: 13.000% (39/300)\n",
            "Loss: 417.577 | Acc: 12.750% (51/400)\n",
            "Loss: 406.145 | Acc: 13.200% (66/500)\n",
            "Loss: 391.002 | Acc: 13.167% (79/600)\n",
            "Loss: 379.837 | Acc: 13.000% (91/700)\n",
            "Loss: 374.274 | Acc: 13.250% (106/800)\n",
            "Loss: 386.962 | Acc: 12.444% (112/900)\n",
            "Loss: 377.541 | Acc: 12.100% (121/1000)\n",
            "Loss: 381.543 | Acc: 11.909% (131/1100)\n",
            "Loss: 391.407 | Acc: 11.667% (140/1200)\n",
            "Loss: 406.412 | Acc: 11.615% (151/1300)\n",
            "Loss: 411.218 | Acc: 11.857% (166/1400)\n",
            "Loss: 411.090 | Acc: 11.867% (178/1500)\n",
            "Loss: 410.853 | Acc: 11.750% (188/1600)\n",
            "Loss: 408.842 | Acc: 12.000% (204/1700)\n",
            "Loss: 417.825 | Acc: 11.722% (211/1800)\n",
            "Loss: 417.891 | Acc: 11.895% (226/1900)\n",
            "Loss: 417.866 | Acc: 11.800% (236/2000)\n",
            "Loss: 421.544 | Acc: 11.714% (246/2100)\n",
            "Loss: 414.470 | Acc: 11.955% (263/2200)\n",
            "Loss: 415.342 | Acc: 12.043% (277/2300)\n",
            "Loss: 414.221 | Acc: 11.917% (286/2400)\n",
            "Loss: 415.020 | Acc: 11.880% (297/2500)\n",
            "Loss: 413.027 | Acc: 11.846% (308/2600)\n",
            "Loss: 417.662 | Acc: 11.815% (319/2700)\n",
            "Loss: 428.772 | Acc: 11.857% (332/2800)\n",
            "Loss: 435.491 | Acc: 11.931% (346/2900)\n",
            "Loss: 436.351 | Acc: 12.067% (362/3000)\n",
            "Loss: 434.004 | Acc: 11.968% (371/3100)\n",
            "Loss: 434.084 | Acc: 12.031% (385/3200)\n",
            "Loss: 434.056 | Acc: 12.000% (396/3300)\n",
            "Loss: 431.483 | Acc: 12.147% (413/3400)\n",
            "Loss: 433.693 | Acc: 12.086% (423/3500)\n",
            "Loss: 434.108 | Acc: 12.111% (436/3600)\n",
            "Loss: 435.134 | Acc: 12.216% (452/3700)\n",
            "Loss: 437.855 | Acc: 12.158% (462/3800)\n",
            "Loss: 439.620 | Acc: 12.154% (474/3900)\n",
            "Loss: 441.697 | Acc: 12.225% (489/4000)\n",
            "Loss: 441.497 | Acc: 12.098% (496/4100)\n",
            "Loss: 442.587 | Acc: 12.190% (512/4200)\n",
            "Loss: 442.254 | Acc: 12.140% (522/4300)\n",
            "Loss: 441.234 | Acc: 12.159% (535/4400)\n",
            "Loss: 443.568 | Acc: 12.200% (549/4500)\n",
            "Loss: 441.194 | Acc: 12.196% (561/4600)\n",
            "Loss: 441.002 | Acc: 12.106% (569/4700)\n",
            "Loss: 441.679 | Acc: 12.188% (585/4800)\n",
            "Loss: 442.803 | Acc: 12.224% (599/4900)\n",
            "Loss: 444.711 | Acc: 12.160% (608/5000)\n",
            "Loss: 441.314 | Acc: 12.118% (618/5100)\n",
            "Loss: 439.147 | Acc: 12.096% (629/5200)\n",
            "Loss: 441.114 | Acc: 12.019% (637/5300)\n",
            "Loss: 439.539 | Acc: 12.000% (648/5400)\n",
            "Loss: 438.346 | Acc: 12.036% (662/5500)\n",
            "Loss: 437.171 | Acc: 12.000% (672/5600)\n",
            "Loss: 438.449 | Acc: 11.860% (676/5700)\n",
            "Loss: 437.624 | Acc: 11.914% (691/5800)\n",
            "Loss: 434.257 | Acc: 11.898% (702/5900)\n",
            "Loss: 432.367 | Acc: 12.033% (722/6000)\n",
            "Loss: 432.302 | Acc: 11.967% (730/6100)\n",
            "Loss: 433.391 | Acc: 12.016% (745/6200)\n",
            "Loss: 434.576 | Acc: 11.952% (753/6300)\n",
            "Loss: 433.470 | Acc: 11.969% (766/6400)\n",
            "Loss: 435.878 | Acc: 11.908% (774/6500)\n",
            "Loss: 433.486 | Acc: 11.970% (790/6600)\n",
            "Loss: 431.671 | Acc: 11.970% (802/6700)\n",
            "Loss: 434.857 | Acc: 12.044% (819/6800)\n",
            "Loss: 432.600 | Acc: 12.072% (833/6900)\n",
            "Loss: 434.860 | Acc: 12.057% (844/7000)\n",
            "Loss: 434.793 | Acc: 12.070% (857/7100)\n",
            "Loss: 437.053 | Acc: 12.042% (867/7200)\n",
            "Loss: 436.800 | Acc: 12.014% (877/7300)\n",
            "Loss: 438.084 | Acc: 11.946% (884/7400)\n",
            "Loss: 438.032 | Acc: 11.960% (897/7500)\n",
            "Loss: 438.443 | Acc: 11.974% (910/7600)\n",
            "Loss: 438.488 | Acc: 12.013% (925/7700)\n",
            "Loss: 438.144 | Acc: 11.974% (934/7800)\n",
            "Loss: 437.314 | Acc: 11.987% (947/7900)\n",
            "Loss: 436.171 | Acc: 11.925% (954/8000)\n",
            "Loss: 435.959 | Acc: 11.938% (967/8100)\n",
            "Loss: 434.262 | Acc: 11.866% (973/8200)\n",
            "Loss: 434.803 | Acc: 11.807% (980/8300)\n",
            "Loss: 433.781 | Acc: 11.821% (993/8400)\n",
            "Loss: 434.206 | Acc: 11.871% (1009/8500)\n",
            "Loss: 433.167 | Acc: 11.826% (1017/8600)\n",
            "Loss: 433.131 | Acc: 11.782% (1025/8700)\n",
            "Loss: 431.199 | Acc: 11.784% (1037/8800)\n",
            "Loss: 431.330 | Acc: 11.730% (1044/8900)\n",
            "Loss: 431.401 | Acc: 11.833% (1065/9000)\n",
            "Loss: 432.244 | Acc: 11.934% (1086/9100)\n",
            "Loss: 432.726 | Acc: 11.880% (1093/9200)\n",
            "Loss: 431.539 | Acc: 11.871% (1104/9300)\n",
            "Loss: 432.487 | Acc: 11.851% (1114/9400)\n",
            "Loss: 431.528 | Acc: 11.853% (1126/9500)\n",
            "Loss: 431.369 | Acc: 11.885% (1141/9600)\n",
            "Loss: 431.506 | Acc: 11.897% (1154/9700)\n",
            "Loss: 431.687 | Acc: 11.929% (1169/9800)\n",
            "Loss: 430.904 | Acc: 11.929% (1181/9900)\n",
            "Loss: 433.653 | Acc: 11.860% (1186/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 16\n",
            "Loss: 14.090 | Acc: 10.000% (1/10)\n",
            "Loss: 9.263 | Acc: 5.000% (1/20)\n",
            "Loss: 10.570 | Acc: 3.333% (1/30)\n",
            "Loss: 12.763 | Acc: 2.500% (1/40)\n",
            "Loss: 13.264 | Acc: 2.000% (1/50)\n",
            "Loss: 13.397 | Acc: 3.333% (2/60)\n",
            "Loss: 14.002 | Acc: 4.286% (3/70)\n",
            "Loss: 13.053 | Acc: 5.000% (4/80)\n",
            "Loss: 12.805 | Acc: 6.667% (6/90)\n",
            "Loss: 13.486 | Acc: 7.000% (7/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 16\n",
            "Loss: 37.204 | Acc: 11.000% (11/100)\n",
            "Loss: 40.632 | Acc: 9.500% (19/200)\n",
            "Loss: 53.742 | Acc: 10.000% (30/300)\n",
            "Loss: 54.965 | Acc: 9.500% (38/400)\n",
            "Loss: 50.643 | Acc: 9.800% (49/500)\n",
            "Loss: 49.946 | Acc: 11.000% (66/600)\n",
            "Loss: 49.939 | Acc: 11.000% (77/700)\n",
            "Loss: 50.390 | Acc: 11.250% (90/800)\n",
            "Loss: 53.760 | Acc: 11.000% (99/900)\n",
            "Loss: 51.948 | Acc: 11.200% (112/1000)\n",
            "Loss: 52.962 | Acc: 10.818% (119/1100)\n",
            "Loss: 54.323 | Acc: 10.750% (129/1200)\n",
            "Loss: 55.285 | Acc: 10.385% (135/1300)\n",
            "Loss: 56.907 | Acc: 10.143% (142/1400)\n",
            "Loss: 56.469 | Acc: 10.200% (153/1500)\n",
            "Loss: 55.531 | Acc: 9.625% (154/1600)\n",
            "Loss: 56.932 | Acc: 9.647% (164/1700)\n",
            "Loss: 58.187 | Acc: 9.611% (173/1800)\n",
            "Loss: 58.696 | Acc: 9.368% (178/1900)\n",
            "Loss: 59.765 | Acc: 9.400% (188/2000)\n",
            "Loss: 59.226 | Acc: 9.238% (194/2100)\n",
            "Loss: 59.522 | Acc: 9.182% (202/2200)\n",
            "Loss: 59.169 | Acc: 9.217% (212/2300)\n",
            "Loss: 59.660 | Acc: 9.125% (219/2400)\n",
            "Loss: 60.217 | Acc: 9.280% (232/2500)\n",
            "Loss: 60.995 | Acc: 9.269% (241/2600)\n",
            "Loss: 62.294 | Acc: 9.111% (246/2700)\n",
            "Loss: 64.136 | Acc: 9.214% (258/2800)\n",
            "Loss: 65.162 | Acc: 9.207% (267/2900)\n",
            "Loss: 65.237 | Acc: 9.167% (275/3000)\n",
            "Loss: 64.411 | Acc: 9.065% (281/3100)\n",
            "Loss: 64.758 | Acc: 9.125% (292/3200)\n",
            "Loss: 64.444 | Acc: 9.091% (300/3300)\n",
            "Loss: 64.092 | Acc: 9.000% (306/3400)\n",
            "Loss: 63.689 | Acc: 8.971% (314/3500)\n",
            "Loss: 63.642 | Acc: 8.972% (323/3600)\n",
            "Loss: 63.423 | Acc: 8.865% (328/3700)\n",
            "Loss: 63.147 | Acc: 8.868% (337/3800)\n",
            "Loss: 63.435 | Acc: 8.769% (342/3900)\n",
            "Loss: 63.835 | Acc: 8.800% (352/4000)\n",
            "Loss: 64.245 | Acc: 8.878% (364/4100)\n",
            "Loss: 64.576 | Acc: 8.833% (371/4200)\n",
            "Loss: 64.179 | Acc: 8.977% (386/4300)\n",
            "Loss: 64.195 | Acc: 8.886% (391/4400)\n",
            "Loss: 64.359 | Acc: 8.889% (400/4500)\n",
            "Loss: 64.221 | Acc: 8.848% (407/4600)\n",
            "Loss: 64.383 | Acc: 8.766% (412/4700)\n",
            "Loss: 64.499 | Acc: 8.812% (423/4800)\n",
            "Loss: 64.492 | Acc: 8.735% (428/4900)\n",
            "Loss: 64.758 | Acc: 8.640% (432/5000)\n",
            "Loss: 64.957 | Acc: 8.608% (439/5100)\n",
            "Loss: 64.487 | Acc: 8.615% (448/5200)\n",
            "Loss: 64.228 | Acc: 8.642% (458/5300)\n",
            "Loss: 63.795 | Acc: 8.667% (468/5400)\n",
            "Loss: 64.010 | Acc: 8.655% (476/5500)\n",
            "Loss: 63.849 | Acc: 8.661% (485/5600)\n",
            "Loss: 63.954 | Acc: 8.596% (490/5700)\n",
            "Loss: 63.678 | Acc: 8.569% (497/5800)\n",
            "Loss: 63.196 | Acc: 8.542% (504/5900)\n",
            "Loss: 63.104 | Acc: 8.517% (511/6000)\n",
            "Loss: 63.418 | Acc: 8.443% (515/6100)\n",
            "Loss: 63.733 | Acc: 8.452% (524/6200)\n",
            "Loss: 64.033 | Acc: 8.413% (530/6300)\n",
            "Loss: 63.816 | Acc: 8.422% (539/6400)\n",
            "Loss: 64.510 | Acc: 8.385% (545/6500)\n",
            "Loss: 63.925 | Acc: 8.394% (554/6600)\n",
            "Loss: 63.644 | Acc: 8.433% (565/6700)\n",
            "Loss: 63.854 | Acc: 8.412% (572/6800)\n",
            "Loss: 63.690 | Acc: 8.420% (581/6900)\n",
            "Loss: 63.945 | Acc: 8.414% (589/7000)\n",
            "Loss: 63.852 | Acc: 8.451% (600/7100)\n",
            "Loss: 63.967 | Acc: 8.417% (606/7200)\n",
            "Loss: 64.123 | Acc: 8.397% (613/7300)\n",
            "Loss: 64.284 | Acc: 8.351% (618/7400)\n",
            "Loss: 63.986 | Acc: 8.347% (626/7500)\n",
            "Loss: 64.073 | Acc: 8.368% (636/7600)\n",
            "Loss: 63.987 | Acc: 8.442% (650/7700)\n",
            "Loss: 64.027 | Acc: 8.449% (659/7800)\n",
            "Loss: 64.029 | Acc: 8.506% (672/7900)\n",
            "Loss: 63.899 | Acc: 8.562% (685/8000)\n",
            "Loss: 64.202 | Acc: 8.630% (699/8100)\n",
            "Loss: 63.933 | Acc: 8.634% (708/8200)\n",
            "Loss: 64.097 | Acc: 8.627% (716/8300)\n",
            "Loss: 63.739 | Acc: 8.631% (725/8400)\n",
            "Loss: 63.571 | Acc: 8.624% (733/8500)\n",
            "Loss: 63.366 | Acc: 8.663% (745/8600)\n",
            "Loss: 63.293 | Acc: 8.644% (752/8700)\n",
            "Loss: 62.969 | Acc: 8.614% (758/8800)\n",
            "Loss: 62.963 | Acc: 8.640% (769/8900)\n",
            "Loss: 63.003 | Acc: 8.667% (780/9000)\n",
            "Loss: 63.381 | Acc: 8.659% (788/9100)\n",
            "Loss: 63.595 | Acc: 8.674% (798/9200)\n",
            "Loss: 63.691 | Acc: 8.645% (804/9300)\n",
            "Loss: 63.770 | Acc: 8.638% (812/9400)\n",
            "Loss: 63.660 | Acc: 8.600% (817/9500)\n",
            "Loss: 63.658 | Acc: 8.667% (832/9600)\n",
            "Loss: 63.467 | Acc: 8.691% (843/9700)\n",
            "Loss: 63.603 | Acc: 8.714% (854/9800)\n",
            "Loss: 63.492 | Acc: 8.717% (863/9900)\n",
            "Loss: 63.806 | Acc: 8.720% (872/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 17\n",
            "Loss: 11.780 | Acc: 10.000% (1/10)\n",
            "Loss: 8.731 | Acc: 15.000% (3/20)\n",
            "Loss: 9.706 | Acc: 10.000% (3/30)\n",
            "Loss: 11.490 | Acc: 12.500% (5/40)\n",
            "Loss: 12.095 | Acc: 10.000% (5/50)\n",
            "Loss: 12.697 | Acc: 10.000% (6/60)\n",
            "Loss: 12.502 | Acc: 10.000% (7/70)\n",
            "Loss: 11.942 | Acc: 8.750% (7/80)\n",
            "Loss: 11.020 | Acc: 11.111% (10/90)\n",
            "Loss: 11.954 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 17\n",
            "Loss: 57.689 | Acc: 19.000% (19/100)\n",
            "Loss: 73.704 | Acc: 16.500% (33/200)\n",
            "Loss: 110.629 | Acc: 14.333% (43/300)\n",
            "Loss: 100.681 | Acc: 15.000% (60/400)\n",
            "Loss: 96.843 | Acc: 14.000% (70/500)\n",
            "Loss: 94.234 | Acc: 14.833% (89/600)\n",
            "Loss: 89.636 | Acc: 15.000% (105/700)\n",
            "Loss: 86.644 | Acc: 14.875% (119/800)\n",
            "Loss: 89.376 | Acc: 13.667% (123/900)\n",
            "Loss: 83.932 | Acc: 13.800% (138/1000)\n",
            "Loss: 86.634 | Acc: 14.091% (155/1100)\n",
            "Loss: 90.601 | Acc: 13.667% (164/1200)\n",
            "Loss: 91.426 | Acc: 13.462% (175/1300)\n",
            "Loss: 92.524 | Acc: 13.643% (191/1400)\n",
            "Loss: 92.983 | Acc: 13.400% (201/1500)\n",
            "Loss: 93.141 | Acc: 13.188% (211/1600)\n",
            "Loss: 93.741 | Acc: 13.294% (226/1700)\n",
            "Loss: 92.563 | Acc: 13.056% (235/1800)\n",
            "Loss: 93.738 | Acc: 12.947% (246/1900)\n",
            "Loss: 92.084 | Acc: 13.000% (260/2000)\n",
            "Loss: 93.486 | Acc: 12.810% (269/2100)\n",
            "Loss: 92.218 | Acc: 12.909% (284/2200)\n",
            "Loss: 91.521 | Acc: 12.652% (291/2300)\n",
            "Loss: 92.308 | Acc: 12.625% (303/2400)\n",
            "Loss: 93.623 | Acc: 12.920% (323/2500)\n",
            "Loss: 92.830 | Acc: 12.846% (334/2600)\n",
            "Loss: 93.449 | Acc: 12.741% (344/2700)\n",
            "Loss: 96.129 | Acc: 12.571% (352/2800)\n",
            "Loss: 97.138 | Acc: 12.655% (367/2900)\n",
            "Loss: 97.005 | Acc: 12.433% (373/3000)\n",
            "Loss: 96.024 | Acc: 12.323% (382/3100)\n",
            "Loss: 95.266 | Acc: 12.344% (395/3200)\n",
            "Loss: 94.752 | Acc: 12.152% (401/3300)\n",
            "Loss: 93.144 | Acc: 12.059% (410/3400)\n",
            "Loss: 94.280 | Acc: 12.029% (421/3500)\n",
            "Loss: 95.030 | Acc: 12.111% (436/3600)\n",
            "Loss: 94.642 | Acc: 12.027% (445/3700)\n",
            "Loss: 96.053 | Acc: 12.079% (459/3800)\n",
            "Loss: 96.657 | Acc: 12.077% (471/3900)\n",
            "Loss: 97.161 | Acc: 12.050% (482/4000)\n",
            "Loss: 96.799 | Acc: 11.976% (491/4100)\n",
            "Loss: 97.281 | Acc: 11.929% (501/4200)\n",
            "Loss: 98.175 | Acc: 11.791% (507/4300)\n",
            "Loss: 98.636 | Acc: 11.818% (520/4400)\n",
            "Loss: 99.630 | Acc: 11.733% (528/4500)\n",
            "Loss: 99.700 | Acc: 11.717% (539/4600)\n",
            "Loss: 98.969 | Acc: 11.702% (550/4700)\n",
            "Loss: 99.315 | Acc: 11.896% (571/4800)\n",
            "Loss: 99.637 | Acc: 11.918% (584/4900)\n",
            "Loss: 99.888 | Acc: 11.900% (595/5000)\n",
            "Loss: 99.473 | Acc: 11.922% (608/5100)\n",
            "Loss: 98.800 | Acc: 12.038% (626/5200)\n",
            "Loss: 98.876 | Acc: 11.981% (635/5300)\n",
            "Loss: 98.511 | Acc: 11.944% (645/5400)\n",
            "Loss: 98.678 | Acc: 12.073% (664/5500)\n",
            "Loss: 98.757 | Acc: 12.107% (678/5600)\n",
            "Loss: 99.312 | Acc: 12.053% (687/5700)\n",
            "Loss: 99.222 | Acc: 12.086% (701/5800)\n",
            "Loss: 97.947 | Acc: 12.102% (714/5900)\n",
            "Loss: 97.376 | Acc: 12.217% (733/6000)\n",
            "Loss: 97.549 | Acc: 12.148% (741/6100)\n",
            "Loss: 97.619 | Acc: 12.145% (753/6200)\n",
            "Loss: 97.883 | Acc: 12.111% (763/6300)\n",
            "Loss: 97.450 | Acc: 12.062% (772/6400)\n",
            "Loss: 98.136 | Acc: 12.031% (782/6500)\n",
            "Loss: 97.826 | Acc: 12.061% (796/6600)\n",
            "Loss: 97.710 | Acc: 12.015% (805/6700)\n",
            "Loss: 98.550 | Acc: 11.956% (813/6800)\n",
            "Loss: 98.328 | Acc: 12.014% (829/6900)\n",
            "Loss: 98.799 | Acc: 12.029% (842/7000)\n",
            "Loss: 98.654 | Acc: 12.028% (854/7100)\n",
            "Loss: 99.155 | Acc: 12.028% (866/7200)\n",
            "Loss: 98.919 | Acc: 11.959% (873/7300)\n",
            "Loss: 98.755 | Acc: 11.946% (884/7400)\n",
            "Loss: 98.417 | Acc: 11.933% (895/7500)\n",
            "Loss: 98.667 | Acc: 11.947% (908/7600)\n",
            "Loss: 98.943 | Acc: 11.948% (920/7700)\n",
            "Loss: 98.352 | Acc: 11.923% (930/7800)\n",
            "Loss: 98.030 | Acc: 11.937% (943/7900)\n",
            "Loss: 97.817 | Acc: 11.900% (952/8000)\n",
            "Loss: 97.731 | Acc: 11.877% (962/8100)\n",
            "Loss: 97.112 | Acc: 11.927% (978/8200)\n",
            "Loss: 97.280 | Acc: 11.892% (987/8300)\n",
            "Loss: 96.640 | Acc: 11.893% (999/8400)\n",
            "Loss: 96.779 | Acc: 11.847% (1007/8500)\n",
            "Loss: 96.614 | Acc: 11.907% (1024/8600)\n",
            "Loss: 96.577 | Acc: 11.908% (1036/8700)\n",
            "Loss: 95.833 | Acc: 11.943% (1051/8800)\n",
            "Loss: 96.411 | Acc: 11.933% (1062/8900)\n",
            "Loss: 96.775 | Acc: 11.978% (1078/9000)\n",
            "Loss: 97.076 | Acc: 11.956% (1088/9100)\n",
            "Loss: 96.841 | Acc: 11.924% (1097/9200)\n",
            "Loss: 96.116 | Acc: 11.935% (1110/9300)\n",
            "Loss: 96.141 | Acc: 11.947% (1123/9400)\n",
            "Loss: 95.871 | Acc: 11.958% (1136/9500)\n",
            "Loss: 96.118 | Acc: 12.010% (1153/9600)\n",
            "Loss: 96.213 | Acc: 12.021% (1166/9700)\n",
            "Loss: 96.326 | Acc: 12.020% (1178/9800)\n",
            "Loss: 96.156 | Acc: 12.000% (1188/9900)\n",
            "Loss: 97.133 | Acc: 11.920% (1192/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 18\n",
            "Loss: 21.441 | Acc: 10.000% (1/10)\n",
            "Loss: 12.372 | Acc: 15.000% (3/20)\n",
            "Loss: 10.319 | Acc: 16.667% (5/30)\n",
            "Loss: 9.456 | Acc: 12.500% (5/40)\n",
            "Loss: 10.660 | Acc: 12.000% (6/50)\n",
            "Loss: 9.843 | Acc: 11.667% (7/60)\n",
            "Loss: 12.414 | Acc: 10.000% (7/70)\n",
            "Loss: 14.253 | Acc: 10.000% (8/80)\n",
            "Loss: 13.859 | Acc: 8.889% (8/90)\n",
            "Loss: 12.890 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 18\n",
            "Loss: 23.088 | Acc: 9.000% (9/100)\n",
            "Loss: 22.366 | Acc: 8.000% (16/200)\n",
            "Loss: 33.013 | Acc: 8.000% (24/300)\n",
            "Loss: 29.364 | Acc: 8.750% (35/400)\n",
            "Loss: 27.420 | Acc: 9.200% (46/500)\n",
            "Loss: 26.849 | Acc: 10.333% (62/600)\n",
            "Loss: 25.862 | Acc: 10.143% (71/700)\n",
            "Loss: 25.195 | Acc: 9.500% (76/800)\n",
            "Loss: 25.788 | Acc: 9.222% (83/900)\n",
            "Loss: 24.292 | Acc: 9.200% (92/1000)\n",
            "Loss: 25.126 | Acc: 9.636% (106/1100)\n",
            "Loss: 25.194 | Acc: 9.250% (111/1200)\n",
            "Loss: 25.260 | Acc: 9.308% (121/1300)\n",
            "Loss: 25.544 | Acc: 9.571% (134/1400)\n",
            "Loss: 25.116 | Acc: 9.533% (143/1500)\n",
            "Loss: 25.009 | Acc: 9.688% (155/1600)\n",
            "Loss: 25.690 | Acc: 9.824% (167/1700)\n",
            "Loss: 25.093 | Acc: 9.889% (178/1800)\n",
            "Loss: 25.463 | Acc: 10.105% (192/1900)\n",
            "Loss: 25.128 | Acc: 10.150% (203/2000)\n",
            "Loss: 25.198 | Acc: 10.429% (219/2100)\n",
            "Loss: 25.188 | Acc: 10.409% (229/2200)\n",
            "Loss: 24.559 | Acc: 10.522% (242/2300)\n",
            "Loss: 24.817 | Acc: 10.458% (251/2400)\n",
            "Loss: 25.245 | Acc: 10.240% (256/2500)\n",
            "Loss: 25.084 | Acc: 10.269% (267/2600)\n",
            "Loss: 25.033 | Acc: 10.296% (278/2700)\n",
            "Loss: 26.198 | Acc: 10.357% (290/2800)\n",
            "Loss: 26.471 | Acc: 10.517% (305/2900)\n",
            "Loss: 26.255 | Acc: 10.533% (316/3000)\n",
            "Loss: 26.176 | Acc: 10.516% (326/3100)\n",
            "Loss: 26.007 | Acc: 10.500% (336/3200)\n",
            "Loss: 25.861 | Acc: 10.576% (349/3300)\n",
            "Loss: 25.541 | Acc: 10.559% (359/3400)\n",
            "Loss: 25.858 | Acc: 10.571% (370/3500)\n",
            "Loss: 25.895 | Acc: 10.472% (377/3600)\n",
            "Loss: 25.815 | Acc: 10.459% (387/3700)\n",
            "Loss: 26.121 | Acc: 10.447% (397/3800)\n",
            "Loss: 26.235 | Acc: 10.436% (407/3900)\n",
            "Loss: 26.165 | Acc: 10.375% (415/4000)\n",
            "Loss: 25.927 | Acc: 10.439% (428/4100)\n",
            "Loss: 26.035 | Acc: 10.405% (437/4200)\n",
            "Loss: 26.187 | Acc: 10.326% (444/4300)\n",
            "Loss: 26.339 | Acc: 10.341% (455/4400)\n",
            "Loss: 26.530 | Acc: 10.222% (460/4500)\n",
            "Loss: 26.584 | Acc: 10.174% (468/4600)\n",
            "Loss: 26.306 | Acc: 10.191% (479/4700)\n",
            "Loss: 26.353 | Acc: 10.188% (489/4800)\n",
            "Loss: 26.434 | Acc: 10.143% (497/4900)\n",
            "Loss: 26.511 | Acc: 10.200% (510/5000)\n",
            "Loss: 26.575 | Acc: 10.255% (523/5100)\n",
            "Loss: 26.400 | Acc: 10.288% (535/5200)\n",
            "Loss: 26.512 | Acc: 10.283% (545/5300)\n",
            "Loss: 26.349 | Acc: 10.296% (556/5400)\n",
            "Loss: 26.489 | Acc: 10.327% (568/5500)\n",
            "Loss: 26.553 | Acc: 10.304% (577/5600)\n",
            "Loss: 26.617 | Acc: 10.386% (592/5700)\n",
            "Loss: 26.664 | Acc: 10.362% (601/5800)\n",
            "Loss: 26.371 | Acc: 10.390% (613/5900)\n",
            "Loss: 26.336 | Acc: 10.433% (626/6000)\n",
            "Loss: 26.370 | Acc: 10.508% (641/6100)\n",
            "Loss: 26.456 | Acc: 10.613% (658/6200)\n",
            "Loss: 26.400 | Acc: 10.683% (673/6300)\n",
            "Loss: 26.290 | Acc: 10.594% (678/6400)\n",
            "Loss: 26.336 | Acc: 10.554% (686/6500)\n",
            "Loss: 26.133 | Acc: 10.606% (700/6600)\n",
            "Loss: 26.099 | Acc: 10.612% (711/6700)\n",
            "Loss: 26.337 | Acc: 10.559% (718/6800)\n",
            "Loss: 26.375 | Acc: 10.522% (726/6900)\n",
            "Loss: 26.396 | Acc: 10.514% (736/7000)\n",
            "Loss: 26.310 | Acc: 10.451% (742/7100)\n",
            "Loss: 26.290 | Acc: 10.472% (754/7200)\n",
            "Loss: 26.216 | Acc: 10.548% (770/7300)\n",
            "Loss: 26.130 | Acc: 10.595% (784/7400)\n",
            "Loss: 25.991 | Acc: 10.627% (797/7500)\n",
            "Loss: 26.105 | Acc: 10.618% (807/7600)\n",
            "Loss: 26.206 | Acc: 10.571% (814/7700)\n",
            "Loss: 26.145 | Acc: 10.590% (826/7800)\n",
            "Loss: 26.191 | Acc: 10.557% (834/7900)\n",
            "Loss: 26.096 | Acc: 10.550% (844/8000)\n",
            "Loss: 26.233 | Acc: 10.506% (851/8100)\n",
            "Loss: 26.179 | Acc: 10.524% (863/8200)\n",
            "Loss: 26.243 | Acc: 10.530% (874/8300)\n",
            "Loss: 26.038 | Acc: 10.560% (887/8400)\n",
            "Loss: 25.992 | Acc: 10.553% (897/8500)\n",
            "Loss: 26.002 | Acc: 10.512% (904/8600)\n",
            "Loss: 25.925 | Acc: 10.517% (915/8700)\n",
            "Loss: 25.741 | Acc: 10.534% (927/8800)\n",
            "Loss: 25.845 | Acc: 10.506% (935/8900)\n",
            "Loss: 25.932 | Acc: 10.489% (944/9000)\n",
            "Loss: 26.021 | Acc: 10.484% (954/9100)\n",
            "Loss: 25.924 | Acc: 10.478% (964/9200)\n",
            "Loss: 25.761 | Acc: 10.484% (975/9300)\n",
            "Loss: 25.777 | Acc: 10.553% (992/9400)\n",
            "Loss: 25.740 | Acc: 10.526% (1000/9500)\n",
            "Loss: 25.836 | Acc: 10.542% (1012/9600)\n",
            "Loss: 25.824 | Acc: 10.557% (1024/9700)\n",
            "Loss: 25.783 | Acc: 10.561% (1035/9800)\n",
            "Loss: 25.737 | Acc: 10.576% (1047/9900)\n",
            "Loss: 25.928 | Acc: 10.560% (1056/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 19\n",
            "Loss: 10.584 | Acc: 10.000% (1/10)\n",
            "Loss: 12.084 | Acc: 10.000% (2/20)\n",
            "Loss: 14.038 | Acc: 6.667% (2/30)\n",
            "Loss: 11.113 | Acc: 15.000% (6/40)\n",
            "Loss: 10.443 | Acc: 16.000% (8/50)\n",
            "Loss: 10.199 | Acc: 16.667% (10/60)\n",
            "Loss: 11.272 | Acc: 14.286% (10/70)\n",
            "Loss: 10.564 | Acc: 13.750% (11/80)\n",
            "Loss: 11.013 | Acc: 15.556% (14/90)\n",
            "Loss: 11.103 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 19\n",
            "Loss: 21.442 | Acc: 10.000% (10/100)\n",
            "Loss: 34.377 | Acc: 10.500% (21/200)\n",
            "Loss: 53.944 | Acc: 12.667% (38/300)\n",
            "Loss: 45.837 | Acc: 12.000% (48/400)\n",
            "Loss: 45.634 | Acc: 11.600% (58/500)\n",
            "Loss: 44.734 | Acc: 12.000% (72/600)\n",
            "Loss: 42.375 | Acc: 12.143% (85/700)\n",
            "Loss: 41.920 | Acc: 11.500% (92/800)\n",
            "Loss: 47.754 | Acc: 10.778% (97/900)\n",
            "Loss: 44.146 | Acc: 10.600% (106/1000)\n",
            "Loss: 44.829 | Acc: 10.182% (112/1100)\n",
            "Loss: 48.655 | Acc: 10.167% (122/1200)\n",
            "Loss: 48.441 | Acc: 10.231% (133/1300)\n",
            "Loss: 49.739 | Acc: 10.286% (144/1400)\n",
            "Loss: 51.007 | Acc: 10.133% (152/1500)\n",
            "Loss: 49.255 | Acc: 9.750% (156/1600)\n",
            "Loss: 49.001 | Acc: 9.882% (168/1700)\n",
            "Loss: 48.542 | Acc: 9.667% (174/1800)\n",
            "Loss: 48.172 | Acc: 9.789% (186/1900)\n",
            "Loss: 47.318 | Acc: 9.650% (193/2000)\n",
            "Loss: 47.014 | Acc: 9.619% (202/2100)\n",
            "Loss: 46.592 | Acc: 9.727% (214/2200)\n",
            "Loss: 45.810 | Acc: 9.826% (226/2300)\n",
            "Loss: 46.096 | Acc: 9.708% (233/2400)\n",
            "Loss: 46.395 | Acc: 9.840% (246/2500)\n",
            "Loss: 46.669 | Acc: 9.654% (251/2600)\n",
            "Loss: 47.298 | Acc: 9.556% (258/2700)\n",
            "Loss: 49.722 | Acc: 9.607% (269/2800)\n",
            "Loss: 50.639 | Acc: 9.621% (279/2900)\n",
            "Loss: 50.746 | Acc: 9.700% (291/3000)\n",
            "Loss: 50.652 | Acc: 9.613% (298/3100)\n",
            "Loss: 50.604 | Acc: 9.906% (317/3200)\n",
            "Loss: 50.470 | Acc: 9.939% (328/3300)\n",
            "Loss: 49.887 | Acc: 9.971% (339/3400)\n",
            "Loss: 49.942 | Acc: 9.943% (348/3500)\n",
            "Loss: 49.568 | Acc: 9.944% (358/3600)\n",
            "Loss: 49.619 | Acc: 10.027% (371/3700)\n",
            "Loss: 49.486 | Acc: 10.079% (383/3800)\n",
            "Loss: 49.241 | Acc: 10.077% (393/3900)\n",
            "Loss: 49.983 | Acc: 10.150% (406/4000)\n",
            "Loss: 49.876 | Acc: 10.024% (411/4100)\n",
            "Loss: 49.815 | Acc: 9.952% (418/4200)\n",
            "Loss: 50.428 | Acc: 10.047% (432/4300)\n",
            "Loss: 50.027 | Acc: 10.023% (441/4400)\n",
            "Loss: 49.612 | Acc: 10.044% (452/4500)\n",
            "Loss: 49.426 | Acc: 10.152% (467/4600)\n",
            "Loss: 49.730 | Acc: 10.064% (473/4700)\n",
            "Loss: 50.391 | Acc: 10.167% (488/4800)\n",
            "Loss: 49.984 | Acc: 10.286% (504/4900)\n",
            "Loss: 50.346 | Acc: 10.340% (517/5000)\n",
            "Loss: 49.886 | Acc: 10.275% (524/5100)\n",
            "Loss: 49.755 | Acc: 10.288% (535/5200)\n",
            "Loss: 50.205 | Acc: 10.226% (542/5300)\n",
            "Loss: 50.182 | Acc: 10.222% (552/5400)\n",
            "Loss: 50.205 | Acc: 10.182% (560/5500)\n",
            "Loss: 50.281 | Acc: 10.125% (567/5600)\n",
            "Loss: 50.694 | Acc: 10.018% (571/5700)\n",
            "Loss: 50.970 | Acc: 10.017% (581/5800)\n",
            "Loss: 50.443 | Acc: 9.915% (585/5900)\n",
            "Loss: 50.386 | Acc: 9.983% (599/6000)\n",
            "Loss: 50.314 | Acc: 9.885% (603/6100)\n",
            "Loss: 50.413 | Acc: 9.952% (617/6200)\n",
            "Loss: 50.740 | Acc: 9.937% (626/6300)\n",
            "Loss: 50.609 | Acc: 9.938% (636/6400)\n",
            "Loss: 50.842 | Acc: 9.892% (643/6500)\n",
            "Loss: 50.759 | Acc: 9.848% (650/6600)\n",
            "Loss: 50.793 | Acc: 9.821% (658/6700)\n",
            "Loss: 50.839 | Acc: 9.794% (666/6800)\n",
            "Loss: 50.743 | Acc: 9.812% (677/6900)\n",
            "Loss: 50.552 | Acc: 9.829% (688/7000)\n",
            "Loss: 50.331 | Acc: 9.817% (697/7100)\n",
            "Loss: 50.218 | Acc: 9.847% (709/7200)\n",
            "Loss: 50.145 | Acc: 9.808% (716/7300)\n",
            "Loss: 50.314 | Acc: 9.743% (721/7400)\n",
            "Loss: 50.204 | Acc: 9.787% (734/7500)\n",
            "Loss: 50.700 | Acc: 9.829% (747/7600)\n",
            "Loss: 50.987 | Acc: 9.909% (763/7700)\n",
            "Loss: 51.044 | Acc: 9.897% (772/7800)\n",
            "Loss: 50.622 | Acc: 9.886% (781/7900)\n",
            "Loss: 50.609 | Acc: 9.912% (793/8000)\n",
            "Loss: 50.490 | Acc: 9.938% (805/8100)\n",
            "Loss: 50.376 | Acc: 9.890% (811/8200)\n",
            "Loss: 50.731 | Acc: 9.843% (817/8300)\n",
            "Loss: 50.526 | Acc: 9.821% (825/8400)\n",
            "Loss: 50.408 | Acc: 9.894% (841/8500)\n",
            "Loss: 50.246 | Acc: 9.872% (849/8600)\n",
            "Loss: 49.978 | Acc: 9.839% (856/8700)\n",
            "Loss: 49.801 | Acc: 9.830% (865/8800)\n",
            "Loss: 49.979 | Acc: 9.854% (877/8900)\n",
            "Loss: 50.068 | Acc: 9.933% (894/9000)\n",
            "Loss: 50.079 | Acc: 9.934% (904/9100)\n",
            "Loss: 50.025 | Acc: 9.935% (914/9200)\n",
            "Loss: 49.824 | Acc: 9.957% (926/9300)\n",
            "Loss: 49.845 | Acc: 9.979% (938/9400)\n",
            "Loss: 49.622 | Acc: 9.979% (948/9500)\n",
            "Loss: 50.139 | Acc: 10.000% (960/9600)\n",
            "Loss: 50.209 | Acc: 10.000% (970/9700)\n",
            "Loss: 50.287 | Acc: 10.051% (985/9800)\n",
            "Loss: 50.220 | Acc: 10.020% (992/9900)\n",
            "Loss: 50.335 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 20\n",
            "Loss: 18.901 | Acc: 10.000% (1/10)\n",
            "Loss: 12.169 | Acc: 15.000% (3/20)\n",
            "Loss: 13.043 | Acc: 16.667% (5/30)\n",
            "Loss: 16.346 | Acc: 15.000% (6/40)\n",
            "Loss: 13.719 | Acc: 12.000% (6/50)\n",
            "Loss: 12.015 | Acc: 11.667% (7/60)\n",
            "Loss: 11.964 | Acc: 10.000% (7/70)\n",
            "Loss: 12.012 | Acc: 8.750% (7/80)\n",
            "Loss: 12.272 | Acc: 8.889% (8/90)\n",
            "Loss: 12.172 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 20\n",
            "Loss: 69.110 | Acc: 10.000% (10/100)\n",
            "Loss: 91.567 | Acc: 9.500% (19/200)\n",
            "Loss: 131.358 | Acc: 11.000% (33/300)\n",
            "Loss: 112.483 | Acc: 10.500% (42/400)\n",
            "Loss: 114.237 | Acc: 10.600% (53/500)\n",
            "Loss: 108.950 | Acc: 11.000% (66/600)\n",
            "Loss: 110.417 | Acc: 10.857% (76/700)\n",
            "Loss: 112.423 | Acc: 10.625% (85/800)\n",
            "Loss: 119.038 | Acc: 10.333% (93/900)\n",
            "Loss: 110.106 | Acc: 10.500% (105/1000)\n",
            "Loss: 114.562 | Acc: 9.909% (109/1100)\n",
            "Loss: 116.753 | Acc: 9.917% (119/1200)\n",
            "Loss: 115.327 | Acc: 9.923% (129/1300)\n",
            "Loss: 119.010 | Acc: 9.929% (139/1400)\n",
            "Loss: 121.336 | Acc: 9.800% (147/1500)\n",
            "Loss: 121.209 | Acc: 9.375% (150/1600)\n",
            "Loss: 123.969 | Acc: 9.529% (162/1700)\n",
            "Loss: 124.021 | Acc: 9.333% (168/1800)\n",
            "Loss: 122.657 | Acc: 9.579% (182/1900)\n",
            "Loss: 123.075 | Acc: 9.500% (190/2000)\n",
            "Loss: 123.905 | Acc: 9.286% (195/2100)\n",
            "Loss: 124.449 | Acc: 9.455% (208/2200)\n",
            "Loss: 123.110 | Acc: 9.522% (219/2300)\n",
            "Loss: 122.179 | Acc: 9.458% (227/2400)\n",
            "Loss: 121.471 | Acc: 9.600% (240/2500)\n",
            "Loss: 121.337 | Acc: 9.577% (249/2600)\n",
            "Loss: 125.027 | Acc: 9.259% (250/2700)\n",
            "Loss: 129.122 | Acc: 9.321% (261/2800)\n",
            "Loss: 130.821 | Acc: 9.276% (269/2900)\n",
            "Loss: 130.786 | Acc: 9.267% (278/3000)\n",
            "Loss: 129.985 | Acc: 9.194% (285/3100)\n",
            "Loss: 131.625 | Acc: 9.344% (299/3200)\n",
            "Loss: 131.853 | Acc: 9.333% (308/3300)\n",
            "Loss: 131.239 | Acc: 9.353% (318/3400)\n",
            "Loss: 132.470 | Acc: 9.286% (325/3500)\n",
            "Loss: 131.338 | Acc: 9.278% (334/3600)\n",
            "Loss: 131.144 | Acc: 9.351% (346/3700)\n",
            "Loss: 132.799 | Acc: 9.289% (353/3800)\n",
            "Loss: 133.655 | Acc: 9.333% (364/3900)\n",
            "Loss: 134.539 | Acc: 9.425% (377/4000)\n",
            "Loss: 133.422 | Acc: 9.341% (383/4100)\n",
            "Loss: 133.589 | Acc: 9.262% (389/4200)\n",
            "Loss: 133.287 | Acc: 9.395% (404/4300)\n",
            "Loss: 132.907 | Acc: 9.386% (413/4400)\n",
            "Loss: 131.901 | Acc: 9.378% (422/4500)\n",
            "Loss: 131.018 | Acc: 9.478% (436/4600)\n",
            "Loss: 132.376 | Acc: 9.383% (441/4700)\n",
            "Loss: 133.391 | Acc: 9.479% (455/4800)\n",
            "Loss: 133.848 | Acc: 9.510% (466/4900)\n",
            "Loss: 134.889 | Acc: 9.540% (477/5000)\n",
            "Loss: 134.204 | Acc: 9.549% (487/5100)\n",
            "Loss: 133.136 | Acc: 9.538% (496/5200)\n",
            "Loss: 133.420 | Acc: 9.491% (503/5300)\n",
            "Loss: 133.459 | Acc: 9.463% (511/5400)\n",
            "Loss: 134.169 | Acc: 9.418% (518/5500)\n",
            "Loss: 133.724 | Acc: 9.375% (525/5600)\n",
            "Loss: 134.177 | Acc: 9.281% (529/5700)\n",
            "Loss: 133.824 | Acc: 9.241% (536/5800)\n",
            "Loss: 132.552 | Acc: 9.169% (541/5900)\n",
            "Loss: 132.128 | Acc: 9.267% (556/6000)\n",
            "Loss: 131.783 | Acc: 9.197% (561/6100)\n",
            "Loss: 132.036 | Acc: 9.274% (575/6200)\n",
            "Loss: 132.365 | Acc: 9.270% (584/6300)\n",
            "Loss: 131.934 | Acc: 9.297% (595/6400)\n",
            "Loss: 132.209 | Acc: 9.277% (603/6500)\n",
            "Loss: 131.221 | Acc: 9.227% (609/6600)\n",
            "Loss: 130.552 | Acc: 9.209% (617/6700)\n",
            "Loss: 130.952 | Acc: 9.176% (624/6800)\n",
            "Loss: 130.690 | Acc: 9.232% (637/6900)\n",
            "Loss: 131.096 | Acc: 9.243% (647/7000)\n",
            "Loss: 130.551 | Acc: 9.239% (656/7100)\n",
            "Loss: 131.044 | Acc: 9.222% (664/7200)\n",
            "Loss: 131.409 | Acc: 9.164% (669/7300)\n",
            "Loss: 131.127 | Acc: 9.081% (672/7400)\n",
            "Loss: 130.568 | Acc: 9.120% (684/7500)\n",
            "Loss: 131.436 | Acc: 9.158% (696/7600)\n",
            "Loss: 131.652 | Acc: 9.221% (710/7700)\n",
            "Loss: 131.521 | Acc: 9.244% (721/7800)\n",
            "Loss: 130.947 | Acc: 9.215% (728/7900)\n",
            "Loss: 130.310 | Acc: 9.250% (740/8000)\n",
            "Loss: 130.685 | Acc: 9.247% (749/8100)\n",
            "Loss: 130.027 | Acc: 9.220% (756/8200)\n",
            "Loss: 130.260 | Acc: 9.169% (761/8300)\n",
            "Loss: 129.304 | Acc: 9.155% (769/8400)\n",
            "Loss: 129.438 | Acc: 9.176% (780/8500)\n",
            "Loss: 129.043 | Acc: 9.140% (786/8600)\n",
            "Loss: 128.948 | Acc: 9.092% (791/8700)\n",
            "Loss: 128.034 | Acc: 9.091% (800/8800)\n",
            "Loss: 128.209 | Acc: 9.112% (811/8900)\n",
            "Loss: 128.375 | Acc: 9.178% (826/9000)\n",
            "Loss: 128.326 | Acc: 9.198% (837/9100)\n",
            "Loss: 128.392 | Acc: 9.228% (849/9200)\n",
            "Loss: 127.954 | Acc: 9.258% (861/9300)\n",
            "Loss: 128.141 | Acc: 9.266% (871/9400)\n",
            "Loss: 127.620 | Acc: 9.253% (879/9500)\n",
            "Loss: 128.096 | Acc: 9.281% (891/9600)\n",
            "Loss: 128.288 | Acc: 9.278% (900/9700)\n",
            "Loss: 128.216 | Acc: 9.327% (914/9800)\n",
            "Loss: 128.070 | Acc: 9.303% (921/9900)\n",
            "Loss: 128.901 | Acc: 9.280% (928/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 21\n",
            "Loss: 2.436 | Acc: 30.000% (3/10)\n",
            "Loss: 10.030 | Acc: 15.000% (3/20)\n",
            "Loss: 10.928 | Acc: 23.333% (7/30)\n",
            "Loss: 11.148 | Acc: 22.500% (9/40)\n",
            "Loss: 11.594 | Acc: 20.000% (10/50)\n",
            "Loss: 12.555 | Acc: 20.000% (12/60)\n",
            "Loss: 13.934 | Acc: 18.571% (13/70)\n",
            "Loss: 12.946 | Acc: 17.500% (14/80)\n",
            "Loss: 13.071 | Acc: 16.667% (15/90)\n",
            "Loss: 12.281 | Acc: 17.000% (17/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 21\n",
            "Loss: 4.155 | Acc: 11.000% (11/100)\n",
            "Loss: 4.705 | Acc: 9.500% (19/200)\n",
            "Loss: 5.313 | Acc: 11.333% (34/300)\n",
            "Loss: 4.857 | Acc: 10.500% (42/400)\n",
            "Loss: 5.278 | Acc: 10.200% (51/500)\n",
            "Loss: 4.945 | Acc: 10.500% (63/600)\n",
            "Loss: 4.729 | Acc: 10.571% (74/700)\n",
            "Loss: 4.752 | Acc: 10.125% (81/800)\n",
            "Loss: 6.295 | Acc: 9.556% (86/900)\n",
            "Loss: 6.052 | Acc: 9.600% (96/1000)\n",
            "Loss: 6.281 | Acc: 9.364% (103/1100)\n",
            "Loss: 6.149 | Acc: 9.333% (112/1200)\n",
            "Loss: 6.032 | Acc: 9.538% (124/1300)\n",
            "Loss: 6.014 | Acc: 9.643% (135/1400)\n",
            "Loss: 6.006 | Acc: 9.533% (143/1500)\n",
            "Loss: 5.882 | Acc: 9.250% (148/1600)\n",
            "Loss: 5.929 | Acc: 9.471% (161/1700)\n",
            "Loss: 6.060 | Acc: 9.222% (166/1800)\n",
            "Loss: 6.119 | Acc: 9.474% (180/1900)\n",
            "Loss: 6.050 | Acc: 9.450% (189/2000)\n",
            "Loss: 6.065 | Acc: 9.238% (194/2100)\n",
            "Loss: 6.159 | Acc: 9.364% (206/2200)\n",
            "Loss: 6.061 | Acc: 9.435% (217/2300)\n",
            "Loss: 5.956 | Acc: 9.333% (224/2400)\n",
            "Loss: 5.885 | Acc: 9.560% (239/2500)\n",
            "Loss: 6.046 | Acc: 9.462% (246/2600)\n",
            "Loss: 6.187 | Acc: 9.222% (249/2700)\n",
            "Loss: 6.461 | Acc: 9.286% (260/2800)\n",
            "Loss: 6.562 | Acc: 9.310% (270/2900)\n",
            "Loss: 6.535 | Acc: 9.367% (281/3000)\n",
            "Loss: 6.520 | Acc: 9.290% (288/3100)\n",
            "Loss: 6.634 | Acc: 9.594% (307/3200)\n",
            "Loss: 6.636 | Acc: 9.636% (318/3300)\n",
            "Loss: 6.586 | Acc: 9.676% (329/3400)\n",
            "Loss: 7.019 | Acc: 9.571% (335/3500)\n",
            "Loss: 6.913 | Acc: 9.556% (344/3600)\n",
            "Loss: 6.887 | Acc: 9.622% (356/3700)\n",
            "Loss: 6.869 | Acc: 9.605% (365/3800)\n",
            "Loss: 6.802 | Acc: 9.615% (375/3900)\n",
            "Loss: 6.762 | Acc: 9.700% (388/4000)\n",
            "Loss: 6.710 | Acc: 9.585% (393/4100)\n",
            "Loss: 6.793 | Acc: 9.429% (396/4200)\n",
            "Loss: 6.859 | Acc: 9.512% (409/4300)\n",
            "Loss: 6.872 | Acc: 9.545% (420/4400)\n",
            "Loss: 6.868 | Acc: 9.578% (431/4500)\n",
            "Loss: 6.793 | Acc: 9.674% (445/4600)\n",
            "Loss: 6.878 | Acc: 9.574% (450/4700)\n",
            "Loss: 7.026 | Acc: 9.708% (466/4800)\n",
            "Loss: 6.993 | Acc: 9.735% (477/4900)\n",
            "Loss: 6.942 | Acc: 9.780% (489/5000)\n",
            "Loss: 6.933 | Acc: 9.725% (496/5100)\n",
            "Loss: 6.862 | Acc: 9.769% (508/5200)\n",
            "Loss: 7.172 | Acc: 9.660% (512/5300)\n",
            "Loss: 7.205 | Acc: 9.648% (521/5400)\n",
            "Loss: 7.225 | Acc: 9.618% (529/5500)\n",
            "Loss: 7.180 | Acc: 9.589% (537/5600)\n",
            "Loss: 7.219 | Acc: 9.491% (541/5700)\n",
            "Loss: 7.168 | Acc: 9.483% (550/5800)\n",
            "Loss: 7.106 | Acc: 9.424% (556/5900)\n",
            "Loss: 7.075 | Acc: 9.517% (571/6000)\n",
            "Loss: 7.109 | Acc: 9.410% (574/6100)\n",
            "Loss: 7.340 | Acc: 9.468% (587/6200)\n",
            "Loss: 7.313 | Acc: 9.460% (596/6300)\n",
            "Loss: 7.273 | Acc: 9.438% (604/6400)\n",
            "Loss: 7.244 | Acc: 9.385% (610/6500)\n",
            "Loss: 7.184 | Acc: 9.333% (616/6600)\n",
            "Loss: 7.145 | Acc: 9.299% (623/6700)\n",
            "Loss: 7.101 | Acc: 9.265% (630/6800)\n",
            "Loss: 7.058 | Acc: 9.304% (642/6900)\n",
            "Loss: 7.044 | Acc: 9.300% (651/7000)\n",
            "Loss: 6.993 | Acc: 9.296% (660/7100)\n",
            "Loss: 7.022 | Acc: 9.278% (668/7200)\n",
            "Loss: 7.078 | Acc: 9.233% (674/7300)\n",
            "Loss: 7.033 | Acc: 9.162% (678/7400)\n",
            "Loss: 7.167 | Acc: 9.200% (690/7500)\n",
            "Loss: 7.169 | Acc: 9.237% (702/7600)\n",
            "Loss: 7.250 | Acc: 9.273% (714/7700)\n",
            "Loss: 7.265 | Acc: 9.256% (722/7800)\n",
            "Loss: 7.251 | Acc: 9.241% (730/7900)\n",
            "Loss: 7.227 | Acc: 9.287% (743/8000)\n",
            "Loss: 7.210 | Acc: 9.346% (757/8100)\n",
            "Loss: 7.165 | Acc: 9.317% (764/8200)\n",
            "Loss: 7.123 | Acc: 9.265% (769/8300)\n",
            "Loss: 7.095 | Acc: 9.250% (777/8400)\n",
            "Loss: 7.081 | Acc: 9.271% (788/8500)\n",
            "Loss: 7.078 | Acc: 9.256% (796/8600)\n",
            "Loss: 7.140 | Acc: 9.218% (802/8700)\n",
            "Loss: 7.258 | Acc: 9.239% (813/8800)\n",
            "Loss: 7.229 | Acc: 9.236% (822/8900)\n",
            "Loss: 7.207 | Acc: 9.311% (838/9000)\n",
            "Loss: 7.167 | Acc: 9.330% (849/9100)\n",
            "Loss: 7.131 | Acc: 9.337% (859/9200)\n",
            "Loss: 7.097 | Acc: 9.387% (873/9300)\n",
            "Loss: 7.058 | Acc: 9.426% (886/9400)\n",
            "Loss: 7.125 | Acc: 9.411% (894/9500)\n",
            "Loss: 7.158 | Acc: 9.479% (910/9600)\n",
            "Loss: 7.142 | Acc: 9.485% (920/9700)\n",
            "Loss: 7.102 | Acc: 9.520% (933/9800)\n",
            "Loss: 7.068 | Acc: 9.505% (941/9900)\n",
            "Loss: 7.104 | Acc: 9.480% (948/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 22\n",
            "Loss: 4.769 | Acc: 30.000% (3/10)\n",
            "Loss: 6.171 | Acc: 20.000% (4/20)\n",
            "Loss: 7.012 | Acc: 13.333% (4/30)\n",
            "Loss: 7.118 | Acc: 15.000% (6/40)\n",
            "Loss: 7.431 | Acc: 16.000% (8/50)\n",
            "Loss: 7.043 | Acc: 15.000% (9/60)\n",
            "Loss: 8.949 | Acc: 12.857% (9/70)\n",
            "Loss: 8.343 | Acc: 15.000% (12/80)\n",
            "Loss: 11.933 | Acc: 13.333% (12/90)\n",
            "Loss: 11.439 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 22\n",
            "Loss: 9.526 | Acc: 9.000% (9/100)\n",
            "Loss: 16.625 | Acc: 10.000% (20/200)\n",
            "Loss: 21.022 | Acc: 11.000% (33/300)\n",
            "Loss: 17.106 | Acc: 10.250% (41/400)\n",
            "Loss: 18.478 | Acc: 10.800% (54/500)\n",
            "Loss: 18.737 | Acc: 10.500% (63/600)\n",
            "Loss: 18.426 | Acc: 10.714% (75/700)\n",
            "Loss: 19.542 | Acc: 10.375% (83/800)\n",
            "Loss: 21.345 | Acc: 10.667% (96/900)\n",
            "Loss: 19.720 | Acc: 10.900% (109/1000)\n",
            "Loss: 19.942 | Acc: 11.273% (124/1100)\n",
            "Loss: 19.314 | Acc: 11.083% (133/1200)\n",
            "Loss: 19.640 | Acc: 10.846% (141/1300)\n",
            "Loss: 19.402 | Acc: 10.571% (148/1400)\n",
            "Loss: 19.559 | Acc: 10.467% (157/1500)\n",
            "Loss: 19.248 | Acc: 10.312% (165/1600)\n",
            "Loss: 19.084 | Acc: 10.059% (171/1700)\n",
            "Loss: 19.122 | Acc: 10.389% (187/1800)\n",
            "Loss: 20.282 | Acc: 10.211% (194/1900)\n",
            "Loss: 20.283 | Acc: 10.400% (208/2000)\n",
            "Loss: 20.599 | Acc: 10.429% (219/2100)\n",
            "Loss: 20.099 | Acc: 10.682% (235/2200)\n",
            "Loss: 19.770 | Acc: 10.696% (246/2300)\n",
            "Loss: 19.433 | Acc: 10.583% (254/2400)\n",
            "Loss: 19.033 | Acc: 10.520% (263/2500)\n",
            "Loss: 19.478 | Acc: 10.500% (273/2600)\n",
            "Loss: 19.634 | Acc: 10.593% (286/2700)\n",
            "Loss: 19.980 | Acc: 10.536% (295/2800)\n",
            "Loss: 20.100 | Acc: 10.414% (302/2900)\n",
            "Loss: 20.351 | Acc: 10.467% (314/3000)\n",
            "Loss: 20.241 | Acc: 10.419% (323/3100)\n",
            "Loss: 20.150 | Acc: 10.562% (338/3200)\n",
            "Loss: 20.106 | Acc: 10.455% (345/3300)\n",
            "Loss: 19.868 | Acc: 10.471% (356/3400)\n",
            "Loss: 20.845 | Acc: 10.571% (370/3500)\n",
            "Loss: 20.559 | Acc: 10.500% (378/3600)\n",
            "Loss: 20.553 | Acc: 10.514% (389/3700)\n",
            "Loss: 20.921 | Acc: 10.368% (394/3800)\n",
            "Loss: 20.820 | Acc: 10.359% (404/3900)\n",
            "Loss: 20.720 | Acc: 10.250% (410/4000)\n",
            "Loss: 20.426 | Acc: 10.220% (419/4100)\n",
            "Loss: 20.483 | Acc: 10.262% (431/4200)\n",
            "Loss: 20.910 | Acc: 10.233% (440/4300)\n",
            "Loss: 20.927 | Acc: 10.227% (450/4400)\n",
            "Loss: 20.671 | Acc: 10.267% (462/4500)\n",
            "Loss: 20.452 | Acc: 10.217% (470/4600)\n",
            "Loss: 20.528 | Acc: 10.191% (479/4700)\n",
            "Loss: 21.202 | Acc: 10.250% (492/4800)\n",
            "Loss: 21.368 | Acc: 10.224% (501/4900)\n",
            "Loss: 21.435 | Acc: 10.200% (510/5000)\n",
            "Loss: 21.329 | Acc: 10.216% (521/5100)\n",
            "Loss: 21.116 | Acc: 10.231% (532/5200)\n",
            "Loss: 21.657 | Acc: 10.377% (550/5300)\n",
            "Loss: 21.871 | Acc: 10.407% (562/5400)\n",
            "Loss: 22.241 | Acc: 10.436% (574/5500)\n",
            "Loss: 22.103 | Acc: 10.429% (584/5600)\n",
            "Loss: 21.973 | Acc: 10.333% (589/5700)\n",
            "Loss: 21.765 | Acc: 10.310% (598/5800)\n",
            "Loss: 21.496 | Acc: 10.322% (609/5900)\n",
            "Loss: 21.254 | Acc: 10.333% (620/6000)\n",
            "Loss: 21.219 | Acc: 10.443% (637/6100)\n",
            "Loss: 21.441 | Acc: 10.387% (644/6200)\n",
            "Loss: 21.336 | Acc: 10.302% (649/6300)\n",
            "Loss: 21.220 | Acc: 10.359% (663/6400)\n",
            "Loss: 21.274 | Acc: 10.354% (673/6500)\n",
            "Loss: 21.091 | Acc: 10.348% (683/6600)\n",
            "Loss: 20.996 | Acc: 10.433% (699/6700)\n",
            "Loss: 20.946 | Acc: 10.412% (708/6800)\n",
            "Loss: 20.838 | Acc: 10.420% (719/6900)\n",
            "Loss: 20.905 | Acc: 10.386% (727/7000)\n",
            "Loss: 20.793 | Acc: 10.352% (735/7100)\n",
            "Loss: 20.996 | Acc: 10.403% (749/7200)\n",
            "Loss: 20.942 | Acc: 10.356% (756/7300)\n",
            "Loss: 20.812 | Acc: 10.311% (763/7400)\n",
            "Loss: 21.293 | Acc: 10.320% (774/7500)\n",
            "Loss: 21.330 | Acc: 10.316% (784/7600)\n",
            "Loss: 21.483 | Acc: 10.338% (796/7700)\n",
            "Loss: 21.419 | Acc: 10.333% (806/7800)\n",
            "Loss: 21.341 | Acc: 10.418% (823/7900)\n",
            "Loss: 21.447 | Acc: 10.400% (832/8000)\n",
            "Loss: 21.579 | Acc: 10.457% (847/8100)\n",
            "Loss: 21.579 | Acc: 10.463% (858/8200)\n",
            "Loss: 21.417 | Acc: 10.434% (866/8300)\n",
            "Loss: 21.276 | Acc: 10.405% (874/8400)\n",
            "Loss: 21.391 | Acc: 10.412% (885/8500)\n",
            "Loss: 21.337 | Acc: 10.384% (893/8600)\n",
            "Loss: 21.444 | Acc: 10.333% (899/8700)\n",
            "Loss: 21.811 | Acc: 10.330% (909/8800)\n",
            "Loss: 21.817 | Acc: 10.360% (922/8900)\n",
            "Loss: 21.785 | Acc: 10.367% (933/9000)\n",
            "Loss: 21.620 | Acc: 10.396% (946/9100)\n",
            "Loss: 21.534 | Acc: 10.402% (957/9200)\n",
            "Loss: 21.367 | Acc: 10.398% (967/9300)\n",
            "Loss: 21.337 | Acc: 10.362% (974/9400)\n",
            "Loss: 21.562 | Acc: 10.379% (986/9500)\n",
            "Loss: 21.479 | Acc: 10.406% (999/9600)\n",
            "Loss: 21.458 | Acc: 10.381% (1007/9700)\n",
            "Loss: 21.400 | Acc: 10.367% (1016/9800)\n",
            "Loss: 21.257 | Acc: 10.475% (1037/9900)\n",
            "Loss: 21.425 | Acc: 10.500% (1050/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 23\n",
            "Loss: 3.628 | Acc: 10.000% (1/10)\n",
            "Loss: 6.574 | Acc: 10.000% (2/20)\n",
            "Loss: 16.289 | Acc: 16.667% (5/30)\n",
            "Loss: 13.648 | Acc: 15.000% (6/40)\n",
            "Loss: 12.206 | Acc: 12.000% (6/50)\n",
            "Loss: 10.739 | Acc: 13.333% (8/60)\n",
            "Loss: 11.194 | Acc: 12.857% (9/70)\n",
            "Loss: 11.806 | Acc: 11.250% (9/80)\n",
            "Loss: 12.557 | Acc: 12.222% (11/90)\n",
            "Loss: 13.531 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 23\n",
            "Loss: 31.039 | Acc: 10.000% (10/100)\n",
            "Loss: 39.332 | Acc: 11.000% (22/200)\n",
            "Loss: 39.615 | Acc: 12.000% (36/300)\n",
            "Loss: 35.839 | Acc: 11.500% (46/400)\n",
            "Loss: 38.288 | Acc: 11.400% (57/500)\n",
            "Loss: 44.805 | Acc: 11.667% (70/600)\n",
            "Loss: 45.880 | Acc: 11.571% (81/700)\n",
            "Loss: 45.135 | Acc: 11.000% (88/800)\n",
            "Loss: 46.584 | Acc: 10.444% (94/900)\n",
            "Loss: 44.529 | Acc: 10.900% (109/1000)\n",
            "Loss: 45.819 | Acc: 10.455% (115/1100)\n",
            "Loss: 49.874 | Acc: 10.333% (124/1200)\n",
            "Loss: 48.160 | Acc: 10.385% (135/1300)\n",
            "Loss: 48.167 | Acc: 10.429% (146/1400)\n",
            "Loss: 50.872 | Acc: 10.400% (156/1500)\n",
            "Loss: 50.433 | Acc: 10.062% (161/1600)\n",
            "Loss: 50.675 | Acc: 10.235% (174/1700)\n",
            "Loss: 48.778 | Acc: 10.056% (181/1800)\n",
            "Loss: 50.649 | Acc: 10.263% (195/1900)\n",
            "Loss: 50.693 | Acc: 10.200% (204/2000)\n",
            "Loss: 49.525 | Acc: 10.143% (213/2100)\n",
            "Loss: 49.678 | Acc: 10.273% (226/2200)\n",
            "Loss: 48.507 | Acc: 10.435% (240/2300)\n",
            "Loss: 47.969 | Acc: 10.292% (247/2400)\n",
            "Loss: 47.666 | Acc: 10.360% (259/2500)\n",
            "Loss: 48.567 | Acc: 10.115% (263/2600)\n",
            "Loss: 47.971 | Acc: 9.963% (269/2700)\n",
            "Loss: 48.532 | Acc: 10.036% (281/2800)\n",
            "Loss: 48.740 | Acc: 10.103% (293/2900)\n",
            "Loss: 48.521 | Acc: 10.167% (305/3000)\n",
            "Loss: 47.825 | Acc: 10.129% (314/3100)\n",
            "Loss: 48.243 | Acc: 10.312% (330/3200)\n",
            "Loss: 47.995 | Acc: 10.333% (341/3300)\n",
            "Loss: 47.871 | Acc: 10.324% (351/3400)\n",
            "Loss: 48.122 | Acc: 10.314% (361/3500)\n",
            "Loss: 47.853 | Acc: 10.333% (372/3600)\n",
            "Loss: 47.315 | Acc: 10.405% (385/3700)\n",
            "Loss: 47.754 | Acc: 10.474% (398/3800)\n",
            "Loss: 48.187 | Acc: 10.385% (405/3900)\n",
            "Loss: 48.748 | Acc: 10.475% (419/4000)\n",
            "Loss: 48.862 | Acc: 10.341% (424/4100)\n",
            "Loss: 49.183 | Acc: 10.190% (428/4200)\n",
            "Loss: 49.829 | Acc: 10.302% (443/4300)\n",
            "Loss: 50.633 | Acc: 10.250% (451/4400)\n",
            "Loss: 50.586 | Acc: 10.222% (460/4500)\n",
            "Loss: 50.226 | Acc: 10.304% (474/4600)\n",
            "Loss: 49.945 | Acc: 10.191% (479/4700)\n",
            "Loss: 51.394 | Acc: 10.292% (494/4800)\n",
            "Loss: 50.726 | Acc: 10.429% (511/4900)\n",
            "Loss: 50.090 | Acc: 10.420% (521/5000)\n",
            "Loss: 50.027 | Acc: 10.392% (530/5100)\n",
            "Loss: 50.044 | Acc: 10.404% (541/5200)\n",
            "Loss: 49.916 | Acc: 10.340% (548/5300)\n",
            "Loss: 50.303 | Acc: 10.315% (557/5400)\n",
            "Loss: 51.108 | Acc: 10.236% (563/5500)\n",
            "Loss: 51.165 | Acc: 10.196% (571/5600)\n",
            "Loss: 50.997 | Acc: 10.123% (577/5700)\n",
            "Loss: 50.709 | Acc: 10.069% (584/5800)\n",
            "Loss: 50.365 | Acc: 9.966% (588/5900)\n",
            "Loss: 50.063 | Acc: 10.050% (603/6000)\n",
            "Loss: 49.923 | Acc: 9.951% (607/6100)\n",
            "Loss: 50.173 | Acc: 10.016% (621/6200)\n",
            "Loss: 49.957 | Acc: 9.984% (629/6300)\n",
            "Loss: 49.796 | Acc: 9.953% (637/6400)\n",
            "Loss: 49.838 | Acc: 9.938% (646/6500)\n",
            "Loss: 50.198 | Acc: 9.894% (653/6600)\n",
            "Loss: 50.783 | Acc: 9.881% (662/6700)\n",
            "Loss: 50.650 | Acc: 9.853% (670/6800)\n",
            "Loss: 50.265 | Acc: 9.913% (684/6900)\n",
            "Loss: 49.925 | Acc: 9.943% (696/7000)\n",
            "Loss: 49.760 | Acc: 9.958% (707/7100)\n",
            "Loss: 49.774 | Acc: 9.944% (716/7200)\n",
            "Loss: 49.527 | Acc: 9.904% (723/7300)\n",
            "Loss: 49.583 | Acc: 9.865% (730/7400)\n",
            "Loss: 49.577 | Acc: 9.920% (744/7500)\n",
            "Loss: 50.013 | Acc: 9.974% (758/7600)\n",
            "Loss: 50.102 | Acc: 10.013% (771/7700)\n",
            "Loss: 50.273 | Acc: 10.000% (780/7800)\n",
            "Loss: 49.987 | Acc: 10.000% (790/7900)\n",
            "Loss: 50.328 | Acc: 10.050% (804/8000)\n",
            "Loss: 50.745 | Acc: 10.086% (817/8100)\n",
            "Loss: 50.775 | Acc: 10.049% (824/8200)\n",
            "Loss: 50.679 | Acc: 10.000% (830/8300)\n",
            "Loss: 50.720 | Acc: 9.988% (839/8400)\n",
            "Loss: 50.745 | Acc: 10.024% (852/8500)\n",
            "Loss: 50.897 | Acc: 10.023% (862/8600)\n",
            "Loss: 50.679 | Acc: 10.011% (871/8700)\n",
            "Loss: 50.665 | Acc: 9.989% (879/8800)\n",
            "Loss: 51.185 | Acc: 9.989% (889/8900)\n",
            "Loss: 51.091 | Acc: 10.056% (905/9000)\n",
            "Loss: 50.916 | Acc: 10.044% (914/9100)\n",
            "Loss: 50.846 | Acc: 10.054% (925/9200)\n",
            "Loss: 50.717 | Acc: 10.043% (934/9300)\n",
            "Loss: 50.610 | Acc: 10.053% (945/9400)\n",
            "Loss: 50.675 | Acc: 10.042% (954/9500)\n",
            "Loss: 51.066 | Acc: 10.062% (966/9600)\n",
            "Loss: 50.851 | Acc: 10.062% (976/9700)\n",
            "Loss: 51.156 | Acc: 10.092% (989/9800)\n",
            "Loss: 51.167 | Acc: 10.091% (999/9900)\n",
            "Loss: 51.137 | Acc: 10.070% (1007/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 24\n",
            "Loss: 15.117 | Acc: 10.000% (1/10)\n",
            "Loss: 12.921 | Acc: 10.000% (2/20)\n",
            "Loss: 12.764 | Acc: 13.333% (4/30)\n",
            "Loss: 10.485 | Acc: 17.500% (7/40)\n",
            "Loss: 12.835 | Acc: 16.000% (8/50)\n",
            "Loss: 12.082 | Acc: 18.333% (11/60)\n",
            "Loss: 10.804 | Acc: 18.571% (13/70)\n",
            "Loss: 10.598 | Acc: 16.250% (13/80)\n",
            "Loss: 10.291 | Acc: 16.667% (15/90)\n",
            "Loss: 9.575 | Acc: 18.000% (18/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 24\n",
            "Loss: 56.689 | Acc: 9.000% (9/100)\n",
            "Loss: 92.691 | Acc: 8.500% (17/200)\n",
            "Loss: 117.022 | Acc: 10.000% (30/300)\n",
            "Loss: 95.657 | Acc: 9.250% (37/400)\n",
            "Loss: 101.648 | Acc: 9.400% (47/500)\n",
            "Loss: 105.230 | Acc: 9.000% (54/600)\n",
            "Loss: 113.580 | Acc: 8.286% (58/700)\n",
            "Loss: 112.071 | Acc: 8.375% (67/800)\n",
            "Loss: 108.906 | Acc: 9.222% (83/900)\n",
            "Loss: 101.583 | Acc: 9.300% (93/1000)\n",
            "Loss: 110.137 | Acc: 9.909% (109/1100)\n",
            "Loss: 115.060 | Acc: 10.000% (120/1200)\n",
            "Loss: 115.423 | Acc: 9.769% (127/1300)\n",
            "Loss: 112.316 | Acc: 9.429% (132/1400)\n",
            "Loss: 117.094 | Acc: 9.200% (138/1500)\n",
            "Loss: 118.910 | Acc: 9.062% (145/1600)\n",
            "Loss: 120.707 | Acc: 8.824% (150/1700)\n",
            "Loss: 118.184 | Acc: 9.389% (169/1800)\n",
            "Loss: 121.460 | Acc: 9.158% (174/1900)\n",
            "Loss: 123.542 | Acc: 9.200% (184/2000)\n",
            "Loss: 125.431 | Acc: 9.238% (194/2100)\n",
            "Loss: 124.611 | Acc: 9.318% (205/2200)\n",
            "Loss: 122.179 | Acc: 9.435% (217/2300)\n",
            "Loss: 120.392 | Acc: 9.417% (226/2400)\n",
            "Loss: 118.981 | Acc: 9.360% (234/2500)\n",
            "Loss: 117.910 | Acc: 9.346% (243/2600)\n",
            "Loss: 118.134 | Acc: 9.519% (257/2700)\n",
            "Loss: 119.771 | Acc: 9.607% (269/2800)\n",
            "Loss: 121.313 | Acc: 9.586% (278/2900)\n",
            "Loss: 121.158 | Acc: 9.633% (289/3000)\n",
            "Loss: 119.520 | Acc: 9.581% (297/3100)\n",
            "Loss: 119.817 | Acc: 9.719% (311/3200)\n",
            "Loss: 119.370 | Acc: 9.636% (318/3300)\n",
            "Loss: 118.468 | Acc: 9.647% (328/3400)\n",
            "Loss: 122.114 | Acc: 9.800% (343/3500)\n",
            "Loss: 121.440 | Acc: 9.722% (350/3600)\n",
            "Loss: 121.927 | Acc: 9.784% (362/3700)\n",
            "Loss: 126.397 | Acc: 9.579% (364/3800)\n",
            "Loss: 128.679 | Acc: 9.513% (371/3900)\n",
            "Loss: 129.070 | Acc: 9.500% (380/4000)\n",
            "Loss: 127.719 | Acc: 9.488% (389/4100)\n",
            "Loss: 128.861 | Acc: 9.571% (402/4200)\n",
            "Loss: 129.502 | Acc: 9.535% (410/4300)\n",
            "Loss: 131.218 | Acc: 9.523% (419/4400)\n",
            "Loss: 131.194 | Acc: 9.556% (430/4500)\n",
            "Loss: 129.783 | Acc: 9.500% (437/4600)\n",
            "Loss: 129.056 | Acc: 9.489% (446/4700)\n",
            "Loss: 132.003 | Acc: 9.479% (455/4800)\n",
            "Loss: 133.515 | Acc: 9.449% (463/4900)\n",
            "Loss: 133.480 | Acc: 9.440% (472/5000)\n",
            "Loss: 132.606 | Acc: 9.412% (480/5100)\n",
            "Loss: 132.148 | Acc: 9.423% (490/5200)\n",
            "Loss: 131.633 | Acc: 9.566% (507/5300)\n",
            "Loss: 131.746 | Acc: 9.648% (521/5400)\n",
            "Loss: 134.424 | Acc: 9.618% (529/5500)\n",
            "Loss: 134.255 | Acc: 9.607% (538/5600)\n",
            "Loss: 133.467 | Acc: 9.561% (545/5700)\n",
            "Loss: 132.220 | Acc: 9.586% (556/5800)\n",
            "Loss: 130.634 | Acc: 9.593% (566/5900)\n",
            "Loss: 129.373 | Acc: 9.533% (572/6000)\n",
            "Loss: 128.821 | Acc: 9.639% (588/6100)\n",
            "Loss: 128.979 | Acc: 9.613% (596/6200)\n",
            "Loss: 127.783 | Acc: 9.587% (604/6300)\n",
            "Loss: 126.825 | Acc: 9.656% (618/6400)\n",
            "Loss: 127.449 | Acc: 9.708% (631/6500)\n",
            "Loss: 127.304 | Acc: 9.667% (638/6600)\n",
            "Loss: 127.833 | Acc: 9.761% (654/6700)\n",
            "Loss: 128.563 | Acc: 9.750% (663/6800)\n",
            "Loss: 127.745 | Acc: 9.754% (673/6900)\n",
            "Loss: 128.239 | Acc: 9.757% (683/7000)\n",
            "Loss: 127.987 | Acc: 9.732% (691/7100)\n",
            "Loss: 128.377 | Acc: 9.764% (703/7200)\n",
            "Loss: 127.668 | Acc: 9.726% (710/7300)\n",
            "Loss: 127.523 | Acc: 9.676% (716/7400)\n",
            "Loss: 127.482 | Acc: 9.693% (727/7500)\n",
            "Loss: 128.312 | Acc: 9.684% (736/7600)\n",
            "Loss: 128.246 | Acc: 9.688% (746/7700)\n",
            "Loss: 127.563 | Acc: 9.692% (756/7800)\n",
            "Loss: 127.466 | Acc: 9.759% (771/7900)\n",
            "Loss: 127.354 | Acc: 9.750% (780/8000)\n",
            "Loss: 128.217 | Acc: 9.765% (791/8100)\n",
            "Loss: 127.405 | Acc: 9.720% (797/8200)\n",
            "Loss: 126.931 | Acc: 9.663% (802/8300)\n",
            "Loss: 126.298 | Acc: 9.631% (809/8400)\n",
            "Loss: 127.442 | Acc: 9.659% (821/8500)\n",
            "Loss: 127.966 | Acc: 9.628% (828/8600)\n",
            "Loss: 128.223 | Acc: 9.563% (832/8700)\n",
            "Loss: 127.109 | Acc: 9.557% (841/8800)\n",
            "Loss: 128.336 | Acc: 9.584% (853/8900)\n",
            "Loss: 128.525 | Acc: 9.611% (865/9000)\n",
            "Loss: 127.911 | Acc: 9.648% (878/9100)\n",
            "Loss: 127.600 | Acc: 9.685% (891/9200)\n",
            "Loss: 126.708 | Acc: 9.699% (902/9300)\n",
            "Loss: 126.808 | Acc: 9.649% (907/9400)\n",
            "Loss: 126.617 | Acc: 9.684% (920/9500)\n",
            "Loss: 126.794 | Acc: 9.708% (932/9600)\n",
            "Loss: 126.783 | Acc: 9.701% (941/9700)\n",
            "Loss: 126.546 | Acc: 9.684% (949/9800)\n",
            "Loss: 126.120 | Acc: 9.737% (964/9900)\n",
            "Loss: 127.044 | Acc: 9.750% (975/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 25\n",
            "Loss: 3.417 | Acc: 30.000% (3/10)\n",
            "Loss: 11.889 | Acc: 15.000% (3/20)\n",
            "Loss: 17.663 | Acc: 13.333% (4/30)\n",
            "Loss: 14.562 | Acc: 10.000% (4/40)\n",
            "Loss: 16.971 | Acc: 8.000% (4/50)\n",
            "Loss: 16.910 | Acc: 8.333% (5/60)\n",
            "Loss: 16.576 | Acc: 11.429% (8/70)\n",
            "Loss: 16.878 | Acc: 11.250% (9/80)\n",
            "Loss: 15.926 | Acc: 11.111% (10/90)\n",
            "Loss: 14.793 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 25\n",
            "Loss: 11.776 | Acc: 11.000% (11/100)\n",
            "Loss: 62.154 | Acc: 12.000% (24/200)\n",
            "Loss: 103.605 | Acc: 13.667% (41/300)\n",
            "Loss: 86.405 | Acc: 13.250% (53/400)\n",
            "Loss: 89.475 | Acc: 13.600% (68/500)\n",
            "Loss: 88.363 | Acc: 14.333% (86/600)\n",
            "Loss: 93.461 | Acc: 13.857% (97/700)\n",
            "Loss: 94.332 | Acc: 13.875% (111/800)\n",
            "Loss: 95.342 | Acc: 13.889% (125/900)\n",
            "Loss: 88.076 | Acc: 13.700% (137/1000)\n",
            "Loss: 93.067 | Acc: 13.364% (147/1100)\n",
            "Loss: 99.507 | Acc: 13.083% (157/1200)\n",
            "Loss: 101.966 | Acc: 13.000% (169/1300)\n",
            "Loss: 102.444 | Acc: 13.429% (188/1400)\n",
            "Loss: 103.624 | Acc: 13.400% (201/1500)\n",
            "Loss: 105.901 | Acc: 13.688% (219/1600)\n",
            "Loss: 106.597 | Acc: 14.000% (238/1700)\n",
            "Loss: 108.635 | Acc: 13.944% (251/1800)\n",
            "Loss: 110.529 | Acc: 13.947% (265/1900)\n",
            "Loss: 113.361 | Acc: 13.950% (279/2000)\n",
            "Loss: 115.933 | Acc: 13.714% (288/2100)\n",
            "Loss: 115.869 | Acc: 13.773% (303/2200)\n",
            "Loss: 114.333 | Acc: 13.870% (319/2300)\n",
            "Loss: 113.636 | Acc: 13.667% (328/2400)\n",
            "Loss: 112.600 | Acc: 13.680% (342/2500)\n",
            "Loss: 112.786 | Acc: 13.769% (358/2600)\n",
            "Loss: 116.518 | Acc: 13.889% (375/2700)\n",
            "Loss: 120.532 | Acc: 13.964% (391/2800)\n",
            "Loss: 124.679 | Acc: 14.138% (410/2900)\n",
            "Loss: 123.000 | Acc: 14.233% (427/3000)\n",
            "Loss: 121.672 | Acc: 14.194% (440/3100)\n",
            "Loss: 120.861 | Acc: 14.094% (451/3200)\n",
            "Loss: 119.630 | Acc: 14.242% (470/3300)\n",
            "Loss: 118.689 | Acc: 14.235% (484/3400)\n",
            "Loss: 120.648 | Acc: 14.257% (499/3500)\n",
            "Loss: 121.077 | Acc: 14.389% (518/3600)\n",
            "Loss: 122.688 | Acc: 14.459% (535/3700)\n",
            "Loss: 124.531 | Acc: 14.474% (550/3800)\n",
            "Loss: 126.317 | Acc: 14.385% (561/3900)\n",
            "Loss: 126.552 | Acc: 14.375% (575/4000)\n",
            "Loss: 125.612 | Acc: 14.390% (590/4100)\n",
            "Loss: 125.803 | Acc: 14.357% (603/4200)\n",
            "Loss: 125.976 | Acc: 14.302% (615/4300)\n",
            "Loss: 126.147 | Acc: 14.409% (634/4400)\n",
            "Loss: 125.623 | Acc: 14.422% (649/4500)\n",
            "Loss: 124.432 | Acc: 14.522% (668/4600)\n",
            "Loss: 125.334 | Acc: 14.511% (682/4700)\n",
            "Loss: 126.968 | Acc: 14.500% (696/4800)\n",
            "Loss: 127.936 | Acc: 14.531% (712/4900)\n",
            "Loss: 129.388 | Acc: 14.440% (722/5000)\n",
            "Loss: 127.762 | Acc: 14.510% (740/5100)\n",
            "Loss: 126.924 | Acc: 14.481% (753/5200)\n",
            "Loss: 126.609 | Acc: 14.472% (767/5300)\n",
            "Loss: 126.485 | Acc: 14.407% (778/5400)\n",
            "Loss: 128.073 | Acc: 14.345% (789/5500)\n",
            "Loss: 127.728 | Acc: 14.357% (804/5600)\n",
            "Loss: 128.101 | Acc: 14.386% (820/5700)\n",
            "Loss: 127.367 | Acc: 14.328% (831/5800)\n",
            "Loss: 125.948 | Acc: 14.288% (843/5900)\n",
            "Loss: 124.623 | Acc: 14.433% (866/6000)\n",
            "Loss: 124.018 | Acc: 14.377% (877/6100)\n",
            "Loss: 123.390 | Acc: 14.452% (896/6200)\n",
            "Loss: 123.822 | Acc: 14.429% (909/6300)\n",
            "Loss: 123.015 | Acc: 14.406% (922/6400)\n",
            "Loss: 123.531 | Acc: 14.462% (940/6500)\n",
            "Loss: 122.708 | Acc: 14.424% (952/6600)\n",
            "Loss: 122.978 | Acc: 14.418% (966/6700)\n",
            "Loss: 123.715 | Acc: 14.368% (977/6800)\n",
            "Loss: 122.977 | Acc: 14.406% (994/6900)\n",
            "Loss: 123.707 | Acc: 14.386% (1007/7000)\n",
            "Loss: 123.823 | Acc: 14.408% (1023/7100)\n",
            "Loss: 123.648 | Acc: 14.417% (1038/7200)\n",
            "Loss: 123.562 | Acc: 14.452% (1055/7300)\n",
            "Loss: 123.555 | Acc: 14.514% (1074/7400)\n",
            "Loss: 123.662 | Acc: 14.493% (1087/7500)\n",
            "Loss: 124.133 | Acc: 14.461% (1099/7600)\n",
            "Loss: 124.216 | Acc: 14.390% (1108/7700)\n",
            "Loss: 124.371 | Acc: 14.385% (1122/7800)\n",
            "Loss: 123.434 | Acc: 14.380% (1136/7900)\n",
            "Loss: 124.003 | Acc: 14.262% (1141/8000)\n",
            "Loss: 124.097 | Acc: 14.259% (1155/8100)\n",
            "Loss: 123.643 | Acc: 14.195% (1164/8200)\n",
            "Loss: 123.473 | Acc: 14.145% (1174/8300)\n",
            "Loss: 122.637 | Acc: 14.179% (1191/8400)\n",
            "Loss: 123.380 | Acc: 14.176% (1205/8500)\n",
            "Loss: 122.938 | Acc: 14.151% (1217/8600)\n",
            "Loss: 123.286 | Acc: 14.149% (1231/8700)\n",
            "Loss: 122.586 | Acc: 14.114% (1242/8800)\n",
            "Loss: 123.629 | Acc: 14.124% (1257/8900)\n",
            "Loss: 123.798 | Acc: 14.167% (1275/9000)\n",
            "Loss: 123.207 | Acc: 14.165% (1289/9100)\n",
            "Loss: 123.030 | Acc: 14.207% (1307/9200)\n",
            "Loss: 122.320 | Acc: 14.140% (1315/9300)\n",
            "Loss: 122.493 | Acc: 14.149% (1330/9400)\n",
            "Loss: 122.211 | Acc: 14.126% (1342/9500)\n",
            "Loss: 122.340 | Acc: 14.135% (1357/9600)\n",
            "Loss: 122.480 | Acc: 14.103% (1368/9700)\n",
            "Loss: 122.360 | Acc: 14.173% (1389/9800)\n",
            "Loss: 122.193 | Acc: 14.172% (1403/9900)\n",
            "Loss: 122.995 | Acc: 14.180% (1418/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 26\n",
            "Loss: 20.997 | Acc: 10.000% (1/10)\n",
            "Loss: 21.760 | Acc: 10.000% (2/20)\n",
            "Loss: 18.487 | Acc: 13.333% (4/30)\n",
            "Loss: 17.718 | Acc: 15.000% (6/40)\n",
            "Loss: 15.560 | Acc: 16.000% (8/50)\n",
            "Loss: 13.959 | Acc: 13.333% (8/60)\n",
            "Loss: 15.327 | Acc: 14.286% (10/70)\n",
            "Loss: 14.765 | Acc: 12.500% (10/80)\n",
            "Loss: 13.357 | Acc: 14.444% (13/90)\n",
            "Loss: 15.217 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 26\n",
            "Loss: 11.553 | Acc: 8.000% (8/100)\n",
            "Loss: 36.238 | Acc: 9.500% (19/200)\n",
            "Loss: 65.914 | Acc: 11.000% (33/300)\n",
            "Loss: 55.313 | Acc: 10.000% (40/400)\n",
            "Loss: 52.626 | Acc: 10.200% (51/500)\n",
            "Loss: 49.746 | Acc: 10.000% (60/600)\n",
            "Loss: 54.576 | Acc: 9.286% (65/700)\n",
            "Loss: 53.024 | Acc: 8.875% (71/800)\n",
            "Loss: 50.409 | Acc: 9.000% (81/900)\n",
            "Loss: 47.127 | Acc: 9.100% (91/1000)\n",
            "Loss: 46.710 | Acc: 9.545% (105/1100)\n",
            "Loss: 47.869 | Acc: 9.750% (117/1200)\n",
            "Loss: 48.768 | Acc: 9.462% (123/1300)\n",
            "Loss: 48.149 | Acc: 9.071% (127/1400)\n",
            "Loss: 48.000 | Acc: 9.133% (137/1500)\n",
            "Loss: 49.256 | Acc: 9.000% (144/1600)\n",
            "Loss: 50.511 | Acc: 8.647% (147/1700)\n",
            "Loss: 49.225 | Acc: 9.056% (163/1800)\n",
            "Loss: 50.908 | Acc: 8.789% (167/1900)\n",
            "Loss: 52.568 | Acc: 8.850% (177/2000)\n",
            "Loss: 54.366 | Acc: 8.857% (186/2100)\n",
            "Loss: 54.390 | Acc: 8.909% (196/2200)\n",
            "Loss: 53.286 | Acc: 9.043% (208/2300)\n",
            "Loss: 52.896 | Acc: 9.000% (216/2400)\n",
            "Loss: 53.814 | Acc: 8.960% (224/2500)\n",
            "Loss: 53.076 | Acc: 8.923% (232/2600)\n",
            "Loss: 52.290 | Acc: 8.963% (242/2700)\n",
            "Loss: 53.685 | Acc: 8.964% (251/2800)\n",
            "Loss: 56.668 | Acc: 8.862% (257/2900)\n",
            "Loss: 55.701 | Acc: 9.033% (271/3000)\n",
            "Loss: 55.920 | Acc: 8.968% (278/3100)\n",
            "Loss: 55.124 | Acc: 9.031% (289/3200)\n",
            "Loss: 54.495 | Acc: 9.000% (297/3300)\n",
            "Loss: 54.209 | Acc: 9.029% (307/3400)\n",
            "Loss: 55.090 | Acc: 9.171% (321/3500)\n",
            "Loss: 56.316 | Acc: 9.083% (327/3600)\n",
            "Loss: 57.255 | Acc: 9.108% (337/3700)\n",
            "Loss: 57.866 | Acc: 8.974% (341/3800)\n",
            "Loss: 58.600 | Acc: 8.923% (348/3900)\n",
            "Loss: 57.945 | Acc: 8.900% (356/4000)\n",
            "Loss: 57.289 | Acc: 8.927% (366/4100)\n",
            "Loss: 57.542 | Acc: 8.976% (377/4200)\n",
            "Loss: 57.793 | Acc: 9.000% (387/4300)\n",
            "Loss: 58.144 | Acc: 9.023% (397/4400)\n",
            "Loss: 58.124 | Acc: 9.044% (407/4500)\n",
            "Loss: 57.807 | Acc: 9.000% (414/4600)\n",
            "Loss: 57.473 | Acc: 8.936% (420/4700)\n",
            "Loss: 57.468 | Acc: 8.958% (430/4800)\n",
            "Loss: 57.523 | Acc: 8.918% (437/4900)\n",
            "Loss: 57.934 | Acc: 8.900% (445/5000)\n",
            "Loss: 57.419 | Acc: 8.863% (452/5100)\n",
            "Loss: 57.442 | Acc: 8.904% (463/5200)\n",
            "Loss: 57.488 | Acc: 9.019% (478/5300)\n",
            "Loss: 57.407 | Acc: 9.093% (491/5400)\n",
            "Loss: 58.647 | Acc: 9.091% (500/5500)\n",
            "Loss: 58.168 | Acc: 9.107% (510/5600)\n",
            "Loss: 58.095 | Acc: 9.018% (514/5700)\n",
            "Loss: 58.381 | Acc: 9.069% (526/5800)\n",
            "Loss: 57.802 | Acc: 9.068% (535/5900)\n",
            "Loss: 57.439 | Acc: 9.017% (541/6000)\n",
            "Loss: 57.194 | Acc: 9.115% (556/6100)\n",
            "Loss: 56.735 | Acc: 9.081% (563/6200)\n",
            "Loss: 56.779 | Acc: 9.048% (570/6300)\n",
            "Loss: 56.477 | Acc: 9.094% (582/6400)\n",
            "Loss: 56.328 | Acc: 9.123% (593/6500)\n",
            "Loss: 55.856 | Acc: 9.121% (602/6600)\n",
            "Loss: 55.974 | Acc: 9.269% (621/6700)\n",
            "Loss: 57.053 | Acc: 9.265% (630/6800)\n",
            "Loss: 56.736 | Acc: 9.275% (640/6900)\n",
            "Loss: 56.907 | Acc: 9.257% (648/7000)\n",
            "Loss: 56.746 | Acc: 9.225% (655/7100)\n",
            "Loss: 56.688 | Acc: 9.278% (668/7200)\n",
            "Loss: 56.228 | Acc: 9.205% (672/7300)\n",
            "Loss: 55.628 | Acc: 9.162% (678/7400)\n",
            "Loss: 55.689 | Acc: 9.187% (689/7500)\n",
            "Loss: 56.083 | Acc: 9.184% (698/7600)\n",
            "Loss: 56.633 | Acc: 9.221% (710/7700)\n",
            "Loss: 56.966 | Acc: 9.218% (719/7800)\n",
            "Loss: 56.705 | Acc: 9.278% (733/7900)\n",
            "Loss: 56.740 | Acc: 9.262% (741/8000)\n",
            "Loss: 57.025 | Acc: 9.284% (752/8100)\n",
            "Loss: 57.271 | Acc: 9.256% (759/8200)\n",
            "Loss: 57.512 | Acc: 9.205% (764/8300)\n",
            "Loss: 56.984 | Acc: 9.190% (772/8400)\n",
            "Loss: 57.060 | Acc: 9.247% (786/8500)\n",
            "Loss: 56.806 | Acc: 9.233% (794/8600)\n",
            "Loss: 56.709 | Acc: 9.172% (798/8700)\n",
            "Loss: 56.440 | Acc: 9.159% (806/8800)\n",
            "Loss: 56.704 | Acc: 9.202% (819/8900)\n",
            "Loss: 56.516 | Acc: 9.233% (831/9000)\n",
            "Loss: 56.341 | Acc: 9.286% (845/9100)\n",
            "Loss: 56.106 | Acc: 9.272% (853/9200)\n",
            "Loss: 55.808 | Acc: 9.269% (862/9300)\n",
            "Loss: 56.072 | Acc: 9.213% (866/9400)\n",
            "Loss: 55.939 | Acc: 9.242% (878/9500)\n",
            "Loss: 56.159 | Acc: 9.271% (890/9600)\n",
            "Loss: 56.277 | Acc: 9.268% (899/9700)\n",
            "Loss: 56.135 | Acc: 9.255% (907/9800)\n",
            "Loss: 55.985 | Acc: 9.303% (921/9900)\n",
            "Loss: 56.526 | Acc: 9.310% (931/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 27\n",
            "Loss: 6.548 | Acc: 10.000% (1/10)\n",
            "Loss: 19.126 | Acc: 10.000% (2/20)\n",
            "Loss: 14.112 | Acc: 6.667% (2/30)\n",
            "Loss: 17.652 | Acc: 7.500% (3/40)\n",
            "Loss: 17.166 | Acc: 10.000% (5/50)\n",
            "Loss: 16.192 | Acc: 13.333% (8/60)\n",
            "Loss: 15.460 | Acc: 11.429% (8/70)\n",
            "Loss: 14.859 | Acc: 10.000% (8/80)\n",
            "Loss: 15.658 | Acc: 10.000% (9/90)\n",
            "Loss: 15.253 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 27\n",
            "Loss: 48.104 | Acc: 9.000% (9/100)\n",
            "Loss: 57.365 | Acc: 14.000% (28/200)\n",
            "Loss: 71.121 | Acc: 13.333% (40/300)\n",
            "Loss: 60.538 | Acc: 14.250% (57/400)\n",
            "Loss: 59.129 | Acc: 14.200% (71/500)\n",
            "Loss: 54.504 | Acc: 14.000% (84/600)\n",
            "Loss: 60.677 | Acc: 13.429% (94/700)\n",
            "Loss: 61.096 | Acc: 13.875% (111/800)\n",
            "Loss: 60.002 | Acc: 13.667% (123/900)\n",
            "Loss: 55.585 | Acc: 13.500% (135/1000)\n",
            "Loss: 60.779 | Acc: 13.182% (145/1100)\n",
            "Loss: 62.322 | Acc: 13.167% (158/1200)\n",
            "Loss: 62.363 | Acc: 13.308% (173/1300)\n",
            "Loss: 60.163 | Acc: 13.500% (189/1400)\n",
            "Loss: 60.623 | Acc: 13.067% (196/1500)\n",
            "Loss: 60.904 | Acc: 12.938% (207/1600)\n",
            "Loss: 63.463 | Acc: 12.765% (217/1700)\n",
            "Loss: 62.812 | Acc: 12.722% (229/1800)\n",
            "Loss: 64.981 | Acc: 12.684% (241/1900)\n",
            "Loss: 66.714 | Acc: 12.600% (252/2000)\n",
            "Loss: 68.674 | Acc: 12.857% (270/2100)\n",
            "Loss: 68.909 | Acc: 12.909% (284/2200)\n",
            "Loss: 67.104 | Acc: 12.913% (297/2300)\n",
            "Loss: 66.851 | Acc: 13.000% (312/2400)\n",
            "Loss: 66.452 | Acc: 12.880% (322/2500)\n",
            "Loss: 64.964 | Acc: 13.038% (339/2600)\n",
            "Loss: 65.089 | Acc: 13.037% (352/2700)\n",
            "Loss: 66.587 | Acc: 13.036% (365/2800)\n",
            "Loss: 67.538 | Acc: 13.069% (379/2900)\n",
            "Loss: 66.671 | Acc: 13.067% (392/3000)\n",
            "Loss: 65.215 | Acc: 13.032% (404/3100)\n",
            "Loss: 64.681 | Acc: 12.969% (415/3200)\n",
            "Loss: 64.229 | Acc: 12.758% (421/3300)\n",
            "Loss: 63.829 | Acc: 12.765% (434/3400)\n",
            "Loss: 66.031 | Acc: 12.771% (447/3500)\n",
            "Loss: 65.494 | Acc: 12.861% (463/3600)\n",
            "Loss: 66.001 | Acc: 13.000% (481/3700)\n",
            "Loss: 68.902 | Acc: 13.026% (495/3800)\n",
            "Loss: 70.097 | Acc: 12.974% (506/3900)\n",
            "Loss: 69.581 | Acc: 13.025% (521/4000)\n",
            "Loss: 68.610 | Acc: 12.951% (531/4100)\n",
            "Loss: 69.582 | Acc: 12.952% (544/4200)\n",
            "Loss: 69.606 | Acc: 12.930% (556/4300)\n",
            "Loss: 70.148 | Acc: 12.909% (568/4400)\n",
            "Loss: 70.118 | Acc: 12.911% (581/4500)\n",
            "Loss: 69.300 | Acc: 12.870% (592/4600)\n",
            "Loss: 69.020 | Acc: 12.766% (600/4700)\n",
            "Loss: 69.804 | Acc: 12.708% (610/4800)\n",
            "Loss: 70.867 | Acc: 12.714% (623/4900)\n",
            "Loss: 71.137 | Acc: 12.600% (630/5000)\n",
            "Loss: 70.647 | Acc: 12.569% (641/5100)\n",
            "Loss: 70.080 | Acc: 12.596% (655/5200)\n",
            "Loss: 69.846 | Acc: 12.566% (666/5300)\n",
            "Loss: 69.847 | Acc: 12.519% (676/5400)\n",
            "Loss: 71.521 | Acc: 12.473% (686/5500)\n",
            "Loss: 71.226 | Acc: 12.446% (697/5600)\n",
            "Loss: 71.056 | Acc: 12.386% (706/5700)\n",
            "Loss: 70.363 | Acc: 12.466% (723/5800)\n",
            "Loss: 69.444 | Acc: 12.424% (733/5900)\n",
            "Loss: 68.531 | Acc: 12.383% (743/6000)\n",
            "Loss: 68.274 | Acc: 12.410% (757/6100)\n",
            "Loss: 68.326 | Acc: 12.500% (775/6200)\n",
            "Loss: 67.676 | Acc: 12.524% (789/6300)\n",
            "Loss: 67.437 | Acc: 12.547% (803/6400)\n",
            "Loss: 67.712 | Acc: 12.523% (814/6500)\n",
            "Loss: 67.096 | Acc: 12.561% (829/6600)\n",
            "Loss: 66.363 | Acc: 12.522% (839/6700)\n",
            "Loss: 66.963 | Acc: 12.588% (856/6800)\n",
            "Loss: 66.495 | Acc: 12.551% (866/6900)\n",
            "Loss: 67.070 | Acc: 12.543% (878/7000)\n",
            "Loss: 66.539 | Acc: 12.592% (894/7100)\n",
            "Loss: 66.728 | Acc: 12.556% (904/7200)\n",
            "Loss: 66.346 | Acc: 12.534% (915/7300)\n",
            "Loss: 65.917 | Acc: 12.554% (929/7400)\n",
            "Loss: 65.999 | Acc: 12.560% (942/7500)\n",
            "Loss: 66.371 | Acc: 12.632% (960/7600)\n",
            "Loss: 66.327 | Acc: 12.662% (975/7700)\n",
            "Loss: 66.134 | Acc: 12.641% (986/7800)\n",
            "Loss: 66.442 | Acc: 12.671% (1001/7900)\n",
            "Loss: 65.807 | Acc: 12.662% (1013/8000)\n",
            "Loss: 66.065 | Acc: 12.630% (1023/8100)\n",
            "Loss: 65.809 | Acc: 12.549% (1029/8200)\n",
            "Loss: 65.430 | Acc: 12.482% (1036/8300)\n",
            "Loss: 64.777 | Acc: 12.452% (1046/8400)\n",
            "Loss: 65.269 | Acc: 12.518% (1064/8500)\n",
            "Loss: 65.527 | Acc: 12.419% (1068/8600)\n",
            "Loss: 65.825 | Acc: 12.414% (1080/8700)\n",
            "Loss: 65.365 | Acc: 12.398% (1091/8800)\n",
            "Loss: 65.727 | Acc: 12.416% (1105/8900)\n",
            "Loss: 65.872 | Acc: 12.444% (1120/9000)\n",
            "Loss: 65.778 | Acc: 12.571% (1144/9100)\n",
            "Loss: 65.344 | Acc: 12.554% (1155/9200)\n",
            "Loss: 64.892 | Acc: 12.538% (1166/9300)\n",
            "Loss: 65.098 | Acc: 12.521% (1177/9400)\n",
            "Loss: 64.995 | Acc: 12.558% (1193/9500)\n",
            "Loss: 64.734 | Acc: 12.542% (1204/9600)\n",
            "Loss: 64.736 | Acc: 12.536% (1216/9700)\n",
            "Loss: 64.321 | Acc: 12.582% (1233/9800)\n",
            "Loss: 63.908 | Acc: 12.576% (1245/9900)\n",
            "Loss: 64.661 | Acc: 12.580% (1258/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 28\n",
            "Loss: 7.416 | Acc: 0.000% (0/10)\n",
            "Loss: 11.762 | Acc: 5.000% (1/20)\n",
            "Loss: 12.695 | Acc: 6.667% (2/30)\n",
            "Loss: 11.839 | Acc: 5.000% (2/40)\n",
            "Loss: 11.135 | Acc: 6.000% (3/50)\n",
            "Loss: 11.839 | Acc: 6.667% (4/60)\n",
            "Loss: 11.232 | Acc: 7.143% (5/70)\n",
            "Loss: 13.883 | Acc: 6.250% (5/80)\n",
            "Loss: 14.217 | Acc: 6.667% (6/90)\n",
            "Loss: 15.731 | Acc: 7.000% (7/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 28\n",
            "Loss: 143.043 | Acc: 8.000% (8/100)\n",
            "Loss: 107.632 | Acc: 9.500% (19/200)\n",
            "Loss: 113.829 | Acc: 11.333% (34/300)\n",
            "Loss: 99.088 | Acc: 10.500% (42/400)\n",
            "Loss: 91.252 | Acc: 10.800% (54/500)\n",
            "Loss: 90.110 | Acc: 10.667% (64/600)\n",
            "Loss: 99.982 | Acc: 10.000% (70/700)\n",
            "Loss: 98.137 | Acc: 9.500% (76/800)\n",
            "Loss: 100.665 | Acc: 9.778% (88/900)\n",
            "Loss: 94.049 | Acc: 9.800% (98/1000)\n",
            "Loss: 99.619 | Acc: 10.273% (113/1100)\n",
            "Loss: 96.584 | Acc: 10.417% (125/1200)\n",
            "Loss: 94.236 | Acc: 10.077% (131/1300)\n",
            "Loss: 95.505 | Acc: 9.714% (136/1400)\n",
            "Loss: 93.177 | Acc: 9.733% (146/1500)\n",
            "Loss: 94.842 | Acc: 9.562% (153/1600)\n",
            "Loss: 105.512 | Acc: 9.235% (157/1700)\n",
            "Loss: 104.126 | Acc: 9.611% (173/1800)\n",
            "Loss: 109.163 | Acc: 9.368% (178/1900)\n",
            "Loss: 112.946 | Acc: 9.400% (188/2000)\n",
            "Loss: 111.929 | Acc: 9.524% (200/2100)\n",
            "Loss: 114.383 | Acc: 9.545% (210/2200)\n",
            "Loss: 109.980 | Acc: 9.565% (220/2300)\n",
            "Loss: 109.726 | Acc: 9.500% (228/2400)\n",
            "Loss: 111.217 | Acc: 9.440% (236/2500)\n",
            "Loss: 110.364 | Acc: 9.385% (244/2600)\n",
            "Loss: 112.551 | Acc: 9.407% (254/2700)\n",
            "Loss: 119.197 | Acc: 9.321% (261/2800)\n",
            "Loss: 121.749 | Acc: 9.172% (266/2900)\n",
            "Loss: 119.591 | Acc: 9.233% (277/3000)\n",
            "Loss: 116.729 | Acc: 9.161% (284/3100)\n",
            "Loss: 117.078 | Acc: 9.250% (296/3200)\n",
            "Loss: 115.461 | Acc: 9.242% (305/3300)\n",
            "Loss: 114.753 | Acc: 9.235% (314/3400)\n",
            "Loss: 116.147 | Acc: 9.400% (329/3500)\n",
            "Loss: 117.126 | Acc: 9.306% (335/3600)\n",
            "Loss: 115.726 | Acc: 9.351% (346/3700)\n",
            "Loss: 117.812 | Acc: 9.211% (350/3800)\n",
            "Loss: 119.611 | Acc: 9.154% (357/3900)\n",
            "Loss: 119.960 | Acc: 9.125% (365/4000)\n",
            "Loss: 118.610 | Acc: 9.171% (376/4100)\n",
            "Loss: 119.621 | Acc: 9.262% (389/4200)\n",
            "Loss: 118.115 | Acc: 9.349% (402/4300)\n",
            "Loss: 119.467 | Acc: 9.341% (411/4400)\n",
            "Loss: 121.075 | Acc: 9.333% (420/4500)\n",
            "Loss: 120.540 | Acc: 9.283% (427/4600)\n",
            "Loss: 120.153 | Acc: 9.213% (433/4700)\n",
            "Loss: 120.906 | Acc: 9.229% (443/4800)\n",
            "Loss: 121.467 | Acc: 9.184% (450/4900)\n",
            "Loss: 121.205 | Acc: 9.120% (456/5000)\n",
            "Loss: 121.869 | Acc: 9.078% (463/5100)\n",
            "Loss: 120.557 | Acc: 9.096% (473/5200)\n",
            "Loss: 120.005 | Acc: 9.226% (489/5300)\n",
            "Loss: 119.206 | Acc: 9.296% (502/5400)\n",
            "Loss: 121.571 | Acc: 9.273% (510/5500)\n",
            "Loss: 122.709 | Acc: 9.268% (519/5600)\n",
            "Loss: 123.302 | Acc: 9.211% (525/5700)\n",
            "Loss: 122.508 | Acc: 9.259% (537/5800)\n",
            "Loss: 121.249 | Acc: 9.254% (546/5900)\n",
            "Loss: 119.955 | Acc: 9.200% (552/6000)\n",
            "Loss: 120.923 | Acc: 9.295% (567/6100)\n",
            "Loss: 122.207 | Acc: 9.290% (576/6200)\n",
            "Loss: 121.523 | Acc: 9.286% (585/6300)\n",
            "Loss: 121.553 | Acc: 9.344% (598/6400)\n",
            "Loss: 121.996 | Acc: 9.369% (609/6500)\n",
            "Loss: 121.140 | Acc: 9.364% (618/6600)\n",
            "Loss: 121.561 | Acc: 9.478% (635/6700)\n",
            "Loss: 121.812 | Acc: 9.471% (644/6800)\n",
            "Loss: 121.537 | Acc: 9.478% (654/6900)\n",
            "Loss: 120.890 | Acc: 9.457% (662/7000)\n",
            "Loss: 120.580 | Acc: 9.423% (669/7100)\n",
            "Loss: 120.813 | Acc: 9.472% (682/7200)\n",
            "Loss: 120.419 | Acc: 9.397% (686/7300)\n",
            "Loss: 119.788 | Acc: 9.338% (691/7400)\n",
            "Loss: 119.057 | Acc: 9.347% (701/7500)\n",
            "Loss: 119.580 | Acc: 9.342% (710/7600)\n",
            "Loss: 118.877 | Acc: 9.403% (724/7700)\n",
            "Loss: 119.506 | Acc: 9.423% (735/7800)\n",
            "Loss: 120.559 | Acc: 9.506% (751/7900)\n",
            "Loss: 120.883 | Acc: 9.488% (759/8000)\n",
            "Loss: 121.893 | Acc: 9.494% (769/8100)\n",
            "Loss: 121.536 | Acc: 9.451% (775/8200)\n",
            "Loss: 121.043 | Acc: 9.398% (780/8300)\n",
            "Loss: 119.938 | Acc: 9.369% (787/8400)\n",
            "Loss: 119.344 | Acc: 9.400% (799/8500)\n",
            "Loss: 120.073 | Acc: 9.384% (807/8600)\n",
            "Loss: 120.209 | Acc: 9.310% (810/8700)\n",
            "Loss: 119.175 | Acc: 9.284% (817/8800)\n",
            "Loss: 119.494 | Acc: 9.326% (830/8900)\n",
            "Loss: 120.056 | Acc: 9.356% (842/9000)\n",
            "Loss: 120.821 | Acc: 9.363% (852/9100)\n",
            "Loss: 120.379 | Acc: 9.348% (860/9200)\n",
            "Loss: 119.958 | Acc: 9.344% (869/9300)\n",
            "Loss: 119.765 | Acc: 9.287% (873/9400)\n",
            "Loss: 119.694 | Acc: 9.305% (884/9500)\n",
            "Loss: 119.707 | Acc: 9.323% (895/9600)\n",
            "Loss: 119.279 | Acc: 9.320% (904/9700)\n",
            "Loss: 118.773 | Acc: 9.306% (912/9800)\n",
            "Loss: 118.844 | Acc: 9.354% (926/9900)\n",
            "Loss: 119.330 | Acc: 9.350% (935/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 29\n",
            "Loss: 17.710 | Acc: 0.000% (0/10)\n",
            "Loss: 11.679 | Acc: 10.000% (2/20)\n",
            "Loss: 11.513 | Acc: 16.667% (5/30)\n",
            "Loss: 12.016 | Acc: 12.500% (5/40)\n",
            "Loss: 11.774 | Acc: 10.000% (5/50)\n",
            "Loss: 11.259 | Acc: 8.333% (5/60)\n",
            "Loss: 11.032 | Acc: 10.000% (7/70)\n",
            "Loss: 12.588 | Acc: 10.000% (8/80)\n",
            "Loss: 13.599 | Acc: 11.111% (10/90)\n",
            "Loss: 13.366 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 29\n",
            "Loss: 31.622 | Acc: 12.000% (12/100)\n",
            "Loss: 23.093 | Acc: 13.500% (27/200)\n",
            "Loss: 21.655 | Acc: 13.000% (39/300)\n",
            "Loss: 17.729 | Acc: 12.250% (49/400)\n",
            "Loss: 15.172 | Acc: 11.600% (58/500)\n",
            "Loss: 14.199 | Acc: 10.333% (62/600)\n",
            "Loss: 14.392 | Acc: 10.143% (71/700)\n",
            "Loss: 13.563 | Acc: 9.875% (79/800)\n",
            "Loss: 16.371 | Acc: 10.333% (93/900)\n",
            "Loss: 15.426 | Acc: 10.600% (106/1000)\n",
            "Loss: 16.598 | Acc: 10.455% (115/1100)\n",
            "Loss: 17.842 | Acc: 10.333% (124/1200)\n",
            "Loss: 17.600 | Acc: 10.615% (138/1300)\n",
            "Loss: 17.747 | Acc: 10.286% (144/1400)\n",
            "Loss: 17.185 | Acc: 10.400% (156/1500)\n",
            "Loss: 16.903 | Acc: 10.125% (162/1600)\n",
            "Loss: 18.041 | Acc: 10.235% (174/1700)\n",
            "Loss: 17.777 | Acc: 10.167% (183/1800)\n",
            "Loss: 17.913 | Acc: 10.211% (194/1900)\n",
            "Loss: 17.776 | Acc: 10.150% (203/2000)\n",
            "Loss: 18.096 | Acc: 10.048% (211/2100)\n",
            "Loss: 17.871 | Acc: 9.955% (219/2200)\n",
            "Loss: 17.697 | Acc: 10.130% (233/2300)\n",
            "Loss: 17.671 | Acc: 9.917% (238/2400)\n",
            "Loss: 17.861 | Acc: 10.000% (250/2500)\n",
            "Loss: 17.381 | Acc: 10.038% (261/2600)\n",
            "Loss: 17.084 | Acc: 9.963% (269/2700)\n",
            "Loss: 17.447 | Acc: 9.750% (273/2800)\n",
            "Loss: 17.329 | Acc: 9.655% (280/2900)\n",
            "Loss: 16.907 | Acc: 9.767% (293/3000)\n",
            "Loss: 16.539 | Acc: 9.806% (304/3100)\n",
            "Loss: 16.481 | Acc: 9.812% (314/3200)\n",
            "Loss: 16.423 | Acc: 9.758% (322/3300)\n",
            "Loss: 16.155 | Acc: 9.794% (333/3400)\n",
            "Loss: 16.724 | Acc: 9.657% (338/3500)\n",
            "Loss: 17.263 | Acc: 9.722% (350/3600)\n",
            "Loss: 17.423 | Acc: 9.730% (360/3700)\n",
            "Loss: 18.157 | Acc: 9.632% (366/3800)\n",
            "Loss: 18.613 | Acc: 9.769% (381/3900)\n",
            "Loss: 18.572 | Acc: 9.775% (391/4000)\n",
            "Loss: 18.346 | Acc: 9.902% (406/4100)\n",
            "Loss: 18.671 | Acc: 9.905% (416/4200)\n",
            "Loss: 18.612 | Acc: 9.860% (424/4300)\n",
            "Loss: 18.594 | Acc: 9.886% (435/4400)\n",
            "Loss: 18.796 | Acc: 9.889% (445/4500)\n",
            "Loss: 18.481 | Acc: 9.826% (452/4600)\n",
            "Loss: 18.216 | Acc: 9.766% (459/4700)\n",
            "Loss: 18.278 | Acc: 9.750% (468/4800)\n",
            "Loss: 18.289 | Acc: 9.714% (476/4900)\n",
            "Loss: 18.140 | Acc: 9.700% (485/5000)\n",
            "Loss: 18.018 | Acc: 9.686% (494/5100)\n",
            "Loss: 17.797 | Acc: 9.692% (504/5200)\n",
            "Loss: 17.730 | Acc: 9.604% (509/5300)\n",
            "Loss: 17.560 | Acc: 9.630% (520/5400)\n",
            "Loss: 17.802 | Acc: 9.673% (532/5500)\n",
            "Loss: 18.133 | Acc: 9.571% (536/5600)\n",
            "Loss: 18.256 | Acc: 9.614% (548/5700)\n",
            "Loss: 18.050 | Acc: 9.603% (557/5800)\n",
            "Loss: 17.846 | Acc: 9.576% (565/5900)\n",
            "Loss: 17.618 | Acc: 9.517% (571/6000)\n",
            "Loss: 17.684 | Acc: 9.508% (580/6100)\n",
            "Loss: 17.759 | Acc: 9.468% (587/6200)\n",
            "Loss: 17.783 | Acc: 9.492% (598/6300)\n",
            "Loss: 17.814 | Acc: 9.516% (609/6400)\n",
            "Loss: 17.773 | Acc: 9.600% (624/6500)\n",
            "Loss: 17.706 | Acc: 9.545% (630/6600)\n",
            "Loss: 17.831 | Acc: 9.463% (634/6700)\n",
            "Loss: 17.725 | Acc: 9.574% (651/6800)\n",
            "Loss: 17.580 | Acc: 9.536% (658/6900)\n",
            "Loss: 17.530 | Acc: 9.571% (670/7000)\n",
            "Loss: 17.494 | Acc: 9.521% (676/7100)\n",
            "Loss: 17.967 | Acc: 9.472% (682/7200)\n",
            "Loss: 17.841 | Acc: 9.425% (688/7300)\n",
            "Loss: 17.705 | Acc: 9.432% (698/7400)\n",
            "Loss: 17.696 | Acc: 9.453% (709/7500)\n",
            "Loss: 17.827 | Acc: 9.513% (723/7600)\n",
            "Loss: 17.701 | Acc: 9.571% (737/7700)\n",
            "Loss: 17.631 | Acc: 9.577% (747/7800)\n",
            "Loss: 17.828 | Acc: 9.557% (755/7900)\n",
            "Loss: 17.921 | Acc: 9.537% (763/8000)\n",
            "Loss: 17.778 | Acc: 9.519% (771/8100)\n",
            "Loss: 17.614 | Acc: 9.598% (787/8200)\n",
            "Loss: 17.462 | Acc: 9.663% (802/8300)\n",
            "Loss: 17.464 | Acc: 9.655% (811/8400)\n",
            "Loss: 17.489 | Acc: 9.659% (821/8500)\n",
            "Loss: 17.640 | Acc: 9.698% (834/8600)\n",
            "Loss: 17.760 | Acc: 9.701% (844/8700)\n",
            "Loss: 17.599 | Acc: 9.716% (855/8800)\n",
            "Loss: 17.678 | Acc: 9.775% (870/8900)\n",
            "Loss: 17.752 | Acc: 9.767% (879/9000)\n",
            "Loss: 17.777 | Acc: 9.758% (888/9100)\n",
            "Loss: 17.681 | Acc: 9.772% (899/9200)\n",
            "Loss: 17.578 | Acc: 9.796% (911/9300)\n",
            "Loss: 17.575 | Acc: 9.777% (919/9400)\n",
            "Loss: 17.546 | Acc: 9.779% (929/9500)\n",
            "Loss: 17.523 | Acc: 9.729% (934/9600)\n",
            "Loss: 17.451 | Acc: 9.722% (943/9700)\n",
            "Loss: 17.380 | Acc: 9.745% (955/9800)\n",
            "Loss: 17.409 | Acc: 9.737% (964/9900)\n",
            "Loss: 17.592 | Acc: 9.780% (978/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DB-yPhzj4CJ",
        "colab_type": "code",
        "outputId": "d7be277e-e2a1-4099-b1c5-a4814648d23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 28799
        }
      },
      "source": [
        "lr = 0.00001\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "start_epoch = 50\n",
        "epochs = 20\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Train===\n",
            "Epoch: 50\n",
            "Loss: 2.528 | Acc: 30.000% (3/10)\n",
            "Loss: 2.500 | Acc: 25.000% (5/20)\n",
            "Loss: 2.737 | Acc: 16.667% (5/30)\n",
            "Loss: 2.539 | Acc: 22.500% (9/40)\n",
            "Loss: 2.499 | Acc: 26.000% (13/50)\n",
            "Loss: 2.406 | Acc: 23.333% (14/60)\n",
            "Loss: 2.330 | Acc: 25.714% (18/70)\n",
            "Loss: 2.289 | Acc: 26.250% (21/80)\n",
            "Loss: 2.305 | Acc: 26.667% (24/90)\n",
            "Loss: 2.317 | Acc: 24.000% (24/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 50\n",
            "Loss: 9.139 | Acc: 10.000% (10/100)\n",
            "Loss: 10.863 | Acc: 9.500% (19/200)\n",
            "Loss: 8.570 | Acc: 11.333% (34/300)\n",
            "Loss: 8.303 | Acc: 11.250% (45/400)\n",
            "Loss: 7.425 | Acc: 12.200% (61/500)\n",
            "Loss: 7.386 | Acc: 12.833% (77/600)\n",
            "Loss: 6.930 | Acc: 13.429% (94/700)\n",
            "Loss: 6.896 | Acc: 13.250% (106/800)\n",
            "Loss: 6.768 | Acc: 13.556% (122/900)\n",
            "Loss: 6.631 | Acc: 13.200% (132/1000)\n",
            "Loss: 6.593 | Acc: 12.636% (139/1100)\n",
            "Loss: 6.379 | Acc: 12.583% (151/1200)\n",
            "Loss: 6.514 | Acc: 12.385% (161/1300)\n",
            "Loss: 6.694 | Acc: 12.786% (179/1400)\n",
            "Loss: 6.870 | Acc: 12.867% (193/1500)\n",
            "Loss: 6.607 | Acc: 13.375% (214/1600)\n",
            "Loss: 6.482 | Acc: 13.647% (232/1700)\n",
            "Loss: 6.636 | Acc: 13.500% (243/1800)\n",
            "Loss: 6.907 | Acc: 13.421% (255/1900)\n",
            "Loss: 6.900 | Acc: 13.600% (272/2000)\n",
            "Loss: 6.992 | Acc: 13.381% (281/2100)\n",
            "Loss: 6.908 | Acc: 13.591% (299/2200)\n",
            "Loss: 6.799 | Acc: 13.783% (317/2300)\n",
            "Loss: 6.816 | Acc: 13.625% (327/2400)\n",
            "Loss: 6.796 | Acc: 13.640% (341/2500)\n",
            "Loss: 6.679 | Acc: 13.808% (359/2600)\n",
            "Loss: 6.591 | Acc: 13.926% (376/2700)\n",
            "Loss: 6.568 | Acc: 13.964% (391/2800)\n",
            "Loss: 6.568 | Acc: 14.000% (406/2900)\n",
            "Loss: 6.610 | Acc: 14.033% (421/3000)\n",
            "Loss: 6.545 | Acc: 14.000% (434/3100)\n",
            "Loss: 6.483 | Acc: 13.906% (445/3200)\n",
            "Loss: 6.471 | Acc: 14.061% (464/3300)\n",
            "Loss: 6.559 | Acc: 14.118% (480/3400)\n",
            "Loss: 6.493 | Acc: 14.229% (498/3500)\n",
            "Loss: 6.435 | Acc: 14.417% (519/3600)\n",
            "Loss: 6.364 | Acc: 14.541% (538/3700)\n",
            "Loss: 6.326 | Acc: 14.632% (556/3800)\n",
            "Loss: 6.281 | Acc: 14.615% (570/3900)\n",
            "Loss: 6.343 | Acc: 14.575% (583/4000)\n",
            "Loss: 6.287 | Acc: 14.610% (599/4100)\n",
            "Loss: 6.225 | Acc: 14.595% (613/4200)\n",
            "Loss: 6.234 | Acc: 14.651% (630/4300)\n",
            "Loss: 6.233 | Acc: 14.727% (648/4400)\n",
            "Loss: 6.283 | Acc: 14.667% (660/4500)\n",
            "Loss: 6.239 | Acc: 14.761% (679/4600)\n",
            "Loss: 6.270 | Acc: 14.787% (695/4700)\n",
            "Loss: 6.256 | Acc: 14.875% (714/4800)\n",
            "Loss: 6.241 | Acc: 14.898% (730/4900)\n",
            "Loss: 6.182 | Acc: 14.820% (741/5000)\n",
            "Loss: 6.137 | Acc: 14.863% (758/5100)\n",
            "Loss: 6.112 | Acc: 14.827% (771/5200)\n",
            "Loss: 6.284 | Acc: 14.868% (788/5300)\n",
            "Loss: 6.256 | Acc: 14.815% (800/5400)\n",
            "Loss: 6.217 | Acc: 14.818% (815/5500)\n",
            "Loss: 6.222 | Acc: 14.839% (831/5600)\n",
            "Loss: 6.167 | Acc: 14.860% (847/5700)\n",
            "Loss: 6.169 | Acc: 14.793% (858/5800)\n",
            "Loss: 6.136 | Acc: 14.763% (871/5900)\n",
            "Loss: 6.124 | Acc: 14.867% (892/6000)\n",
            "Loss: 6.111 | Acc: 14.869% (907/6100)\n",
            "Loss: 6.080 | Acc: 14.887% (923/6200)\n",
            "Loss: 6.068 | Acc: 14.810% (933/6300)\n",
            "Loss: 6.088 | Acc: 14.750% (944/6400)\n",
            "Loss: 6.091 | Acc: 14.815% (963/6500)\n",
            "Loss: 6.081 | Acc: 14.742% (973/6600)\n",
            "Loss: 6.055 | Acc: 14.761% (989/6700)\n",
            "Loss: 6.011 | Acc: 14.691% (999/6800)\n",
            "Loss: 5.987 | Acc: 14.652% (1011/6900)\n",
            "Loss: 5.978 | Acc: 14.657% (1026/7000)\n",
            "Loss: 5.987 | Acc: 14.662% (1041/7100)\n",
            "Loss: 6.018 | Acc: 14.667% (1056/7200)\n",
            "Loss: 6.028 | Acc: 14.685% (1072/7300)\n",
            "Loss: 6.012 | Acc: 14.730% (1090/7400)\n",
            "Loss: 6.094 | Acc: 14.667% (1100/7500)\n",
            "Loss: 6.059 | Acc: 14.697% (1117/7600)\n",
            "Loss: 6.058 | Acc: 14.662% (1129/7700)\n",
            "Loss: 6.034 | Acc: 14.731% (1149/7800)\n",
            "Loss: 6.069 | Acc: 14.696% (1161/7900)\n",
            "Loss: 6.076 | Acc: 14.575% (1166/8000)\n",
            "Loss: 6.082 | Acc: 14.543% (1178/8100)\n",
            "Loss: 6.095 | Acc: 14.537% (1192/8200)\n",
            "Loss: 6.093 | Acc: 14.458% (1200/8300)\n",
            "Loss: 6.133 | Acc: 14.488% (1217/8400)\n",
            "Loss: 6.170 | Acc: 14.471% (1230/8500)\n",
            "Loss: 6.160 | Acc: 14.453% (1243/8600)\n",
            "Loss: 6.144 | Acc: 14.460% (1258/8700)\n",
            "Loss: 6.136 | Acc: 14.409% (1268/8800)\n",
            "Loss: 6.105 | Acc: 14.416% (1283/8900)\n",
            "Loss: 6.093 | Acc: 14.422% (1298/9000)\n",
            "Loss: 6.064 | Acc: 14.374% (1308/9100)\n",
            "Loss: 6.075 | Acc: 14.402% (1325/9200)\n",
            "Loss: 6.041 | Acc: 14.344% (1334/9300)\n",
            "Loss: 6.014 | Acc: 14.372% (1351/9400)\n",
            "Loss: 6.020 | Acc: 14.316% (1360/9500)\n",
            "Loss: 6.035 | Acc: 14.344% (1377/9600)\n",
            "Loss: 6.033 | Acc: 14.330% (1390/9700)\n",
            "Loss: 6.015 | Acc: 14.357% (1407/9800)\n",
            "Loss: 6.002 | Acc: 14.404% (1426/9900)\n",
            "Loss: 5.992 | Acc: 14.380% (1438/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 51\n",
            "Loss: 2.135 | Acc: 50.000% (5/10)\n",
            "Loss: 2.143 | Acc: 40.000% (8/20)\n",
            "Loss: 2.223 | Acc: 36.667% (11/30)\n",
            "Loss: 2.317 | Acc: 35.000% (14/40)\n",
            "Loss: 2.252 | Acc: 32.000% (16/50)\n",
            "Loss: 2.150 | Acc: 35.000% (21/60)\n",
            "Loss: 2.183 | Acc: 31.429% (22/70)\n",
            "Loss: 2.162 | Acc: 30.000% (24/80)\n",
            "Loss: 2.165 | Acc: 30.000% (27/90)\n",
            "Loss: 2.169 | Acc: 29.000% (29/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 51\n",
            "Loss: 9.048 | Acc: 13.000% (13/100)\n",
            "Loss: 10.913 | Acc: 10.500% (21/200)\n",
            "Loss: 8.683 | Acc: 12.333% (37/300)\n",
            "Loss: 8.400 | Acc: 12.250% (49/400)\n",
            "Loss: 7.539 | Acc: 12.600% (63/500)\n",
            "Loss: 7.491 | Acc: 13.167% (79/600)\n",
            "Loss: 7.038 | Acc: 13.714% (96/700)\n",
            "Loss: 6.994 | Acc: 13.500% (108/800)\n",
            "Loss: 6.869 | Acc: 13.778% (124/900)\n",
            "Loss: 6.727 | Acc: 13.500% (135/1000)\n",
            "Loss: 6.685 | Acc: 12.909% (142/1100)\n",
            "Loss: 6.461 | Acc: 12.917% (155/1200)\n",
            "Loss: 6.609 | Acc: 12.615% (164/1300)\n",
            "Loss: 6.789 | Acc: 13.000% (182/1400)\n",
            "Loss: 6.972 | Acc: 13.067% (196/1500)\n",
            "Loss: 6.705 | Acc: 13.500% (216/1600)\n",
            "Loss: 6.583 | Acc: 13.765% (234/1700)\n",
            "Loss: 6.742 | Acc: 13.611% (245/1800)\n",
            "Loss: 7.008 | Acc: 13.474% (256/1900)\n",
            "Loss: 7.002 | Acc: 13.700% (274/2000)\n",
            "Loss: 7.091 | Acc: 13.476% (283/2100)\n",
            "Loss: 7.004 | Acc: 13.636% (300/2200)\n",
            "Loss: 6.889 | Acc: 13.826% (318/2300)\n",
            "Loss: 6.906 | Acc: 13.667% (328/2400)\n",
            "Loss: 6.889 | Acc: 13.680% (342/2500)\n",
            "Loss: 6.775 | Acc: 13.846% (360/2600)\n",
            "Loss: 6.689 | Acc: 13.963% (377/2700)\n",
            "Loss: 6.668 | Acc: 14.000% (392/2800)\n",
            "Loss: 6.667 | Acc: 14.138% (410/2900)\n",
            "Loss: 6.710 | Acc: 14.200% (426/3000)\n",
            "Loss: 6.643 | Acc: 14.097% (437/3100)\n",
            "Loss: 6.578 | Acc: 14.031% (449/3200)\n",
            "Loss: 6.568 | Acc: 14.212% (469/3300)\n",
            "Loss: 6.651 | Acc: 14.265% (485/3400)\n",
            "Loss: 6.585 | Acc: 14.429% (505/3500)\n",
            "Loss: 6.527 | Acc: 14.556% (524/3600)\n",
            "Loss: 6.460 | Acc: 14.622% (541/3700)\n",
            "Loss: 6.426 | Acc: 14.737% (560/3800)\n",
            "Loss: 6.382 | Acc: 14.718% (574/3900)\n",
            "Loss: 6.443 | Acc: 14.675% (587/4000)\n",
            "Loss: 6.389 | Acc: 14.732% (604/4100)\n",
            "Loss: 6.329 | Acc: 14.667% (616/4200)\n",
            "Loss: 6.336 | Acc: 14.698% (632/4300)\n",
            "Loss: 6.334 | Acc: 14.750% (649/4400)\n",
            "Loss: 6.383 | Acc: 14.756% (664/4500)\n",
            "Loss: 6.338 | Acc: 14.848% (683/4600)\n",
            "Loss: 6.367 | Acc: 14.872% (699/4700)\n",
            "Loss: 6.357 | Acc: 15.000% (720/4800)\n",
            "Loss: 6.341 | Acc: 15.000% (735/4900)\n",
            "Loss: 6.283 | Acc: 14.940% (747/5000)\n",
            "Loss: 6.237 | Acc: 15.020% (766/5100)\n",
            "Loss: 6.210 | Acc: 15.000% (780/5200)\n",
            "Loss: 6.381 | Acc: 15.019% (796/5300)\n",
            "Loss: 6.353 | Acc: 14.944% (807/5400)\n",
            "Loss: 6.314 | Acc: 14.927% (821/5500)\n",
            "Loss: 6.321 | Acc: 14.946% (837/5600)\n",
            "Loss: 6.267 | Acc: 14.965% (853/5700)\n",
            "Loss: 6.269 | Acc: 14.897% (864/5800)\n",
            "Loss: 6.234 | Acc: 14.864% (877/5900)\n",
            "Loss: 6.220 | Acc: 14.967% (898/6000)\n",
            "Loss: 6.206 | Acc: 14.951% (912/6100)\n",
            "Loss: 6.176 | Acc: 14.968% (928/6200)\n",
            "Loss: 6.165 | Acc: 14.889% (938/6300)\n",
            "Loss: 6.186 | Acc: 14.828% (949/6400)\n",
            "Loss: 6.186 | Acc: 14.877% (967/6500)\n",
            "Loss: 6.180 | Acc: 14.803% (977/6600)\n",
            "Loss: 6.153 | Acc: 14.836% (994/6700)\n",
            "Loss: 6.110 | Acc: 14.750% (1003/6800)\n",
            "Loss: 6.085 | Acc: 14.696% (1014/6900)\n",
            "Loss: 6.074 | Acc: 14.686% (1028/7000)\n",
            "Loss: 6.083 | Acc: 14.676% (1042/7100)\n",
            "Loss: 6.119 | Acc: 14.681% (1057/7200)\n",
            "Loss: 6.128 | Acc: 14.712% (1074/7300)\n",
            "Loss: 6.113 | Acc: 14.730% (1090/7400)\n",
            "Loss: 6.197 | Acc: 14.680% (1101/7500)\n",
            "Loss: 6.161 | Acc: 14.697% (1117/7600)\n",
            "Loss: 6.159 | Acc: 14.649% (1128/7700)\n",
            "Loss: 6.137 | Acc: 14.692% (1146/7800)\n",
            "Loss: 6.172 | Acc: 14.658% (1158/7900)\n",
            "Loss: 6.178 | Acc: 14.537% (1163/8000)\n",
            "Loss: 6.183 | Acc: 14.506% (1175/8100)\n",
            "Loss: 6.195 | Acc: 14.463% (1186/8200)\n",
            "Loss: 6.192 | Acc: 14.386% (1194/8300)\n",
            "Loss: 6.231 | Acc: 14.417% (1211/8400)\n",
            "Loss: 6.267 | Acc: 14.400% (1224/8500)\n",
            "Loss: 6.259 | Acc: 14.372% (1236/8600)\n",
            "Loss: 6.243 | Acc: 14.391% (1252/8700)\n",
            "Loss: 6.234 | Acc: 14.330% (1261/8800)\n",
            "Loss: 6.203 | Acc: 14.337% (1276/8900)\n",
            "Loss: 6.190 | Acc: 14.344% (1291/9000)\n",
            "Loss: 6.162 | Acc: 14.286% (1300/9100)\n",
            "Loss: 6.173 | Acc: 14.326% (1318/9200)\n",
            "Loss: 6.138 | Acc: 14.269% (1327/9300)\n",
            "Loss: 6.112 | Acc: 14.287% (1343/9400)\n",
            "Loss: 6.118 | Acc: 14.221% (1351/9500)\n",
            "Loss: 6.133 | Acc: 14.260% (1369/9600)\n",
            "Loss: 6.131 | Acc: 14.237% (1381/9700)\n",
            "Loss: 6.114 | Acc: 14.255% (1397/9800)\n",
            "Loss: 6.101 | Acc: 14.293% (1415/9900)\n",
            "Loss: 6.091 | Acc: 14.260% (1426/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 52\n",
            "Loss: 1.939 | Acc: 30.000% (3/10)\n",
            "Loss: 2.094 | Acc: 25.000% (5/20)\n",
            "Loss: 2.154 | Acc: 20.000% (6/30)\n",
            "Loss: 2.082 | Acc: 22.500% (9/40)\n",
            "Loss: 2.017 | Acc: 28.000% (14/50)\n",
            "Loss: 2.080 | Acc: 25.000% (15/60)\n",
            "Loss: 2.175 | Acc: 24.286% (17/70)\n",
            "Loss: 2.169 | Acc: 27.500% (22/80)\n",
            "Loss: 2.170 | Acc: 26.667% (24/90)\n",
            "Loss: 2.140 | Acc: 27.000% (27/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 52\n",
            "Loss: 6.825 | Acc: 10.000% (10/100)\n",
            "Loss: 8.511 | Acc: 10.500% (21/200)\n",
            "Loss: 6.838 | Acc: 12.000% (36/300)\n",
            "Loss: 6.713 | Acc: 11.750% (47/400)\n",
            "Loss: 6.027 | Acc: 12.600% (63/500)\n",
            "Loss: 5.910 | Acc: 13.500% (81/600)\n",
            "Loss: 5.519 | Acc: 14.000% (98/700)\n",
            "Loss: 5.505 | Acc: 14.125% (113/800)\n",
            "Loss: 5.428 | Acc: 14.222% (128/900)\n",
            "Loss: 5.345 | Acc: 13.900% (139/1000)\n",
            "Loss: 5.317 | Acc: 13.273% (146/1100)\n",
            "Loss: 5.159 | Acc: 13.167% (158/1200)\n",
            "Loss: 5.246 | Acc: 13.077% (170/1300)\n",
            "Loss: 5.339 | Acc: 13.429% (188/1400)\n",
            "Loss: 5.487 | Acc: 13.533% (203/1500)\n",
            "Loss: 5.296 | Acc: 14.062% (225/1600)\n",
            "Loss: 5.205 | Acc: 14.235% (242/1700)\n",
            "Loss: 5.325 | Acc: 14.056% (253/1800)\n",
            "Loss: 5.529 | Acc: 14.000% (266/1900)\n",
            "Loss: 5.514 | Acc: 14.000% (280/2000)\n",
            "Loss: 5.625 | Acc: 13.714% (288/2100)\n",
            "Loss: 5.564 | Acc: 13.955% (307/2200)\n",
            "Loss: 5.468 | Acc: 14.087% (324/2300)\n",
            "Loss: 5.459 | Acc: 13.833% (332/2400)\n",
            "Loss: 5.448 | Acc: 13.800% (345/2500)\n",
            "Loss: 5.358 | Acc: 13.923% (362/2600)\n",
            "Loss: 5.293 | Acc: 14.000% (378/2700)\n",
            "Loss: 5.289 | Acc: 14.071% (394/2800)\n",
            "Loss: 5.290 | Acc: 14.172% (411/2900)\n",
            "Loss: 5.319 | Acc: 14.300% (429/3000)\n",
            "Loss: 5.259 | Acc: 14.226% (441/3100)\n",
            "Loss: 5.210 | Acc: 14.156% (453/3200)\n",
            "Loss: 5.197 | Acc: 14.273% (471/3300)\n",
            "Loss: 5.280 | Acc: 14.324% (487/3400)\n",
            "Loss: 5.234 | Acc: 14.371% (503/3500)\n",
            "Loss: 5.192 | Acc: 14.472% (521/3600)\n",
            "Loss: 5.135 | Acc: 14.568% (539/3700)\n",
            "Loss: 5.105 | Acc: 14.553% (553/3800)\n",
            "Loss: 5.074 | Acc: 14.513% (566/3900)\n",
            "Loss: 5.129 | Acc: 14.525% (581/4000)\n",
            "Loss: 5.081 | Acc: 14.585% (598/4100)\n",
            "Loss: 5.038 | Acc: 14.524% (610/4200)\n",
            "Loss: 5.044 | Acc: 14.581% (627/4300)\n",
            "Loss: 5.042 | Acc: 14.659% (645/4400)\n",
            "Loss: 5.079 | Acc: 14.622% (658/4500)\n",
            "Loss: 5.048 | Acc: 14.739% (678/4600)\n",
            "Loss: 5.067 | Acc: 14.766% (694/4700)\n",
            "Loss: 5.054 | Acc: 14.875% (714/4800)\n",
            "Loss: 5.045 | Acc: 14.918% (731/4900)\n",
            "Loss: 5.003 | Acc: 14.840% (742/5000)\n",
            "Loss: 4.967 | Acc: 14.882% (759/5100)\n",
            "Loss: 4.943 | Acc: 14.923% (776/5200)\n",
            "Loss: 5.068 | Acc: 14.943% (792/5300)\n",
            "Loss: 5.041 | Acc: 14.907% (805/5400)\n",
            "Loss: 5.012 | Acc: 14.891% (819/5500)\n",
            "Loss: 5.021 | Acc: 14.893% (834/5600)\n",
            "Loss: 4.984 | Acc: 14.930% (851/5700)\n",
            "Loss: 4.987 | Acc: 14.845% (861/5800)\n",
            "Loss: 4.961 | Acc: 14.814% (874/5900)\n",
            "Loss: 4.956 | Acc: 14.967% (898/6000)\n",
            "Loss: 4.948 | Acc: 14.967% (913/6100)\n",
            "Loss: 4.922 | Acc: 15.032% (932/6200)\n",
            "Loss: 4.920 | Acc: 14.937% (941/6300)\n",
            "Loss: 4.938 | Acc: 14.891% (953/6400)\n",
            "Loss: 4.944 | Acc: 14.954% (972/6500)\n",
            "Loss: 4.941 | Acc: 14.909% (984/6600)\n",
            "Loss: 4.926 | Acc: 14.925% (1000/6700)\n",
            "Loss: 4.894 | Acc: 14.838% (1009/6800)\n",
            "Loss: 4.871 | Acc: 14.826% (1023/6900)\n",
            "Loss: 4.866 | Acc: 14.814% (1037/7000)\n",
            "Loss: 4.866 | Acc: 14.817% (1052/7100)\n",
            "Loss: 4.885 | Acc: 14.819% (1067/7200)\n",
            "Loss: 4.892 | Acc: 14.822% (1082/7300)\n",
            "Loss: 4.883 | Acc: 14.851% (1099/7400)\n",
            "Loss: 4.950 | Acc: 14.787% (1109/7500)\n",
            "Loss: 4.925 | Acc: 14.803% (1125/7600)\n",
            "Loss: 4.926 | Acc: 14.740% (1135/7700)\n",
            "Loss: 4.910 | Acc: 14.769% (1152/7800)\n",
            "Loss: 4.931 | Acc: 14.747% (1165/7900)\n",
            "Loss: 4.939 | Acc: 14.625% (1170/8000)\n",
            "Loss: 4.939 | Acc: 14.593% (1182/8100)\n",
            "Loss: 4.950 | Acc: 14.585% (1196/8200)\n",
            "Loss: 4.949 | Acc: 14.494% (1203/8300)\n",
            "Loss: 4.974 | Acc: 14.548% (1222/8400)\n",
            "Loss: 5.002 | Acc: 14.541% (1236/8500)\n",
            "Loss: 4.993 | Acc: 14.523% (1249/8600)\n",
            "Loss: 4.983 | Acc: 14.529% (1264/8700)\n",
            "Loss: 4.978 | Acc: 14.477% (1274/8800)\n",
            "Loss: 4.952 | Acc: 14.483% (1289/8900)\n",
            "Loss: 4.945 | Acc: 14.478% (1303/9000)\n",
            "Loss: 4.930 | Acc: 14.462% (1316/9100)\n",
            "Loss: 4.941 | Acc: 14.489% (1333/9200)\n",
            "Loss: 4.918 | Acc: 14.430% (1342/9300)\n",
            "Loss: 4.898 | Acc: 14.436% (1357/9400)\n",
            "Loss: 4.900 | Acc: 14.379% (1366/9500)\n",
            "Loss: 4.912 | Acc: 14.406% (1383/9600)\n",
            "Loss: 4.910 | Acc: 14.381% (1395/9700)\n",
            "Loss: 4.896 | Acc: 14.418% (1413/9800)\n",
            "Loss: 4.890 | Acc: 14.444% (1430/9900)\n",
            "Loss: 4.884 | Acc: 14.430% (1443/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 53\n",
            "Loss: 1.698 | Acc: 40.000% (4/10)\n",
            "Loss: 1.828 | Acc: 35.000% (7/20)\n",
            "Loss: 1.970 | Acc: 33.333% (10/30)\n",
            "Loss: 1.984 | Acc: 30.000% (12/40)\n",
            "Loss: 1.989 | Acc: 30.000% (15/50)\n",
            "Loss: 2.038 | Acc: 28.333% (17/60)\n",
            "Loss: 2.025 | Acc: 27.143% (19/70)\n",
            "Loss: 2.043 | Acc: 27.500% (22/80)\n",
            "Loss: 2.066 | Acc: 26.667% (24/90)\n",
            "Loss: 2.086 | Acc: 24.000% (24/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 53\n",
            "Loss: 8.950 | Acc: 10.000% (10/100)\n",
            "Loss: 10.801 | Acc: 10.000% (20/200)\n",
            "Loss: 8.539 | Acc: 11.667% (35/300)\n",
            "Loss: 8.290 | Acc: 11.500% (46/400)\n",
            "Loss: 7.440 | Acc: 12.200% (61/500)\n",
            "Loss: 7.416 | Acc: 13.000% (78/600)\n",
            "Loss: 6.989 | Acc: 13.571% (95/700)\n",
            "Loss: 6.924 | Acc: 13.625% (109/800)\n",
            "Loss: 6.765 | Acc: 13.889% (125/900)\n",
            "Loss: 6.633 | Acc: 13.500% (135/1000)\n",
            "Loss: 6.606 | Acc: 12.909% (142/1100)\n",
            "Loss: 6.388 | Acc: 12.917% (155/1200)\n",
            "Loss: 6.520 | Acc: 12.615% (164/1300)\n",
            "Loss: 6.683 | Acc: 13.000% (182/1400)\n",
            "Loss: 6.845 | Acc: 13.200% (198/1500)\n",
            "Loss: 6.584 | Acc: 13.625% (218/1600)\n",
            "Loss: 6.468 | Acc: 13.824% (235/1700)\n",
            "Loss: 6.619 | Acc: 13.722% (247/1800)\n",
            "Loss: 6.896 | Acc: 13.579% (258/1900)\n",
            "Loss: 6.893 | Acc: 13.800% (276/2000)\n",
            "Loss: 6.985 | Acc: 13.571% (285/2100)\n",
            "Loss: 6.907 | Acc: 13.864% (305/2200)\n",
            "Loss: 6.797 | Acc: 14.043% (323/2300)\n",
            "Loss: 6.814 | Acc: 13.792% (331/2400)\n",
            "Loss: 6.791 | Acc: 13.760% (344/2500)\n",
            "Loss: 6.673 | Acc: 13.885% (361/2600)\n",
            "Loss: 6.591 | Acc: 13.926% (376/2700)\n",
            "Loss: 6.572 | Acc: 13.964% (391/2800)\n",
            "Loss: 6.569 | Acc: 14.000% (406/2900)\n",
            "Loss: 6.610 | Acc: 14.033% (421/3000)\n",
            "Loss: 6.551 | Acc: 13.935% (432/3100)\n",
            "Loss: 6.488 | Acc: 13.906% (445/3200)\n",
            "Loss: 6.479 | Acc: 14.030% (463/3300)\n",
            "Loss: 6.560 | Acc: 14.059% (478/3400)\n",
            "Loss: 6.488 | Acc: 14.114% (494/3500)\n",
            "Loss: 6.425 | Acc: 14.250% (513/3600)\n",
            "Loss: 6.351 | Acc: 14.351% (531/3700)\n",
            "Loss: 6.317 | Acc: 14.395% (547/3800)\n",
            "Loss: 6.271 | Acc: 14.359% (560/3900)\n",
            "Loss: 6.342 | Acc: 14.325% (573/4000)\n",
            "Loss: 6.283 | Acc: 14.390% (590/4100)\n",
            "Loss: 6.221 | Acc: 14.357% (603/4200)\n",
            "Loss: 6.230 | Acc: 14.419% (620/4300)\n",
            "Loss: 6.235 | Acc: 14.477% (637/4400)\n",
            "Loss: 6.280 | Acc: 14.444% (650/4500)\n",
            "Loss: 6.235 | Acc: 14.522% (668/4600)\n",
            "Loss: 6.267 | Acc: 14.553% (684/4700)\n",
            "Loss: 6.254 | Acc: 14.646% (703/4800)\n",
            "Loss: 6.240 | Acc: 14.653% (718/4900)\n",
            "Loss: 6.182 | Acc: 14.560% (728/5000)\n",
            "Loss: 6.137 | Acc: 14.588% (744/5100)\n",
            "Loss: 6.113 | Acc: 14.558% (757/5200)\n",
            "Loss: 6.284 | Acc: 14.585% (773/5300)\n",
            "Loss: 6.255 | Acc: 14.537% (785/5400)\n",
            "Loss: 6.215 | Acc: 14.545% (800/5500)\n",
            "Loss: 6.219 | Acc: 14.571% (816/5600)\n",
            "Loss: 6.164 | Acc: 14.632% (834/5700)\n",
            "Loss: 6.167 | Acc: 14.569% (845/5800)\n",
            "Loss: 6.135 | Acc: 14.542% (858/5900)\n",
            "Loss: 6.125 | Acc: 14.650% (879/6000)\n",
            "Loss: 6.111 | Acc: 14.639% (893/6100)\n",
            "Loss: 6.080 | Acc: 14.645% (908/6200)\n",
            "Loss: 6.066 | Acc: 14.556% (917/6300)\n",
            "Loss: 6.089 | Acc: 14.516% (929/6400)\n",
            "Loss: 6.091 | Acc: 14.600% (949/6500)\n",
            "Loss: 6.083 | Acc: 14.530% (959/6600)\n",
            "Loss: 6.058 | Acc: 14.552% (975/6700)\n",
            "Loss: 6.013 | Acc: 14.471% (984/6800)\n",
            "Loss: 5.989 | Acc: 14.435% (996/6900)\n",
            "Loss: 5.981 | Acc: 14.429% (1010/7000)\n",
            "Loss: 5.987 | Acc: 14.423% (1024/7100)\n",
            "Loss: 6.018 | Acc: 14.431% (1039/7200)\n",
            "Loss: 6.027 | Acc: 14.452% (1055/7300)\n",
            "Loss: 6.012 | Acc: 14.486% (1072/7400)\n",
            "Loss: 6.096 | Acc: 14.427% (1082/7500)\n",
            "Loss: 6.062 | Acc: 14.461% (1099/7600)\n",
            "Loss: 6.060 | Acc: 14.416% (1110/7700)\n",
            "Loss: 6.037 | Acc: 14.462% (1128/7800)\n",
            "Loss: 6.074 | Acc: 14.443% (1141/7900)\n",
            "Loss: 6.079 | Acc: 14.325% (1146/8000)\n",
            "Loss: 6.084 | Acc: 14.296% (1158/8100)\n",
            "Loss: 6.096 | Acc: 14.293% (1172/8200)\n",
            "Loss: 6.092 | Acc: 14.217% (1180/8300)\n",
            "Loss: 6.131 | Acc: 14.250% (1197/8400)\n",
            "Loss: 6.166 | Acc: 14.247% (1211/8500)\n",
            "Loss: 6.154 | Acc: 14.233% (1224/8600)\n",
            "Loss: 6.138 | Acc: 14.241% (1239/8700)\n",
            "Loss: 6.131 | Acc: 14.193% (1249/8800)\n",
            "Loss: 6.100 | Acc: 14.202% (1264/8900)\n",
            "Loss: 6.089 | Acc: 14.211% (1279/9000)\n",
            "Loss: 6.062 | Acc: 14.176% (1290/9100)\n",
            "Loss: 6.074 | Acc: 14.207% (1307/9200)\n",
            "Loss: 6.039 | Acc: 14.161% (1317/9300)\n",
            "Loss: 6.011 | Acc: 14.191% (1334/9400)\n",
            "Loss: 6.020 | Acc: 14.126% (1342/9500)\n",
            "Loss: 6.036 | Acc: 14.167% (1360/9600)\n",
            "Loss: 6.033 | Acc: 14.155% (1373/9700)\n",
            "Loss: 6.017 | Acc: 14.184% (1390/9800)\n",
            "Loss: 6.002 | Acc: 14.222% (1408/9900)\n",
            "Loss: 5.992 | Acc: 14.180% (1418/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 54\n",
            "Loss: 2.127 | Acc: 20.000% (2/10)\n",
            "Loss: 2.216 | Acc: 25.000% (5/20)\n",
            "Loss: 2.132 | Acc: 33.333% (10/30)\n",
            "Loss: 2.202 | Acc: 27.500% (11/40)\n",
            "Loss: 2.223 | Acc: 26.000% (13/50)\n",
            "Loss: 2.213 | Acc: 26.667% (16/60)\n",
            "Loss: 2.236 | Acc: 27.143% (19/70)\n",
            "Loss: 2.195 | Acc: 28.750% (23/80)\n",
            "Loss: 2.186 | Acc: 30.000% (27/90)\n",
            "Loss: 2.213 | Acc: 29.000% (29/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 54\n",
            "Loss: 17.170 | Acc: 12.000% (12/100)\n",
            "Loss: 19.962 | Acc: 12.000% (24/200)\n",
            "Loss: 15.677 | Acc: 12.667% (38/300)\n",
            "Loss: 14.816 | Acc: 12.500% (50/400)\n",
            "Loss: 13.088 | Acc: 12.400% (62/500)\n",
            "Loss: 13.355 | Acc: 12.833% (77/600)\n",
            "Loss: 12.557 | Acc: 13.429% (94/700)\n",
            "Loss: 12.395 | Acc: 13.625% (109/800)\n",
            "Loss: 12.139 | Acc: 14.000% (126/900)\n",
            "Loss: 11.885 | Acc: 13.500% (135/1000)\n",
            "Loss: 11.869 | Acc: 12.909% (142/1100)\n",
            "Loss: 11.352 | Acc: 12.917% (155/1200)\n",
            "Loss: 11.667 | Acc: 12.692% (165/1300)\n",
            "Loss: 11.919 | Acc: 13.143% (184/1400)\n",
            "Loss: 12.248 | Acc: 13.133% (197/1500)\n",
            "Loss: 11.696 | Acc: 13.625% (218/1600)\n",
            "Loss: 11.540 | Acc: 13.824% (235/1700)\n",
            "Loss: 11.836 | Acc: 13.722% (247/1800)\n",
            "Loss: 12.283 | Acc: 13.632% (259/1900)\n",
            "Loss: 12.272 | Acc: 13.650% (273/2000)\n",
            "Loss: 12.335 | Acc: 13.381% (281/2100)\n",
            "Loss: 12.217 | Acc: 13.591% (299/2200)\n",
            "Loss: 12.030 | Acc: 13.696% (315/2300)\n",
            "Loss: 12.053 | Acc: 13.542% (325/2400)\n",
            "Loss: 12.003 | Acc: 13.520% (338/2500)\n",
            "Loss: 11.756 | Acc: 13.654% (355/2600)\n",
            "Loss: 11.618 | Acc: 13.778% (372/2700)\n",
            "Loss: 11.630 | Acc: 13.786% (386/2800)\n",
            "Loss: 11.611 | Acc: 13.828% (401/2900)\n",
            "Loss: 11.691 | Acc: 13.933% (418/3000)\n",
            "Loss: 11.586 | Acc: 13.903% (431/3100)\n",
            "Loss: 11.479 | Acc: 13.781% (441/3200)\n",
            "Loss: 11.472 | Acc: 13.848% (457/3300)\n",
            "Loss: 11.571 | Acc: 13.882% (472/3400)\n",
            "Loss: 11.445 | Acc: 14.000% (490/3500)\n",
            "Loss: 11.339 | Acc: 14.111% (508/3600)\n",
            "Loss: 11.226 | Acc: 14.216% (526/3700)\n",
            "Loss: 11.153 | Acc: 14.289% (543/3800)\n",
            "Loss: 11.029 | Acc: 14.205% (554/3900)\n",
            "Loss: 11.138 | Acc: 14.175% (567/4000)\n",
            "Loss: 11.010 | Acc: 14.195% (582/4100)\n",
            "Loss: 10.895 | Acc: 14.167% (595/4200)\n",
            "Loss: 10.900 | Acc: 14.209% (611/4300)\n",
            "Loss: 10.927 | Acc: 14.295% (629/4400)\n",
            "Loss: 10.980 | Acc: 14.244% (641/4500)\n",
            "Loss: 10.907 | Acc: 14.326% (659/4600)\n",
            "Loss: 10.972 | Acc: 14.383% (676/4700)\n",
            "Loss: 10.953 | Acc: 14.458% (694/4800)\n",
            "Loss: 10.897 | Acc: 14.449% (708/4900)\n",
            "Loss: 10.759 | Acc: 14.380% (719/5000)\n",
            "Loss: 10.658 | Acc: 14.412% (735/5100)\n",
            "Loss: 10.627 | Acc: 14.423% (750/5200)\n",
            "Loss: 10.933 | Acc: 14.472% (767/5300)\n",
            "Loss: 10.877 | Acc: 14.426% (779/5400)\n",
            "Loss: 10.801 | Acc: 14.418% (793/5500)\n",
            "Loss: 10.793 | Acc: 14.429% (808/5600)\n",
            "Loss: 10.678 | Acc: 14.456% (824/5700)\n",
            "Loss: 10.684 | Acc: 14.362% (833/5800)\n",
            "Loss: 10.625 | Acc: 14.373% (848/5900)\n",
            "Loss: 10.605 | Acc: 14.483% (869/6000)\n",
            "Loss: 10.548 | Acc: 14.492% (884/6100)\n",
            "Loss: 10.505 | Acc: 14.516% (900/6200)\n",
            "Loss: 10.474 | Acc: 14.444% (910/6300)\n",
            "Loss: 10.503 | Acc: 14.469% (926/6400)\n",
            "Loss: 10.501 | Acc: 14.538% (945/6500)\n",
            "Loss: 10.484 | Acc: 14.455% (954/6600)\n",
            "Loss: 10.431 | Acc: 14.478% (970/6700)\n",
            "Loss: 10.345 | Acc: 14.426% (981/6800)\n",
            "Loss: 10.310 | Acc: 14.362% (991/6900)\n",
            "Loss: 10.323 | Acc: 14.343% (1004/7000)\n",
            "Loss: 10.352 | Acc: 14.324% (1017/7100)\n",
            "Loss: 10.421 | Acc: 14.319% (1031/7200)\n",
            "Loss: 10.441 | Acc: 14.356% (1048/7300)\n",
            "Loss: 10.414 | Acc: 14.392% (1065/7400)\n",
            "Loss: 10.562 | Acc: 14.347% (1076/7500)\n",
            "Loss: 10.494 | Acc: 14.355% (1091/7600)\n",
            "Loss: 10.478 | Acc: 14.299% (1101/7700)\n",
            "Loss: 10.457 | Acc: 14.321% (1117/7800)\n",
            "Loss: 10.541 | Acc: 14.304% (1130/7900)\n",
            "Loss: 10.538 | Acc: 14.200% (1136/8000)\n",
            "Loss: 10.551 | Acc: 14.198% (1150/8100)\n",
            "Loss: 10.578 | Acc: 14.183% (1163/8200)\n",
            "Loss: 10.576 | Acc: 14.096% (1170/8300)\n",
            "Loss: 10.668 | Acc: 14.143% (1188/8400)\n",
            "Loss: 10.735 | Acc: 14.129% (1201/8500)\n",
            "Loss: 10.738 | Acc: 14.105% (1213/8600)\n",
            "Loss: 10.713 | Acc: 14.115% (1228/8700)\n",
            "Loss: 10.688 | Acc: 14.057% (1237/8800)\n",
            "Loss: 10.633 | Acc: 14.079% (1253/8900)\n",
            "Loss: 10.609 | Acc: 14.089% (1268/9000)\n",
            "Loss: 10.567 | Acc: 14.022% (1276/9100)\n",
            "Loss: 10.585 | Acc: 14.043% (1292/9200)\n",
            "Loss: 10.508 | Acc: 13.989% (1301/9300)\n",
            "Loss: 10.463 | Acc: 14.011% (1317/9400)\n",
            "Loss: 10.492 | Acc: 13.958% (1326/9500)\n",
            "Loss: 10.508 | Acc: 13.990% (1343/9600)\n",
            "Loss: 10.509 | Acc: 13.979% (1356/9700)\n",
            "Loss: 10.494 | Acc: 14.010% (1373/9800)\n",
            "Loss: 10.467 | Acc: 14.061% (1392/9900)\n",
            "Loss: 10.438 | Acc: 14.050% (1405/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 55\n",
            "Loss: 2.747 | Acc: 10.000% (1/10)\n",
            "Loss: 2.398 | Acc: 10.000% (2/20)\n",
            "Loss: 2.310 | Acc: 10.000% (3/30)\n",
            "Loss: 2.299 | Acc: 12.500% (5/40)\n",
            "Loss: 2.196 | Acc: 18.000% (9/50)\n",
            "Loss: 2.178 | Acc: 21.667% (13/60)\n",
            "Loss: 2.133 | Acc: 25.714% (18/70)\n",
            "Loss: 2.184 | Acc: 23.750% (19/80)\n",
            "Loss: 2.227 | Acc: 22.222% (20/90)\n",
            "Loss: 2.205 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 55\n",
            "Loss: 8.224 | Acc: 10.000% (10/100)\n",
            "Loss: 10.005 | Acc: 10.000% (20/200)\n",
            "Loss: 7.950 | Acc: 11.667% (35/300)\n",
            "Loss: 7.783 | Acc: 11.250% (45/400)\n",
            "Loss: 6.962 | Acc: 12.000% (60/500)\n",
            "Loss: 6.886 | Acc: 12.667% (76/600)\n",
            "Loss: 6.467 | Acc: 13.286% (93/700)\n",
            "Loss: 6.436 | Acc: 13.500% (108/800)\n",
            "Loss: 6.299 | Acc: 13.667% (123/900)\n",
            "Loss: 6.177 | Acc: 13.300% (133/1000)\n",
            "Loss: 6.152 | Acc: 12.727% (140/1100)\n",
            "Loss: 5.960 | Acc: 12.667% (152/1200)\n",
            "Loss: 6.078 | Acc: 12.462% (162/1300)\n",
            "Loss: 6.227 | Acc: 12.857% (180/1400)\n",
            "Loss: 6.381 | Acc: 13.000% (195/1500)\n",
            "Loss: 6.142 | Acc: 13.562% (217/1600)\n",
            "Loss: 6.039 | Acc: 13.824% (235/1700)\n",
            "Loss: 6.179 | Acc: 13.667% (246/1800)\n",
            "Loss: 6.427 | Acc: 13.526% (257/1900)\n",
            "Loss: 6.420 | Acc: 13.650% (273/2000)\n",
            "Loss: 6.507 | Acc: 13.429% (282/2100)\n",
            "Loss: 6.439 | Acc: 13.636% (300/2200)\n",
            "Loss: 6.333 | Acc: 13.870% (319/2300)\n",
            "Loss: 6.345 | Acc: 13.625% (327/2400)\n",
            "Loss: 6.323 | Acc: 13.600% (340/2500)\n",
            "Loss: 6.210 | Acc: 13.731% (357/2600)\n",
            "Loss: 6.131 | Acc: 13.926% (376/2700)\n",
            "Loss: 6.113 | Acc: 14.000% (392/2800)\n",
            "Loss: 6.112 | Acc: 14.069% (408/2900)\n",
            "Loss: 6.148 | Acc: 14.100% (423/3000)\n",
            "Loss: 6.093 | Acc: 14.000% (434/3100)\n",
            "Loss: 6.036 | Acc: 13.906% (445/3200)\n",
            "Loss: 6.028 | Acc: 14.030% (463/3300)\n",
            "Loss: 6.111 | Acc: 14.059% (478/3400)\n",
            "Loss: 6.048 | Acc: 14.143% (495/3500)\n",
            "Loss: 5.992 | Acc: 14.278% (514/3600)\n",
            "Loss: 5.926 | Acc: 14.405% (533/3700)\n",
            "Loss: 5.890 | Acc: 14.474% (550/3800)\n",
            "Loss: 5.850 | Acc: 14.436% (563/3900)\n",
            "Loss: 5.910 | Acc: 14.400% (576/4000)\n",
            "Loss: 5.852 | Acc: 14.488% (594/4100)\n",
            "Loss: 5.800 | Acc: 14.452% (607/4200)\n",
            "Loss: 5.805 | Acc: 14.488% (623/4300)\n",
            "Loss: 5.812 | Acc: 14.568% (641/4400)\n",
            "Loss: 5.856 | Acc: 14.533% (654/4500)\n",
            "Loss: 5.820 | Acc: 14.652% (674/4600)\n",
            "Loss: 5.849 | Acc: 14.681% (690/4700)\n",
            "Loss: 5.839 | Acc: 14.792% (710/4800)\n",
            "Loss: 5.825 | Acc: 14.796% (725/4900)\n",
            "Loss: 5.770 | Acc: 14.680% (734/5000)\n",
            "Loss: 5.730 | Acc: 14.725% (751/5100)\n",
            "Loss: 5.709 | Acc: 14.692% (764/5200)\n",
            "Loss: 5.860 | Acc: 14.717% (780/5300)\n",
            "Loss: 5.831 | Acc: 14.685% (793/5400)\n",
            "Loss: 5.795 | Acc: 14.709% (809/5500)\n",
            "Loss: 5.796 | Acc: 14.732% (825/5600)\n",
            "Loss: 5.747 | Acc: 14.772% (842/5700)\n",
            "Loss: 5.751 | Acc: 14.707% (853/5800)\n",
            "Loss: 5.721 | Acc: 14.678% (866/5900)\n",
            "Loss: 5.712 | Acc: 14.767% (886/6000)\n",
            "Loss: 5.701 | Acc: 14.770% (901/6100)\n",
            "Loss: 5.672 | Acc: 14.823% (919/6200)\n",
            "Loss: 5.663 | Acc: 14.746% (929/6300)\n",
            "Loss: 5.683 | Acc: 14.688% (940/6400)\n",
            "Loss: 5.687 | Acc: 14.769% (960/6500)\n",
            "Loss: 5.680 | Acc: 14.712% (971/6600)\n",
            "Loss: 5.660 | Acc: 14.746% (988/6700)\n",
            "Loss: 5.619 | Acc: 14.662% (997/6800)\n",
            "Loss: 5.595 | Acc: 14.623% (1009/6900)\n",
            "Loss: 5.592 | Acc: 14.614% (1023/7000)\n",
            "Loss: 5.598 | Acc: 14.620% (1038/7100)\n",
            "Loss: 5.624 | Acc: 14.625% (1053/7200)\n",
            "Loss: 5.632 | Acc: 14.658% (1070/7300)\n",
            "Loss: 5.619 | Acc: 14.689% (1087/7400)\n",
            "Loss: 5.695 | Acc: 14.627% (1097/7500)\n",
            "Loss: 5.665 | Acc: 14.645% (1113/7600)\n",
            "Loss: 5.665 | Acc: 14.597% (1124/7700)\n",
            "Loss: 5.643 | Acc: 14.641% (1142/7800)\n",
            "Loss: 5.673 | Acc: 14.633% (1156/7900)\n",
            "Loss: 5.682 | Acc: 14.512% (1161/8000)\n",
            "Loss: 5.689 | Acc: 14.469% (1172/8100)\n",
            "Loss: 5.700 | Acc: 14.451% (1185/8200)\n",
            "Loss: 5.698 | Acc: 14.373% (1193/8300)\n",
            "Loss: 5.732 | Acc: 14.393% (1209/8400)\n",
            "Loss: 5.767 | Acc: 14.388% (1223/8500)\n",
            "Loss: 5.755 | Acc: 14.384% (1237/8600)\n",
            "Loss: 5.742 | Acc: 14.402% (1253/8700)\n",
            "Loss: 5.734 | Acc: 14.364% (1264/8800)\n",
            "Loss: 5.706 | Acc: 14.371% (1279/8900)\n",
            "Loss: 5.698 | Acc: 14.367% (1293/9000)\n",
            "Loss: 5.674 | Acc: 14.308% (1302/9100)\n",
            "Loss: 5.686 | Acc: 14.337% (1319/9200)\n",
            "Loss: 5.657 | Acc: 14.290% (1329/9300)\n",
            "Loss: 5.632 | Acc: 14.319% (1346/9400)\n",
            "Loss: 5.639 | Acc: 14.263% (1355/9500)\n",
            "Loss: 5.652 | Acc: 14.292% (1372/9600)\n",
            "Loss: 5.649 | Acc: 14.278% (1385/9700)\n",
            "Loss: 5.634 | Acc: 14.306% (1402/9800)\n",
            "Loss: 5.622 | Acc: 14.343% (1420/9900)\n",
            "Loss: 5.614 | Acc: 14.300% (1430/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 56\n",
            "Loss: 1.718 | Acc: 50.000% (5/10)\n",
            "Loss: 1.926 | Acc: 35.000% (7/20)\n",
            "Loss: 2.035 | Acc: 26.667% (8/30)\n",
            "Loss: 2.025 | Acc: 25.000% (10/40)\n",
            "Loss: 2.044 | Acc: 22.000% (11/50)\n",
            "Loss: 2.058 | Acc: 20.000% (12/60)\n",
            "Loss: 2.070 | Acc: 21.429% (15/70)\n",
            "Loss: 2.032 | Acc: 22.500% (18/80)\n",
            "Loss: 2.029 | Acc: 23.333% (21/90)\n",
            "Loss: 2.050 | Acc: 21.000% (21/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 56\n",
            "Loss: 5.882 | Acc: 9.000% (9/100)\n",
            "Loss: 7.314 | Acc: 9.500% (19/200)\n",
            "Loss: 5.912 | Acc: 11.667% (35/300)\n",
            "Loss: 5.891 | Acc: 12.000% (48/400)\n",
            "Loss: 5.350 | Acc: 12.600% (63/500)\n",
            "Loss: 5.283 | Acc: 13.000% (78/600)\n",
            "Loss: 4.979 | Acc: 13.429% (94/700)\n",
            "Loss: 4.941 | Acc: 13.500% (108/800)\n",
            "Loss: 4.853 | Acc: 13.778% (124/900)\n",
            "Loss: 4.781 | Acc: 13.500% (135/1000)\n",
            "Loss: 4.777 | Acc: 13.000% (143/1100)\n",
            "Loss: 4.652 | Acc: 13.000% (156/1200)\n",
            "Loss: 4.720 | Acc: 12.692% (165/1300)\n",
            "Loss: 4.790 | Acc: 13.143% (184/1400)\n",
            "Loss: 4.923 | Acc: 13.400% (201/1500)\n",
            "Loss: 4.771 | Acc: 13.938% (223/1600)\n",
            "Loss: 4.687 | Acc: 13.941% (237/1700)\n",
            "Loss: 4.780 | Acc: 13.778% (248/1800)\n",
            "Loss: 4.970 | Acc: 13.632% (259/1900)\n",
            "Loss: 4.962 | Acc: 13.750% (275/2000)\n",
            "Loss: 5.063 | Acc: 13.429% (282/2100)\n",
            "Loss: 5.026 | Acc: 13.500% (297/2200)\n",
            "Loss: 4.938 | Acc: 13.565% (312/2300)\n",
            "Loss: 4.935 | Acc: 13.333% (320/2400)\n",
            "Loss: 4.920 | Acc: 13.320% (333/2500)\n",
            "Loss: 4.845 | Acc: 13.462% (350/2600)\n",
            "Loss: 4.799 | Acc: 13.556% (366/2700)\n",
            "Loss: 4.804 | Acc: 13.643% (382/2800)\n",
            "Loss: 4.799 | Acc: 13.793% (400/2900)\n",
            "Loss: 4.830 | Acc: 13.900% (417/3000)\n",
            "Loss: 4.778 | Acc: 13.839% (429/3100)\n",
            "Loss: 4.743 | Acc: 13.844% (443/3200)\n",
            "Loss: 4.743 | Acc: 13.939% (460/3300)\n",
            "Loss: 4.807 | Acc: 14.000% (476/3400)\n",
            "Loss: 4.761 | Acc: 14.114% (494/3500)\n",
            "Loss: 4.719 | Acc: 14.194% (511/3600)\n",
            "Loss: 4.674 | Acc: 14.297% (529/3700)\n",
            "Loss: 4.653 | Acc: 14.289% (543/3800)\n",
            "Loss: 4.627 | Acc: 14.282% (557/3900)\n",
            "Loss: 4.680 | Acc: 14.300% (572/4000)\n",
            "Loss: 4.637 | Acc: 14.390% (590/4100)\n",
            "Loss: 4.609 | Acc: 14.333% (602/4200)\n",
            "Loss: 4.621 | Acc: 14.395% (619/4300)\n",
            "Loss: 4.619 | Acc: 14.432% (635/4400)\n",
            "Loss: 4.651 | Acc: 14.422% (649/4500)\n",
            "Loss: 4.625 | Acc: 14.543% (669/4600)\n",
            "Loss: 4.639 | Acc: 14.574% (685/4700)\n",
            "Loss: 4.625 | Acc: 14.708% (706/4800)\n",
            "Loss: 4.618 | Acc: 14.755% (723/4900)\n",
            "Loss: 4.584 | Acc: 14.660% (733/5000)\n",
            "Loss: 4.553 | Acc: 14.745% (752/5100)\n",
            "Loss: 4.529 | Acc: 14.769% (768/5200)\n",
            "Loss: 4.637 | Acc: 14.792% (784/5300)\n",
            "Loss: 4.612 | Acc: 14.741% (796/5400)\n",
            "Loss: 4.589 | Acc: 14.727% (810/5500)\n",
            "Loss: 4.598 | Acc: 14.786% (828/5600)\n",
            "Loss: 4.567 | Acc: 14.807% (844/5700)\n",
            "Loss: 4.565 | Acc: 14.724% (854/5800)\n",
            "Loss: 4.545 | Acc: 14.695% (867/5900)\n",
            "Loss: 4.542 | Acc: 14.817% (889/6000)\n",
            "Loss: 4.543 | Acc: 14.770% (901/6100)\n",
            "Loss: 4.515 | Acc: 14.839% (920/6200)\n",
            "Loss: 4.517 | Acc: 14.762% (930/6300)\n",
            "Loss: 4.530 | Acc: 14.719% (942/6400)\n",
            "Loss: 4.540 | Acc: 14.785% (961/6500)\n",
            "Loss: 4.534 | Acc: 14.758% (974/6600)\n",
            "Loss: 4.523 | Acc: 14.791% (991/6700)\n",
            "Loss: 4.496 | Acc: 14.721% (1001/6800)\n",
            "Loss: 4.478 | Acc: 14.710% (1015/6900)\n",
            "Loss: 4.474 | Acc: 14.700% (1029/7000)\n",
            "Loss: 4.474 | Acc: 14.704% (1044/7100)\n",
            "Loss: 4.488 | Acc: 14.694% (1058/7200)\n",
            "Loss: 4.497 | Acc: 14.712% (1074/7300)\n",
            "Loss: 4.489 | Acc: 14.743% (1091/7400)\n",
            "Loss: 4.546 | Acc: 14.720% (1104/7500)\n",
            "Loss: 4.526 | Acc: 14.737% (1120/7600)\n",
            "Loss: 4.526 | Acc: 14.688% (1131/7700)\n",
            "Loss: 4.519 | Acc: 14.718% (1148/7800)\n",
            "Loss: 4.537 | Acc: 14.658% (1158/7900)\n",
            "Loss: 4.543 | Acc: 14.537% (1163/8000)\n",
            "Loss: 4.540 | Acc: 14.506% (1175/8100)\n",
            "Loss: 4.550 | Acc: 14.488% (1188/8200)\n",
            "Loss: 4.550 | Acc: 14.410% (1196/8300)\n",
            "Loss: 4.568 | Acc: 14.476% (1216/8400)\n",
            "Loss: 4.588 | Acc: 14.471% (1230/8500)\n",
            "Loss: 4.579 | Acc: 14.453% (1243/8600)\n",
            "Loss: 4.569 | Acc: 14.460% (1258/8700)\n",
            "Loss: 4.565 | Acc: 14.420% (1269/8800)\n",
            "Loss: 4.543 | Acc: 14.449% (1286/8900)\n",
            "Loss: 4.539 | Acc: 14.444% (1300/9000)\n",
            "Loss: 4.529 | Acc: 14.407% (1311/9100)\n",
            "Loss: 4.537 | Acc: 14.424% (1327/9200)\n",
            "Loss: 4.518 | Acc: 14.366% (1336/9300)\n",
            "Loss: 4.500 | Acc: 14.372% (1351/9400)\n",
            "Loss: 4.501 | Acc: 14.305% (1359/9500)\n",
            "Loss: 4.513 | Acc: 14.323% (1375/9600)\n",
            "Loss: 4.509 | Acc: 14.278% (1385/9700)\n",
            "Loss: 4.499 | Acc: 14.316% (1403/9800)\n",
            "Loss: 4.495 | Acc: 14.364% (1422/9900)\n",
            "Loss: 4.492 | Acc: 14.330% (1433/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 57\n",
            "Loss: 2.161 | Acc: 20.000% (2/10)\n",
            "Loss: 1.976 | Acc: 30.000% (6/20)\n",
            "Loss: 1.947 | Acc: 30.000% (9/30)\n",
            "Loss: 2.035 | Acc: 22.500% (9/40)\n",
            "Loss: 2.046 | Acc: 24.000% (12/50)\n",
            "Loss: 2.012 | Acc: 28.333% (17/60)\n",
            "Loss: 2.057 | Acc: 25.714% (18/70)\n",
            "Loss: 2.107 | Acc: 23.750% (19/80)\n",
            "Loss: 2.114 | Acc: 22.222% (20/90)\n",
            "Loss: 2.115 | Acc: 22.000% (22/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 57\n",
            "Loss: 7.913 | Acc: 10.000% (10/100)\n",
            "Loss: 9.754 | Acc: 9.500% (19/200)\n",
            "Loss: 7.742 | Acc: 12.000% (36/300)\n",
            "Loss: 7.578 | Acc: 12.000% (48/400)\n",
            "Loss: 6.848 | Acc: 12.600% (63/500)\n",
            "Loss: 6.828 | Acc: 13.000% (78/600)\n",
            "Loss: 6.434 | Acc: 13.714% (96/700)\n",
            "Loss: 6.356 | Acc: 13.625% (109/800)\n",
            "Loss: 6.218 | Acc: 13.889% (125/900)\n",
            "Loss: 6.104 | Acc: 13.500% (135/1000)\n",
            "Loss: 6.093 | Acc: 12.909% (142/1100)\n",
            "Loss: 5.901 | Acc: 12.917% (155/1200)\n",
            "Loss: 6.015 | Acc: 12.615% (164/1300)\n",
            "Loss: 6.164 | Acc: 12.929% (181/1400)\n",
            "Loss: 6.308 | Acc: 13.200% (198/1500)\n",
            "Loss: 6.072 | Acc: 13.750% (220/1600)\n",
            "Loss: 5.963 | Acc: 13.941% (237/1700)\n",
            "Loss: 6.089 | Acc: 13.778% (248/1800)\n",
            "Loss: 6.354 | Acc: 13.684% (260/1900)\n",
            "Loss: 6.354 | Acc: 13.750% (275/2000)\n",
            "Loss: 6.452 | Acc: 13.476% (283/2100)\n",
            "Loss: 6.392 | Acc: 13.727% (302/2200)\n",
            "Loss: 6.289 | Acc: 13.913% (320/2300)\n",
            "Loss: 6.302 | Acc: 13.708% (329/2400)\n",
            "Loss: 6.276 | Acc: 13.680% (342/2500)\n",
            "Loss: 6.171 | Acc: 13.808% (359/2600)\n",
            "Loss: 6.103 | Acc: 13.852% (374/2700)\n",
            "Loss: 6.090 | Acc: 13.964% (391/2800)\n",
            "Loss: 6.084 | Acc: 14.069% (408/2900)\n",
            "Loss: 6.127 | Acc: 14.167% (425/3000)\n",
            "Loss: 6.072 | Acc: 14.065% (436/3100)\n",
            "Loss: 6.019 | Acc: 14.094% (451/3200)\n",
            "Loss: 6.019 | Acc: 14.212% (469/3300)\n",
            "Loss: 6.095 | Acc: 14.265% (485/3400)\n",
            "Loss: 6.028 | Acc: 14.343% (502/3500)\n",
            "Loss: 5.967 | Acc: 14.472% (521/3600)\n",
            "Loss: 5.899 | Acc: 14.541% (538/3700)\n",
            "Loss: 5.872 | Acc: 14.579% (554/3800)\n",
            "Loss: 5.835 | Acc: 14.590% (569/3900)\n",
            "Loss: 5.906 | Acc: 14.575% (583/4000)\n",
            "Loss: 5.851 | Acc: 14.634% (600/4100)\n",
            "Loss: 5.800 | Acc: 14.571% (612/4200)\n",
            "Loss: 5.811 | Acc: 14.605% (628/4300)\n",
            "Loss: 5.816 | Acc: 14.659% (645/4400)\n",
            "Loss: 5.861 | Acc: 14.667% (660/4500)\n",
            "Loss: 5.823 | Acc: 14.739% (678/4600)\n",
            "Loss: 5.851 | Acc: 14.766% (694/4700)\n",
            "Loss: 5.838 | Acc: 14.896% (715/4800)\n",
            "Loss: 5.826 | Acc: 14.898% (730/4900)\n",
            "Loss: 5.777 | Acc: 14.840% (742/5000)\n",
            "Loss: 5.736 | Acc: 14.922% (761/5100)\n",
            "Loss: 5.711 | Acc: 14.962% (778/5200)\n",
            "Loss: 5.868 | Acc: 14.981% (794/5300)\n",
            "Loss: 5.840 | Acc: 14.944% (807/5400)\n",
            "Loss: 5.806 | Acc: 14.909% (820/5500)\n",
            "Loss: 5.812 | Acc: 14.911% (835/5600)\n",
            "Loss: 5.765 | Acc: 14.947% (852/5700)\n",
            "Loss: 5.766 | Acc: 14.879% (863/5800)\n",
            "Loss: 5.737 | Acc: 14.847% (876/5900)\n",
            "Loss: 5.729 | Acc: 14.950% (897/6000)\n",
            "Loss: 5.719 | Acc: 14.902% (909/6100)\n",
            "Loss: 5.687 | Acc: 14.919% (925/6200)\n",
            "Loss: 5.678 | Acc: 14.825% (934/6300)\n",
            "Loss: 5.700 | Acc: 14.766% (945/6400)\n",
            "Loss: 5.707 | Acc: 14.831% (964/6500)\n",
            "Loss: 5.701 | Acc: 14.773% (975/6600)\n",
            "Loss: 5.681 | Acc: 14.806% (992/6700)\n",
            "Loss: 5.641 | Acc: 14.721% (1001/6800)\n",
            "Loss: 5.622 | Acc: 14.681% (1013/6900)\n",
            "Loss: 5.614 | Acc: 14.671% (1027/7000)\n",
            "Loss: 5.621 | Acc: 14.662% (1041/7100)\n",
            "Loss: 5.645 | Acc: 14.653% (1055/7200)\n",
            "Loss: 5.655 | Acc: 14.658% (1070/7300)\n",
            "Loss: 5.640 | Acc: 14.689% (1087/7400)\n",
            "Loss: 5.717 | Acc: 14.667% (1100/7500)\n",
            "Loss: 5.687 | Acc: 14.697% (1117/7600)\n",
            "Loss: 5.685 | Acc: 14.649% (1128/7700)\n",
            "Loss: 5.667 | Acc: 14.679% (1145/7800)\n",
            "Loss: 5.701 | Acc: 14.658% (1158/7900)\n",
            "Loss: 5.706 | Acc: 14.525% (1162/8000)\n",
            "Loss: 5.705 | Acc: 14.494% (1174/8100)\n",
            "Loss: 5.717 | Acc: 14.463% (1186/8200)\n",
            "Loss: 5.712 | Acc: 14.386% (1194/8300)\n",
            "Loss: 5.744 | Acc: 14.417% (1211/8400)\n",
            "Loss: 5.772 | Acc: 14.400% (1224/8500)\n",
            "Loss: 5.760 | Acc: 14.395% (1238/8600)\n",
            "Loss: 5.745 | Acc: 14.402% (1253/8700)\n",
            "Loss: 5.740 | Acc: 14.352% (1263/8800)\n",
            "Loss: 5.711 | Acc: 14.371% (1279/8900)\n",
            "Loss: 5.703 | Acc: 14.378% (1294/9000)\n",
            "Loss: 5.679 | Acc: 14.341% (1305/9100)\n",
            "Loss: 5.690 | Acc: 14.370% (1322/9200)\n",
            "Loss: 5.661 | Acc: 14.301% (1330/9300)\n",
            "Loss: 5.633 | Acc: 14.330% (1347/9400)\n",
            "Loss: 5.640 | Acc: 14.263% (1355/9500)\n",
            "Loss: 5.656 | Acc: 14.292% (1372/9600)\n",
            "Loss: 5.650 | Acc: 14.268% (1384/9700)\n",
            "Loss: 5.637 | Acc: 14.296% (1401/9800)\n",
            "Loss: 5.624 | Acc: 14.323% (1418/9900)\n",
            "Loss: 5.616 | Acc: 14.280% (1428/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 58\n",
            "Loss: 1.846 | Acc: 40.000% (4/10)\n",
            "Loss: 1.999 | Acc: 25.000% (5/20)\n",
            "Loss: 2.067 | Acc: 23.333% (7/30)\n",
            "Loss: 2.079 | Acc: 20.000% (8/40)\n",
            "Loss: 2.274 | Acc: 22.000% (11/50)\n",
            "Loss: 2.274 | Acc: 21.667% (13/60)\n",
            "Loss: 2.255 | Acc: 21.429% (15/70)\n",
            "Loss: 2.212 | Acc: 25.000% (20/80)\n",
            "Loss: 2.209 | Acc: 28.889% (26/90)\n",
            "Loss: 2.172 | Acc: 30.000% (30/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 58\n",
            "Loss: 11.011 | Acc: 13.000% (13/100)\n",
            "Loss: 13.018 | Acc: 11.500% (23/200)\n",
            "Loss: 10.318 | Acc: 13.667% (41/300)\n",
            "Loss: 9.924 | Acc: 13.500% (54/400)\n",
            "Loss: 8.941 | Acc: 13.000% (65/500)\n",
            "Loss: 9.015 | Acc: 13.333% (80/600)\n",
            "Loss: 8.485 | Acc: 13.857% (97/700)\n",
            "Loss: 8.367 | Acc: 13.625% (109/800)\n",
            "Loss: 8.231 | Acc: 13.889% (125/900)\n",
            "Loss: 8.053 | Acc: 13.600% (136/1000)\n",
            "Loss: 8.032 | Acc: 13.182% (145/1100)\n",
            "Loss: 7.719 | Acc: 13.167% (158/1200)\n",
            "Loss: 7.895 | Acc: 12.769% (166/1300)\n",
            "Loss: 8.096 | Acc: 13.143% (184/1400)\n",
            "Loss: 8.299 | Acc: 13.333% (200/1500)\n",
            "Loss: 7.958 | Acc: 13.812% (221/1600)\n",
            "Loss: 7.839 | Acc: 13.765% (234/1700)\n",
            "Loss: 8.023 | Acc: 13.722% (247/1800)\n",
            "Loss: 8.346 | Acc: 13.526% (257/1900)\n",
            "Loss: 8.348 | Acc: 13.750% (275/2000)\n",
            "Loss: 8.416 | Acc: 13.571% (285/2100)\n",
            "Loss: 8.324 | Acc: 13.727% (302/2200)\n",
            "Loss: 8.196 | Acc: 13.826% (318/2300)\n",
            "Loss: 8.225 | Acc: 13.583% (326/2400)\n",
            "Loss: 8.197 | Acc: 13.640% (341/2500)\n",
            "Loss: 8.045 | Acc: 13.808% (359/2600)\n",
            "Loss: 7.949 | Acc: 14.000% (378/2700)\n",
            "Loss: 7.932 | Acc: 14.036% (393/2800)\n",
            "Loss: 7.921 | Acc: 14.138% (410/2900)\n",
            "Loss: 7.984 | Acc: 14.267% (428/3000)\n",
            "Loss: 7.909 | Acc: 14.194% (440/3100)\n",
            "Loss: 7.834 | Acc: 14.156% (453/3200)\n",
            "Loss: 7.834 | Acc: 14.273% (471/3300)\n",
            "Loss: 7.897 | Acc: 14.265% (485/3400)\n",
            "Loss: 7.798 | Acc: 14.400% (504/3500)\n",
            "Loss: 7.720 | Acc: 14.583% (525/3600)\n",
            "Loss: 7.638 | Acc: 14.622% (541/3700)\n",
            "Loss: 7.603 | Acc: 14.684% (558/3800)\n",
            "Loss: 7.538 | Acc: 14.667% (572/3900)\n",
            "Loss: 7.617 | Acc: 14.650% (586/4000)\n",
            "Loss: 7.539 | Acc: 14.707% (603/4100)\n",
            "Loss: 7.463 | Acc: 14.643% (615/4200)\n",
            "Loss: 7.477 | Acc: 14.721% (633/4300)\n",
            "Loss: 7.484 | Acc: 14.773% (650/4400)\n",
            "Loss: 7.541 | Acc: 14.733% (663/4500)\n",
            "Loss: 7.485 | Acc: 14.826% (682/4600)\n",
            "Loss: 7.526 | Acc: 14.872% (699/4700)\n",
            "Loss: 7.514 | Acc: 14.979% (719/4800)\n",
            "Loss: 7.492 | Acc: 14.959% (733/4900)\n",
            "Loss: 7.417 | Acc: 14.880% (744/5000)\n",
            "Loss: 7.357 | Acc: 14.922% (761/5100)\n",
            "Loss: 7.331 | Acc: 14.923% (776/5200)\n",
            "Loss: 7.540 | Acc: 14.943% (792/5300)\n",
            "Loss: 7.502 | Acc: 14.889% (804/5400)\n",
            "Loss: 7.455 | Acc: 14.855% (817/5500)\n",
            "Loss: 7.458 | Acc: 14.875% (833/5600)\n",
            "Loss: 7.383 | Acc: 14.912% (850/5700)\n",
            "Loss: 7.387 | Acc: 14.845% (861/5800)\n",
            "Loss: 7.352 | Acc: 14.847% (876/5900)\n",
            "Loss: 7.337 | Acc: 14.967% (898/6000)\n",
            "Loss: 7.308 | Acc: 14.984% (914/6100)\n",
            "Loss: 7.274 | Acc: 15.000% (930/6200)\n",
            "Loss: 7.256 | Acc: 14.921% (940/6300)\n",
            "Loss: 7.278 | Acc: 14.875% (952/6400)\n",
            "Loss: 7.281 | Acc: 14.938% (971/6500)\n",
            "Loss: 7.275 | Acc: 14.879% (982/6600)\n",
            "Loss: 7.252 | Acc: 14.896% (998/6700)\n",
            "Loss: 7.198 | Acc: 14.809% (1007/6800)\n",
            "Loss: 7.178 | Acc: 14.739% (1017/6900)\n",
            "Loss: 7.172 | Acc: 14.714% (1030/7000)\n",
            "Loss: 7.188 | Acc: 14.704% (1044/7100)\n",
            "Loss: 7.230 | Acc: 14.722% (1060/7200)\n",
            "Loss: 7.244 | Acc: 14.740% (1076/7300)\n",
            "Loss: 7.226 | Acc: 14.770% (1093/7400)\n",
            "Loss: 7.323 | Acc: 14.733% (1105/7500)\n",
            "Loss: 7.280 | Acc: 14.724% (1119/7600)\n",
            "Loss: 7.277 | Acc: 14.688% (1131/7700)\n",
            "Loss: 7.258 | Acc: 14.731% (1149/7800)\n",
            "Loss: 7.308 | Acc: 14.696% (1161/7900)\n",
            "Loss: 7.308 | Acc: 14.588% (1167/8000)\n",
            "Loss: 7.316 | Acc: 14.568% (1180/8100)\n",
            "Loss: 7.336 | Acc: 14.512% (1190/8200)\n",
            "Loss: 7.332 | Acc: 14.422% (1197/8300)\n",
            "Loss: 7.384 | Acc: 14.452% (1214/8400)\n",
            "Loss: 7.425 | Acc: 14.435% (1227/8500)\n",
            "Loss: 7.417 | Acc: 14.407% (1239/8600)\n",
            "Loss: 7.397 | Acc: 14.425% (1255/8700)\n",
            "Loss: 7.386 | Acc: 14.375% (1265/8800)\n",
            "Loss: 7.349 | Acc: 14.371% (1279/8900)\n",
            "Loss: 7.334 | Acc: 14.356% (1292/9000)\n",
            "Loss: 7.302 | Acc: 14.319% (1303/9100)\n",
            "Loss: 7.315 | Acc: 14.348% (1320/9200)\n",
            "Loss: 7.271 | Acc: 14.290% (1329/9300)\n",
            "Loss: 7.238 | Acc: 14.298% (1344/9400)\n",
            "Loss: 7.254 | Acc: 14.232% (1352/9500)\n",
            "Loss: 7.270 | Acc: 14.271% (1370/9600)\n",
            "Loss: 7.270 | Acc: 14.258% (1383/9700)\n",
            "Loss: 7.255 | Acc: 14.286% (1400/9800)\n",
            "Loss: 7.238 | Acc: 14.313% (1417/9900)\n",
            "Loss: 7.221 | Acc: 14.280% (1428/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 59\n",
            "Loss: 1.987 | Acc: 30.000% (3/10)\n",
            "Loss: 2.074 | Acc: 25.000% (5/20)\n",
            "Loss: 2.232 | Acc: 23.333% (7/30)\n",
            "Loss: 2.286 | Acc: 20.000% (8/40)\n",
            "Loss: 2.386 | Acc: 20.000% (10/50)\n",
            "Loss: 2.308 | Acc: 23.333% (14/60)\n",
            "Loss: 2.246 | Acc: 24.286% (17/70)\n",
            "Loss: 2.213 | Acc: 22.500% (18/80)\n",
            "Loss: 2.194 | Acc: 25.556% (23/90)\n",
            "Loss: 2.191 | Acc: 27.000% (27/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 59\n",
            "Loss: 13.635 | Acc: 11.000% (11/100)\n",
            "Loss: 15.967 | Acc: 11.000% (22/200)\n",
            "Loss: 12.597 | Acc: 13.000% (39/300)\n",
            "Loss: 12.004 | Acc: 13.000% (52/400)\n",
            "Loss: 10.757 | Acc: 12.800% (64/500)\n",
            "Loss: 10.908 | Acc: 13.500% (81/600)\n",
            "Loss: 10.239 | Acc: 14.000% (98/700)\n",
            "Loss: 10.105 | Acc: 13.750% (110/800)\n",
            "Loss: 9.952 | Acc: 14.000% (126/900)\n",
            "Loss: 9.725 | Acc: 13.800% (138/1000)\n",
            "Loss: 9.701 | Acc: 13.273% (146/1100)\n",
            "Loss: 9.300 | Acc: 13.333% (160/1200)\n",
            "Loss: 9.566 | Acc: 12.923% (168/1300)\n",
            "Loss: 9.817 | Acc: 13.214% (185/1400)\n",
            "Loss: 10.078 | Acc: 13.467% (202/1500)\n",
            "Loss: 9.642 | Acc: 13.750% (220/1600)\n",
            "Loss: 9.516 | Acc: 13.765% (234/1700)\n",
            "Loss: 9.749 | Acc: 13.722% (247/1800)\n",
            "Loss: 10.152 | Acc: 13.632% (259/1900)\n",
            "Loss: 10.153 | Acc: 13.750% (275/2000)\n",
            "Loss: 10.210 | Acc: 13.524% (284/2100)\n",
            "Loss: 10.100 | Acc: 13.682% (301/2200)\n",
            "Loss: 9.937 | Acc: 13.870% (319/2300)\n",
            "Loss: 9.975 | Acc: 13.667% (328/2400)\n",
            "Loss: 9.955 | Acc: 13.680% (342/2500)\n",
            "Loss: 9.768 | Acc: 13.923% (362/2600)\n",
            "Loss: 9.651 | Acc: 14.074% (380/2700)\n",
            "Loss: 9.636 | Acc: 14.143% (396/2800)\n",
            "Loss: 9.626 | Acc: 14.207% (412/2900)\n",
            "Loss: 9.686 | Acc: 14.367% (431/3000)\n",
            "Loss: 9.587 | Acc: 14.290% (443/3100)\n",
            "Loss: 9.499 | Acc: 14.281% (457/3200)\n",
            "Loss: 9.495 | Acc: 14.424% (476/3300)\n",
            "Loss: 9.563 | Acc: 14.441% (491/3400)\n",
            "Loss: 9.448 | Acc: 14.629% (512/3500)\n",
            "Loss: 9.355 | Acc: 14.778% (532/3600)\n",
            "Loss: 9.272 | Acc: 14.784% (547/3700)\n",
            "Loss: 9.230 | Acc: 14.842% (564/3800)\n",
            "Loss: 9.138 | Acc: 14.821% (578/3900)\n",
            "Loss: 9.232 | Acc: 14.850% (594/4000)\n",
            "Loss: 9.130 | Acc: 14.854% (609/4100)\n",
            "Loss: 9.038 | Acc: 14.786% (621/4200)\n",
            "Loss: 9.050 | Acc: 14.837% (638/4300)\n",
            "Loss: 9.060 | Acc: 14.864% (654/4400)\n",
            "Loss: 9.131 | Acc: 14.844% (668/4500)\n",
            "Loss: 9.067 | Acc: 14.913% (686/4600)\n",
            "Loss: 9.114 | Acc: 15.000% (705/4700)\n",
            "Loss: 9.097 | Acc: 15.083% (724/4800)\n",
            "Loss: 9.064 | Acc: 15.061% (738/4900)\n",
            "Loss: 8.967 | Acc: 15.000% (750/5000)\n",
            "Loss: 8.886 | Acc: 15.020% (766/5100)\n",
            "Loss: 8.858 | Acc: 15.038% (782/5200)\n",
            "Loss: 9.118 | Acc: 15.038% (797/5300)\n",
            "Loss: 9.073 | Acc: 15.000% (810/5400)\n",
            "Loss: 9.013 | Acc: 14.945% (822/5500)\n",
            "Loss: 9.008 | Acc: 14.946% (837/5600)\n",
            "Loss: 8.914 | Acc: 14.982% (854/5700)\n",
            "Loss: 8.918 | Acc: 14.897% (864/5800)\n",
            "Loss: 8.871 | Acc: 14.881% (878/5900)\n",
            "Loss: 8.851 | Acc: 14.967% (898/6000)\n",
            "Loss: 8.814 | Acc: 14.984% (914/6100)\n",
            "Loss: 8.777 | Acc: 15.016% (931/6200)\n",
            "Loss: 8.754 | Acc: 14.937% (941/6300)\n",
            "Loss: 8.776 | Acc: 14.922% (955/6400)\n",
            "Loss: 8.778 | Acc: 14.969% (973/6500)\n",
            "Loss: 8.773 | Acc: 14.894% (983/6600)\n",
            "Loss: 8.741 | Acc: 14.896% (998/6700)\n",
            "Loss: 8.674 | Acc: 14.809% (1007/6800)\n",
            "Loss: 8.654 | Acc: 14.754% (1018/6900)\n",
            "Loss: 8.647 | Acc: 14.714% (1030/7000)\n",
            "Loss: 8.674 | Acc: 14.704% (1044/7100)\n",
            "Loss: 8.726 | Acc: 14.722% (1060/7200)\n",
            "Loss: 8.749 | Acc: 14.740% (1076/7300)\n",
            "Loss: 8.723 | Acc: 14.770% (1093/7400)\n",
            "Loss: 8.840 | Acc: 14.720% (1104/7500)\n",
            "Loss: 8.786 | Acc: 14.684% (1116/7600)\n",
            "Loss: 8.777 | Acc: 14.649% (1128/7700)\n",
            "Loss: 8.760 | Acc: 14.654% (1143/7800)\n",
            "Loss: 8.824 | Acc: 14.633% (1156/7900)\n",
            "Loss: 8.820 | Acc: 14.525% (1162/8000)\n",
            "Loss: 8.827 | Acc: 14.531% (1177/8100)\n",
            "Loss: 8.854 | Acc: 14.488% (1188/8200)\n",
            "Loss: 8.853 | Acc: 14.386% (1194/8300)\n",
            "Loss: 8.921 | Acc: 14.417% (1211/8400)\n",
            "Loss: 8.974 | Acc: 14.400% (1224/8500)\n",
            "Loss: 8.972 | Acc: 14.372% (1236/8600)\n",
            "Loss: 8.947 | Acc: 14.379% (1251/8700)\n",
            "Loss: 8.931 | Acc: 14.318% (1260/8800)\n",
            "Loss: 8.886 | Acc: 14.348% (1277/8900)\n",
            "Loss: 8.863 | Acc: 14.344% (1291/9000)\n",
            "Loss: 8.823 | Acc: 14.308% (1302/9100)\n",
            "Loss: 8.838 | Acc: 14.326% (1318/9200)\n",
            "Loss: 8.781 | Acc: 14.280% (1328/9300)\n",
            "Loss: 8.744 | Acc: 14.277% (1342/9400)\n",
            "Loss: 8.762 | Acc: 14.232% (1352/9500)\n",
            "Loss: 8.781 | Acc: 14.260% (1369/9600)\n",
            "Loss: 8.781 | Acc: 14.216% (1379/9700)\n",
            "Loss: 8.765 | Acc: 14.235% (1395/9800)\n",
            "Loss: 8.748 | Acc: 14.273% (1413/9900)\n",
            "Loss: 8.724 | Acc: 14.250% (1425/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 60\n",
            "Loss: 2.128 | Acc: 20.000% (2/10)\n",
            "Loss: 2.105 | Acc: 20.000% (4/20)\n",
            "Loss: 2.063 | Acc: 26.667% (8/30)\n",
            "Loss: 2.007 | Acc: 32.500% (13/40)\n",
            "Loss: 2.022 | Acc: 30.000% (15/50)\n",
            "Loss: 1.998 | Acc: 26.667% (16/60)\n",
            "Loss: 1.987 | Acc: 27.143% (19/70)\n",
            "Loss: 2.023 | Acc: 27.500% (22/80)\n",
            "Loss: 2.045 | Acc: 25.556% (23/90)\n",
            "Loss: 2.057 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 60\n",
            "Loss: 8.370 | Acc: 13.000% (13/100)\n",
            "Loss: 10.268 | Acc: 10.500% (21/200)\n",
            "Loss: 8.207 | Acc: 12.333% (37/300)\n",
            "Loss: 7.996 | Acc: 12.500% (50/400)\n",
            "Loss: 7.226 | Acc: 12.600% (63/500)\n",
            "Loss: 7.188 | Acc: 13.000% (78/600)\n",
            "Loss: 6.740 | Acc: 13.571% (95/700)\n",
            "Loss: 6.682 | Acc: 13.500% (108/800)\n",
            "Loss: 6.592 | Acc: 13.778% (124/900)\n",
            "Loss: 6.458 | Acc: 13.400% (134/1000)\n",
            "Loss: 6.424 | Acc: 13.182% (145/1100)\n",
            "Loss: 6.210 | Acc: 13.167% (158/1200)\n",
            "Loss: 6.357 | Acc: 12.769% (166/1300)\n",
            "Loss: 6.523 | Acc: 13.214% (185/1400)\n",
            "Loss: 6.695 | Acc: 13.467% (202/1500)\n",
            "Loss: 6.445 | Acc: 13.875% (222/1600)\n",
            "Loss: 6.357 | Acc: 14.118% (240/1700)\n",
            "Loss: 6.508 | Acc: 14.000% (252/1800)\n",
            "Loss: 6.778 | Acc: 13.842% (263/1900)\n",
            "Loss: 6.772 | Acc: 14.050% (281/2000)\n",
            "Loss: 4.659 | Acc: 14.804% (681/4600)\n",
            "Loss: 4.678 | Acc: 14.809% (696/4700)\n",
            "Loss: 4.668 | Acc: 14.896% (715/4800)\n",
            "Loss: 4.662 | Acc: 14.939% (732/4900)\n",
            "Loss: 4.621 | Acc: 14.880% (744/5000)\n",
            "Loss: 4.591 | Acc: 14.922% (761/5100)\n",
            "Loss: 4.571 | Acc: 14.962% (778/5200)\n",
            "Loss: 4.680 | Acc: 14.981% (794/5300)\n",
            "Loss: 4.655 | Acc: 14.889% (804/5400)\n",
            "Loss: 4.629 | Acc: 14.873% (818/5500)\n",
            "Loss: 4.634 | Acc: 14.875% (833/5600)\n",
            "Loss: 4.600 | Acc: 14.877% (848/5700)\n",
            "Loss: 4.596 | Acc: 14.776% (857/5800)\n",
            "Loss: 4.575 | Acc: 14.746% (870/5900)\n",
            "Loss: 4.573 | Acc: 14.900% (894/6000)\n",
            "Loss: 4.564 | Acc: 14.869% (907/6100)\n",
            "Loss: 4.535 | Acc: 14.935% (926/6200)\n",
            "Loss: 4.535 | Acc: 14.841% (935/6300)\n",
            "Loss: 4.548 | Acc: 14.781% (946/6400)\n",
            "Loss: 4.557 | Acc: 14.831% (964/6500)\n",
            "Loss: 4.550 | Acc: 14.773% (975/6600)\n",
            "Loss: 4.537 | Acc: 14.821% (993/6700)\n",
            "Loss: 4.509 | Acc: 14.750% (1003/6800)\n",
            "Loss: 4.492 | Acc: 14.710% (1015/6900)\n",
            "Loss: 4.491 | Acc: 14.686% (1028/7000)\n",
            "Loss: 4.493 | Acc: 14.676% (1042/7100)\n",
            "Loss: 4.504 | Acc: 14.694% (1058/7200)\n",
            "Loss: 4.517 | Acc: 14.712% (1074/7300)\n",
            "Loss: 4.506 | Acc: 14.757% (1092/7400)\n",
            "Loss: 4.569 | Acc: 14.747% (1106/7500)\n",
            "Loss: 4.548 | Acc: 14.763% (1122/7600)\n",
            "Loss: 4.550 | Acc: 14.701% (1132/7700)\n",
            "Loss: 4.538 | Acc: 14.731% (1149/7800)\n",
            "Loss: 4.559 | Acc: 14.684% (1160/7900)\n",
            "Loss: 4.568 | Acc: 14.550% (1164/8000)\n",
            "Loss: 4.568 | Acc: 14.531% (1177/8100)\n",
            "Loss: 4.580 | Acc: 14.512% (1190/8200)\n",
            "Loss: 4.581 | Acc: 14.422% (1197/8300)\n",
            "Loss: 4.602 | Acc: 14.452% (1214/8400)\n",
            "Loss: 4.627 | Acc: 14.435% (1227/8500)\n",
            "Loss: 4.613 | Acc: 14.419% (1240/8600)\n",
            "Loss: 4.602 | Acc: 14.425% (1255/8700)\n",
            "Loss: 4.596 | Acc: 14.375% (1265/8800)\n",
            "Loss: 4.574 | Acc: 14.371% (1279/8900)\n",
            "Loss: 4.569 | Acc: 14.378% (1294/9000)\n",
            "Loss: 4.556 | Acc: 14.363% (1307/9100)\n",
            "Loss: 4.568 | Acc: 14.380% (1323/9200)\n",
            "Loss: 4.548 | Acc: 14.333% (1333/9300)\n",
            "Loss: 4.530 | Acc: 14.351% (1349/9400)\n",
            "Loss: 4.535 | Acc: 14.284% (1357/9500)\n",
            "Loss: 4.549 | Acc: 14.292% (1372/9600)\n",
            "Loss: 4.546 | Acc: 14.237% (1381/9700)\n",
            "Loss: 4.536 | Acc: 14.245% (1396/9800)\n",
            "Loss: 4.531 | Acc: 14.273% (1413/9900)\n",
            "Loss: 4.527 | Acc: 14.250% (1425/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 67\n",
            "Loss: 2.097 | Acc: 10.000% (1/10)\n",
            "Loss: 1.981 | Acc: 20.000% (4/20)\n",
            "Loss: 2.021 | Acc: 20.000% (6/30)\n",
            "Loss: 2.049 | Acc: 20.000% (8/40)\n",
            "Loss: 1.996 | Acc: 26.000% (13/50)\n",
            "Loss: 2.023 | Acc: 23.333% (14/60)\n",
            "Loss: 2.064 | Acc: 20.000% (14/70)\n",
            "Loss: 2.079 | Acc: 20.000% (16/80)\n",
            "Loss: 2.086 | Acc: 18.889% (17/90)\n",
            "Loss: 2.051 | Acc: 19.000% (19/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 67\n",
            "Loss: 8.766 | Acc: 11.000% (11/100)\n",
            "Loss: 10.645 | Acc: 10.000% (20/200)\n",
            "Loss: 8.403 | Acc: 12.000% (36/300)\n",
            "Loss: 8.200 | Acc: 12.000% (48/400)\n",
            "Loss: 7.343 | Acc: 12.400% (62/500)\n",
            "Loss: 7.326 | Acc: 12.833% (77/600)\n",
            "Loss: 6.889 | Acc: 13.429% (94/700)\n",
            "Loss: 6.836 | Acc: 13.500% (108/800)\n",
            "Loss: 6.683 | Acc: 13.778% (124/900)\n",
            "Loss: 6.568 | Acc: 13.400% (134/1000)\n",
            "Loss: 6.552 | Acc: 12.818% (141/1100)\n",
            "Loss: 6.335 | Acc: 12.750% (153/1200)\n",
            "Loss: 6.477 | Acc: 12.462% (162/1300)\n",
            "Loss: 6.646 | Acc: 12.857% (180/1400)\n",
            "Loss: 6.799 | Acc: 13.067% (196/1500)\n",
            "Loss: 6.538 | Acc: 13.562% (217/1600)\n",
            "Loss: 6.429 | Acc: 13.706% (233/1700)\n",
            "Loss: 6.588 | Acc: 13.556% (244/1800)\n",
            "Loss: 6.866 | Acc: 13.421% (255/1900)\n",
            "Loss: 6.865 | Acc: 13.550% (271/2000)\n",
            "Loss: 6.964 | Acc: 13.333% (280/2100)\n",
            "Loss: 6.893 | Acc: 13.545% (298/2200)\n",
            "Loss: 6.778 | Acc: 13.739% (316/2300)\n",
            "Loss: 6.791 | Acc: 13.542% (325/2400)\n",
            "Loss: 6.767 | Acc: 13.520% (338/2500)\n",
            "Loss: 6.648 | Acc: 13.692% (356/2600)\n",
            "Loss: 6.559 | Acc: 13.778% (372/2700)\n",
            "Loss: 6.537 | Acc: 13.821% (387/2800)\n",
            "Loss: 6.538 | Acc: 13.897% (403/2900)\n",
            "Loss: 6.577 | Acc: 13.967% (419/3000)\n",
            "Loss: 6.520 | Acc: 13.871% (430/3100)\n",
            "Loss: 6.458 | Acc: 13.875% (444/3200)\n",
            "Loss: 6.448 | Acc: 13.970% (461/3300)\n",
            "Loss: 6.520 | Acc: 14.059% (478/3400)\n",
            "Loss: 6.445 | Acc: 14.200% (497/3500)\n",
            "Loss: 6.383 | Acc: 14.333% (516/3600)\n",
            "Loss: 6.308 | Acc: 14.432% (534/3700)\n",
            "Loss: 6.276 | Acc: 14.447% (549/3800)\n",
            "Loss: 6.229 | Acc: 14.462% (564/3900)\n",
            "Loss: 6.304 | Acc: 14.450% (578/4000)\n",
            "Loss: 6.242 | Acc: 14.561% (597/4100)\n",
            "Loss: 6.184 | Acc: 14.500% (609/4200)\n",
            "Loss: 6.187 | Acc: 14.535% (625/4300)\n",
            "Loss: 6.199 | Acc: 14.614% (643/4400)\n",
            "Loss: 6.247 | Acc: 14.622% (658/4500)\n",
            "Loss: 6.204 | Acc: 14.717% (677/4600)\n",
            "Loss: 6.235 | Acc: 14.745% (693/4700)\n",
            "Loss: 6.220 | Acc: 14.854% (713/4800)\n",
            "Loss: 6.205 | Acc: 14.857% (728/4900)\n",
            "Loss: 6.143 | Acc: 14.760% (738/5000)\n",
            "Loss: 6.099 | Acc: 14.784% (754/5100)\n",
            "Loss: 6.078 | Acc: 14.769% (768/5200)\n",
            "Loss: 6.247 | Acc: 14.792% (784/5300)\n",
            "Loss: 6.216 | Acc: 14.741% (796/5400)\n",
            "Loss: 6.176 | Acc: 14.764% (812/5500)\n",
            "Loss: 6.170 | Acc: 14.804% (829/5600)\n",
            "Loss: 6.115 | Acc: 14.825% (845/5700)\n",
            "Loss: 6.117 | Acc: 14.759% (856/5800)\n",
            "Loss: 6.085 | Acc: 14.729% (869/5900)\n",
            "Loss: 6.075 | Acc: 14.833% (890/6000)\n",
            "Loss: 6.059 | Acc: 14.803% (903/6100)\n",
            "Loss: 6.027 | Acc: 14.855% (921/6200)\n",
            "Loss: 6.014 | Acc: 14.778% (931/6300)\n",
            "Loss: 6.034 | Acc: 14.719% (942/6400)\n",
            "Loss: 6.039 | Acc: 14.800% (962/6500)\n",
            "Loss: 6.027 | Acc: 14.758% (974/6600)\n",
            "Loss: 6.004 | Acc: 14.776% (990/6700)\n",
            "Loss: 5.960 | Acc: 14.691% (999/6800)\n",
            "Loss: 5.937 | Acc: 14.652% (1011/6900)\n",
            "Loss: 5.931 | Acc: 14.657% (1026/7000)\n",
            "Loss: 5.940 | Acc: 14.662% (1041/7100)\n",
            "Loss: 5.966 | Acc: 14.667% (1056/7200)\n",
            "Loss: 5.977 | Acc: 14.671% (1071/7300)\n",
            "Loss: 5.961 | Acc: 14.703% (1088/7400)\n",
            "Loss: 6.049 | Acc: 14.653% (1099/7500)\n",
            "Loss: 6.017 | Acc: 14.684% (1116/7600)\n",
            "Loss: 6.016 | Acc: 14.636% (1127/7700)\n",
            "Loss: 5.991 | Acc: 14.705% (1147/7800)\n",
            "Loss: 6.031 | Acc: 14.684% (1160/7900)\n",
            "Loss: 6.039 | Acc: 14.562% (1165/8000)\n",
            "Loss: 6.047 | Acc: 14.519% (1176/8100)\n",
            "Loss: 6.062 | Acc: 14.500% (1189/8200)\n",
            "Loss: 6.059 | Acc: 14.422% (1197/8300)\n",
            "Loss: 6.100 | Acc: 14.452% (1214/8400)\n",
            "Loss: 6.138 | Acc: 14.435% (1227/8500)\n",
            "Loss: 6.125 | Acc: 14.419% (1240/8600)\n",
            "Loss: 6.108 | Acc: 14.425% (1255/8700)\n",
            "Loss: 6.099 | Acc: 14.386% (1266/8800)\n",
            "Loss: 6.069 | Acc: 14.393% (1281/8900)\n",
            "Loss: 6.060 | Acc: 14.400% (1296/9000)\n",
            "Loss: 6.033 | Acc: 14.352% (1306/9100)\n",
            "Loss: 6.047 | Acc: 14.380% (1323/9200)\n",
            "Loss: 6.013 | Acc: 14.323% (1332/9300)\n",
            "Loss: 5.985 | Acc: 14.340% (1348/9400)\n",
            "Loss: 5.995 | Acc: 14.274% (1356/9500)\n",
            "Loss: 6.011 | Acc: 14.312% (1374/9600)\n",
            "Loss: 6.008 | Acc: 14.278% (1385/9700)\n",
            "Loss: 5.992 | Acc: 14.306% (1402/9800)\n",
            "Loss: 5.978 | Acc: 14.333% (1419/9900)\n",
            "Loss: 5.967 | Acc: 14.290% (1429/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 68\n",
            "Loss: 2.715 | Acc: 10.000% (1/10)\n",
            "Loss: 2.442 | Acc: 25.000% (5/20)\n",
            "Loss: 2.248 | Acc: 26.667% (8/30)\n",
            "Loss: 2.201 | Acc: 25.000% (10/40)\n",
            "Loss: 2.148 | Acc: 28.000% (14/50)\n",
            "Loss: 2.089 | Acc: 30.000% (18/60)\n",
            "Loss: 2.089 | Acc: 32.857% (23/70)\n",
            "Loss: 2.132 | Acc: 30.000% (24/80)\n",
            "Loss: 2.152 | Acc: 26.667% (24/90)\n",
            "Loss: 2.160 | Acc: 26.000% (26/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 68\n",
            "Loss: 7.655 | Acc: 11.000% (11/100)\n",
            "Loss: 9.498 | Acc: 10.500% (21/200)\n",
            "Loss: 7.540 | Acc: 12.000% (36/300)\n",
            "Loss: 7.407 | Acc: 12.000% (48/400)\n",
            "Loss: 6.639 | Acc: 12.600% (63/500)\n",
            "Loss: 6.592 | Acc: 13.000% (78/600)\n",
            "Loss: 6.179 | Acc: 13.571% (95/700)\n",
            "Loss: 6.136 | Acc: 13.500% (108/800)\n",
            "Loss: 5.998 | Acc: 13.556% (122/900)\n",
            "Loss: 5.889 | Acc: 13.200% (132/1000)\n",
            "Loss: 5.887 | Acc: 12.636% (139/1100)\n",
            "Loss: 5.701 | Acc: 12.583% (151/1200)\n",
            "Loss: 5.805 | Acc: 12.462% (162/1300)\n",
            "Loss: 5.942 | Acc: 12.857% (180/1400)\n",
            "Loss: 6.070 | Acc: 13.000% (195/1500)\n",
            "Loss: 5.851 | Acc: 13.438% (215/1600)\n",
            "Loss: 5.750 | Acc: 13.647% (232/1700)\n",
            "Loss: 5.876 | Acc: 13.500% (243/1800)\n",
            "Loss: 6.127 | Acc: 13.368% (254/1900)\n",
            "Loss: 6.125 | Acc: 13.500% (270/2000)\n",
            "Loss: 6.228 | Acc: 13.286% (279/2100)\n",
            "Loss: 6.168 | Acc: 13.500% (297/2200)\n",
            "Loss: 6.065 | Acc: 13.696% (315/2300)\n",
            "Loss: 6.071 | Acc: 13.458% (323/2400)\n",
            "Loss: 6.050 | Acc: 13.440% (336/2500)\n",
            "Loss: 5.941 | Acc: 13.577% (353/2600)\n",
            "Loss: 5.864 | Acc: 13.704% (370/2700)\n",
            "Loss: 5.850 | Acc: 13.750% (385/2800)\n",
            "Loss: 5.854 | Acc: 13.828% (401/2900)\n",
            "Loss: 5.886 | Acc: 13.933% (418/3000)\n",
            "Loss: 5.837 | Acc: 13.839% (429/3100)\n",
            "Loss: 5.785 | Acc: 13.750% (440/3200)\n",
            "Loss: 5.778 | Acc: 13.848% (457/3300)\n",
            "Loss: 5.849 | Acc: 13.912% (473/3400)\n",
            "Loss: 5.781 | Acc: 14.000% (490/3500)\n",
            "Loss: 5.725 | Acc: 14.111% (508/3600)\n",
            "Loss: 5.661 | Acc: 14.216% (526/3700)\n",
            "Loss: 5.631 | Acc: 14.237% (541/3800)\n",
            "Loss: 5.594 | Acc: 14.231% (555/3900)\n",
            "Loss: 5.664 | Acc: 14.200% (568/4000)\n",
            "Loss: 5.607 | Acc: 14.268% (585/4100)\n",
            "Loss: 5.557 | Acc: 14.214% (597/4200)\n",
            "Loss: 5.561 | Acc: 14.256% (613/4300)\n",
            "Loss: 5.572 | Acc: 14.341% (631/4400)\n",
            "Loss: 5.617 | Acc: 14.333% (645/4500)\n",
            "Loss: 5.582 | Acc: 14.413% (663/4600)\n",
            "Loss: 5.610 | Acc: 14.447% (679/4700)\n",
            "Loss: 5.597 | Acc: 14.583% (700/4800)\n",
            "Loss: 5.585 | Acc: 14.592% (715/4900)\n",
            "Loss: 5.534 | Acc: 14.480% (724/5000)\n",
            "Loss: 5.496 | Acc: 14.549% (742/5100)\n",
            "Loss: 5.475 | Acc: 14.558% (757/5200)\n",
            "Loss: 5.624 | Acc: 14.585% (773/5300)\n",
            "Loss: 5.597 | Acc: 14.537% (785/5400)\n",
            "Loss: 5.563 | Acc: 14.564% (801/5500)\n",
            "Loss: 5.560 | Acc: 14.571% (816/5600)\n",
            "Loss: 5.514 | Acc: 14.614% (833/5700)\n",
            "Loss: 5.514 | Acc: 14.552% (844/5800)\n",
            "Loss: 5.485 | Acc: 14.525% (857/5900)\n",
            "Loss: 5.480 | Acc: 14.617% (877/6000)\n",
            "Loss: 5.468 | Acc: 14.590% (890/6100)\n",
            "Loss: 5.437 | Acc: 14.645% (908/6200)\n",
            "Loss: 5.430 | Acc: 14.571% (918/6300)\n",
            "Loss: 5.449 | Acc: 14.516% (929/6400)\n",
            "Loss: 5.458 | Acc: 14.600% (949/6500)\n",
            "Loss: 5.448 | Acc: 14.576% (962/6600)\n",
            "Loss: 5.431 | Acc: 14.597% (978/6700)\n",
            "Loss: 5.392 | Acc: 14.515% (987/6800)\n",
            "Loss: 5.370 | Acc: 14.478% (999/6900)\n",
            "Loss: 5.369 | Acc: 14.471% (1013/7000)\n",
            "Loss: 5.376 | Acc: 14.479% (1028/7100)\n",
            "Loss: 5.395 | Acc: 14.500% (1044/7200)\n",
            "Loss: 5.406 | Acc: 14.507% (1059/7300)\n",
            "Loss: 5.393 | Acc: 14.541% (1076/7400)\n",
            "Loss: 5.470 | Acc: 14.493% (1087/7500)\n",
            "Loss: 5.443 | Acc: 14.513% (1103/7600)\n",
            "Loss: 5.444 | Acc: 14.455% (1113/7700)\n",
            "Loss: 5.424 | Acc: 14.500% (1131/7800)\n",
            "Loss: 5.457 | Acc: 14.494% (1145/7900)\n",
            "Loss: 5.468 | Acc: 14.375% (1150/8000)\n",
            "Loss: 5.474 | Acc: 14.333% (1161/8100)\n",
            "Loss: 5.487 | Acc: 14.305% (1173/8200)\n",
            "Loss: 5.485 | Acc: 14.229% (1181/8300)\n",
            "Loss: 5.517 | Acc: 14.250% (1197/8400)\n",
            "Loss: 5.551 | Acc: 14.235% (1210/8500)\n",
            "Loss: 5.537 | Acc: 14.209% (1222/8600)\n",
            "Loss: 5.522 | Acc: 14.218% (1237/8700)\n",
            "Loss: 5.515 | Acc: 14.170% (1247/8800)\n",
            "Loss: 5.488 | Acc: 14.180% (1262/8900)\n",
            "Loss: 5.482 | Acc: 14.178% (1276/9000)\n",
            "Loss: 5.460 | Acc: 14.132% (1286/9100)\n",
            "Loss: 5.474 | Acc: 14.152% (1302/9200)\n",
            "Loss: 5.445 | Acc: 14.097% (1311/9300)\n",
            "Loss: 5.420 | Acc: 14.128% (1328/9400)\n",
            "Loss: 5.428 | Acc: 14.063% (1336/9500)\n",
            "Loss: 5.444 | Acc: 14.083% (1352/9600)\n",
            "Loss: 5.441 | Acc: 14.062% (1364/9700)\n",
            "Loss: 5.426 | Acc: 14.092% (1381/9800)\n",
            "Loss: 5.415 | Acc: 14.131% (1399/9900)\n",
            "Loss: 5.408 | Acc: 14.090% (1409/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 69\n",
            "Loss: 2.143 | Acc: 10.000% (1/10)\n",
            "Loss: 2.089 | Acc: 20.000% (4/20)\n",
            "Loss: 2.060 | Acc: 23.333% (7/30)\n",
            "Loss: 2.122 | Acc: 20.000% (8/40)\n",
            "Loss: 2.107 | Acc: 20.000% (10/50)\n",
            "Loss: 2.115 | Acc: 21.667% (13/60)\n",
            "Loss: 2.138 | Acc: 20.000% (14/70)\n",
            "Loss: 2.137 | Acc: 20.000% (16/80)\n",
            "Loss: 2.081 | Acc: 21.111% (19/90)\n",
            "Loss: 2.067 | Acc: 22.000% (22/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 69\n",
            "Loss: 6.108 | Acc: 8.000% (8/100)\n",
            "Loss: 7.761 | Acc: 9.500% (19/200)\n",
            "Loss: 6.218 | Acc: 11.333% (34/300)\n",
            "Loss: 6.188 | Acc: 11.250% (45/400)\n",
            "Loss: 5.604 | Acc: 12.000% (60/500)\n",
            "Loss: 5.542 | Acc: 13.000% (78/600)\n",
            "Loss: 5.193 | Acc: 13.571% (95/700)\n",
            "Loss: 5.152 | Acc: 13.750% (110/800)\n",
            "Loss: 5.064 | Acc: 13.778% (124/900)\n",
            "Loss: 4.991 | Acc: 13.500% (135/1000)\n",
            "Loss: 5.001 | Acc: 13.000% (143/1100)\n",
            "Loss: 4.842 | Acc: 12.917% (155/1200)\n",
            "Loss: 4.915 | Acc: 12.615% (164/1300)\n",
            "Loss: 4.984 | Acc: 13.143% (184/1400)\n",
            "Loss: 5.118 | Acc: 13.267% (199/1500)\n",
            "Loss: 4.955 | Acc: 13.812% (221/1600)\n",
            "Loss: 4.867 | Acc: 14.059% (239/1700)\n",
            "Loss: 4.976 | Acc: 13.889% (250/1800)\n",
            "Loss: 5.186 | Acc: 13.737% (261/1900)\n",
            "Loss: 5.184 | Acc: 13.900% (278/2000)\n",
            "Loss: 5.298 | Acc: 13.571% (285/2100)\n",
            "Loss: 5.263 | Acc: 13.773% (303/2200)\n",
            "Loss: 5.170 | Acc: 13.826% (318/2300)\n",
            "Loss: 5.158 | Acc: 13.500% (324/2400)\n",
            "Loss: 5.143 | Acc: 13.480% (337/2500)\n",
            "Loss: 5.054 | Acc: 13.577% (353/2600)\n",
            "Loss: 4.995 | Acc: 13.630% (368/2700)\n",
            "Loss: 5.000 | Acc: 13.714% (384/2800)\n",
            "Loss: 4.996 | Acc: 13.828% (401/2900)\n",
            "Loss: 5.023 | Acc: 14.000% (420/3000)\n",
            "Loss: 4.972 | Acc: 13.903% (431/3100)\n",
            "Loss: 4.929 | Acc: 13.812% (442/3200)\n",
            "Loss: 4.927 | Acc: 13.939% (460/3300)\n",
            "Loss: 4.993 | Acc: 13.941% (474/3400)\n",
            "Loss: 4.940 | Acc: 14.029% (491/3500)\n",
            "Loss: 4.895 | Acc: 14.111% (508/3600)\n",
            "Loss: 4.842 | Acc: 14.243% (527/3700)\n",
            "Loss: 4.820 | Acc: 14.237% (541/3800)\n",
            "Loss: 4.789 | Acc: 14.179% (553/3900)\n",
            "Loss: 4.852 | Acc: 14.200% (568/4000)\n",
            "Loss: 4.802 | Acc: 14.293% (586/4100)\n",
            "Loss: 4.770 | Acc: 14.238% (598/4200)\n",
            "Loss: 4.780 | Acc: 14.279% (614/4300)\n",
            "Loss: 4.783 | Acc: 14.341% (631/4400)\n",
            "Loss: 4.815 | Acc: 14.333% (645/4500)\n",
            "Loss: 4.788 | Acc: 14.500% (667/4600)\n",
            "Loss: 4.805 | Acc: 14.532% (683/4700)\n",
            "Loss: 4.792 | Acc: 14.646% (703/4800)\n",
            "Loss: 4.787 | Acc: 14.653% (718/4900)\n",
            "Loss: 4.746 | Acc: 14.560% (728/5000)\n",
            "Loss: 4.713 | Acc: 14.627% (746/5100)\n",
            "Loss: 4.692 | Acc: 14.673% (763/5200)\n",
            "Loss: 4.809 | Acc: 14.698% (779/5300)\n",
            "Loss: 4.782 | Acc: 14.648% (791/5400)\n",
            "Loss: 4.757 | Acc: 14.636% (805/5500)\n",
            "Loss: 4.761 | Acc: 14.643% (820/5600)\n",
            "Loss: 4.724 | Acc: 14.649% (835/5700)\n",
            "Loss: 4.720 | Acc: 14.552% (844/5800)\n",
            "Loss: 4.696 | Acc: 14.525% (857/5900)\n",
            "Loss: 4.693 | Acc: 14.683% (881/6000)\n",
            "Loss: 4.686 | Acc: 14.656% (894/6100)\n",
            "Loss: 4.655 | Acc: 14.742% (914/6200)\n",
            "Loss: 4.656 | Acc: 14.651% (923/6300)\n",
            "Loss: 4.670 | Acc: 14.609% (935/6400)\n",
            "Loss: 4.680 | Acc: 14.677% (954/6500)\n",
            "Loss: 4.672 | Acc: 14.606% (964/6600)\n",
            "Loss: 4.660 | Acc: 14.642% (981/6700)\n",
            "Loss: 4.630 | Acc: 14.588% (992/6800)\n",
            "Loss: 4.612 | Acc: 14.551% (1004/6900)\n",
            "Loss: 4.610 | Acc: 14.529% (1017/7000)\n",
            "Loss: 4.613 | Acc: 14.521% (1031/7100)\n",
            "Loss: 4.626 | Acc: 14.542% (1047/7200)\n",
            "Loss: 4.638 | Acc: 14.575% (1064/7300)\n",
            "Loss: 4.627 | Acc: 14.608% (1081/7400)\n",
            "Loss: 4.693 | Acc: 14.560% (1092/7500)\n",
            "Loss: 4.671 | Acc: 14.579% (1108/7600)\n",
            "Loss: 4.673 | Acc: 14.519% (1118/7700)\n",
            "Loss: 4.661 | Acc: 14.538% (1134/7800)\n",
            "Loss: 4.684 | Acc: 14.506% (1146/7900)\n",
            "Loss: 4.692 | Acc: 14.387% (1151/8000)\n",
            "Loss: 4.691 | Acc: 14.358% (1163/8100)\n",
            "Loss: 4.703 | Acc: 14.341% (1176/8200)\n",
            "Loss: 4.704 | Acc: 14.253% (1183/8300)\n",
            "Loss: 4.727 | Acc: 14.310% (1202/8400)\n",
            "Loss: 4.751 | Acc: 14.306% (1216/8500)\n",
            "Loss: 4.739 | Acc: 14.279% (1228/8600)\n",
            "Loss: 4.727 | Acc: 14.287% (1243/8700)\n",
            "Loss: 4.721 | Acc: 14.239% (1253/8800)\n",
            "Loss: 4.698 | Acc: 14.236% (1267/8900)\n",
            "Loss: 4.692 | Acc: 14.233% (1281/9000)\n",
            "Loss: 4.679 | Acc: 14.198% (1292/9100)\n",
            "Loss: 4.691 | Acc: 14.217% (1308/9200)\n",
            "Loss: 4.670 | Acc: 14.161% (1317/9300)\n",
            "Loss: 4.649 | Acc: 14.170% (1332/9400)\n",
            "Loss: 4.653 | Acc: 14.105% (1340/9500)\n",
            "Loss: 4.668 | Acc: 14.146% (1358/9600)\n",
            "Loss: 4.665 | Acc: 14.113% (1369/9700)\n",
            "Loss: 4.654 | Acc: 14.163% (1388/9800)\n",
            "Loss: 4.649 | Acc: 14.202% (1406/9900)\n",
            "Loss: 4.644 | Acc: 14.180% (1418/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsxIiVGOUENw",
        "colab_type": "text"
      },
      "source": [
        "## VGG-like architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ach-Zm4PUENx",
        "colab_type": "text"
      },
      "source": [
        "__Question 4:__ Same question as before, but with a *VGG*. Which model do you recommend?\n",
        "\n",
        "\n",
        "__Answer__: Using the same method as before, and using a cross validation to find the best hyperparameters, we find:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on all the dataset | source\n",
        "|------|------|------|------|------|------|\n",
        "|   VGG16  | 90 | 13 % | 10 %|\t92.64%| https://github.com/kuangliu/pytorch-cifar|\n",
        "|   VGG19  | 100 | 16 % | 10 %|91.52 % |https://www.researchgate.net/publication/324584468_MaxGain_Regularisation_of_Neural_Networks_by_Constraining_Activation_Magnitudes/figures?lo=1|\n",
        "|   VGG11_bn  | 90 | 19 % | 16 %|\t| |\n",
        "\n",
        "we can see that resnet gives a better result than VGG. This result is not surprising in the case of a full dataset, but it is apparently true in the case of small data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4xd2xwcUENx",
        "colab_type": "code",
        "outputId": "4aef8c78-4c10-4557-8eb2-da1bcfd75c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7253
        }
      },
      "source": [
        "# hyperparameters\n",
        "lr = 0.1\n",
        "batch_size = 32\n",
        "\n",
        "# Load train\n",
        "trainset = torchvision.datasets.CIFAR10(root='./root', train=True, download=True, transform=transform_train)\n",
        "# Keep 100 examples\n",
        "p = np.random.permutation(len(trainset.data))\n",
        "trainset.data = trainset.data[p][:100,:,:,:]\n",
        "trainset.targets = np.array(trainset.targets)[p][:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load test\n",
        "testset = torchvision.datasets.CIFAR10(root='./root', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load architecture: vgg11/16/19\n",
        "net = vgg11_bn()\n",
        "net = net.to(device)\n",
        "\n",
        "# Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "start_epoch = 0\n",
        "epochs = 50\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "===Train===\n",
            "Epoch: 0\n",
            "Loss: 5122885.980 | Acc: 7.000% (7/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 0\n",
            "Loss: 3310953146.880 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 1\n",
            "Loss: 14995.516 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 1\n",
            "Loss: 88118913.680 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 2\n",
            "Loss: 15531.241 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 2\n",
            "Loss: 47984592.520 | Acc: 9.470% (947/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 3\n",
            "Loss: 16392.266 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 3\n",
            "Loss: 21947372.660 | Acc: 10.010% (1001/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 4\n",
            "Loss: 30377.628 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 4\n",
            "Loss: 4269852.875 | Acc: 12.380% (1238/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 5\n",
            "Loss: 24455.188 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 5\n",
            "Loss: 851519.432 | Acc: 8.880% (888/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 6\n",
            "Loss: 14823.695 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 6\n",
            "Loss: 1084117.577 | Acc: 16.830% (1683/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 7\n",
            "Loss: 7893.955 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 7\n",
            "Loss: 619084.225 | Acc: 9.580% (958/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 8\n",
            "Loss: 8966.464 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 8\n",
            "Loss: 515307.130 | Acc: 14.330% (1433/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 9\n",
            "Loss: 5391.579 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 9\n",
            "Loss: 69025.829 | Acc: 14.300% (1430/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 10\n",
            "Loss: 1957.333 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 10\n",
            "Loss: 52871.844 | Acc: 11.870% (1187/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 11\n",
            "Loss: 436.999 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 11\n",
            "Loss: 42018.935 | Acc: 12.290% (1229/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 12\n",
            "Loss: 1416.342 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 12\n",
            "Loss: 24010.366 | Acc: 13.570% (1357/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 13\n",
            "Loss: 1221.672 | Acc: 19.000% (19/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 13\n",
            "Loss: 5596.981 | Acc: 7.190% (719/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 14\n",
            "Loss: 228.875 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 14\n",
            "Loss: 1193.998 | Acc: 12.890% (1289/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 15\n",
            "Loss: 9.894 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 15\n",
            "Loss: 756.609 | Acc: 11.750% (1175/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 16\n",
            "Loss: 20.086 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 16\n",
            "Loss: 517.712 | Acc: 12.070% (1207/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 17\n",
            "Loss: 563.882 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 17\n",
            "Loss: 671.486 | Acc: 11.800% (1180/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 18\n",
            "Loss: 4.543 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 18\n",
            "Loss: 639.869 | Acc: 11.700% (1170/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 19\n",
            "Loss: 3.300 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 19\n",
            "Loss: 1029.542 | Acc: 12.540% (1254/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 20\n",
            "Loss: 2.654 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 20\n",
            "Loss: 1117.882 | Acc: 12.590% (1259/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 21\n",
            "Loss: 2.882 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 21\n",
            "Loss: 901.081 | Acc: 12.010% (1201/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 22\n",
            "Loss: 3.018 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 22\n",
            "Loss: 980.599 | Acc: 12.130% (1213/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 23\n",
            "Loss: 2.966 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 23\n",
            "Loss: 644.896 | Acc: 11.820% (1182/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 24\n",
            "Loss: 2.832 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 24\n",
            "Loss: 572.741 | Acc: 11.740% (1174/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 25\n",
            "Loss: 2.623 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 25\n",
            "Loss: 490.015 | Acc: 9.960% (996/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 26\n",
            "Loss: 2.708 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 26\n",
            "Loss: 475.236 | Acc: 11.810% (1181/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 27\n",
            "Loss: 2.902 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 27\n",
            "Loss: 763.973 | Acc: 12.050% (1205/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 28\n",
            "Loss: 2.879 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 28\n",
            "Loss: 604.869 | Acc: 11.960% (1196/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 29\n",
            "Loss: 2.642 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 29\n",
            "Loss: 531.089 | Acc: 11.500% (1150/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 30\n",
            "Loss: 2.740 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 30\n",
            "Loss: 647.713 | Acc: 11.880% (1188/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 31\n",
            "Loss: 2.573 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 31\n",
            "Loss: 625.918 | Acc: 11.950% (1195/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 32\n",
            "Loss: 2.426 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 32\n",
            "Loss: 591.804 | Acc: 11.860% (1186/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 33\n",
            "Loss: 2.534 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 33\n",
            "Loss: 603.243 | Acc: 12.070% (1207/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 34\n",
            "Loss: 2.459 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 34\n",
            "Loss: 448.839 | Acc: 11.860% (1186/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 35\n",
            "Loss: 2.455 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 35\n",
            "Loss: 743.289 | Acc: 12.070% (1207/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 36\n",
            "Loss: 2.323 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 36\n",
            "Loss: 966.851 | Acc: 12.340% (1234/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 37\n",
            "Loss: 2.453 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 37\n",
            "Loss: 854.753 | Acc: 12.390% (1239/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 38\n",
            "Loss: 2.498 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 38\n",
            "Loss: 877.294 | Acc: 9.970% (997/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 39\n",
            "Loss: 2.478 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 39\n",
            "Loss: 692.428 | Acc: 12.120% (1212/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 40\n",
            "Loss: 2.417 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 40\n",
            "Loss: 586.472 | Acc: 12.080% (1208/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 41\n",
            "Loss: 2.421 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 41\n",
            "Loss: 427.322 | Acc: 11.850% (1185/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 42\n",
            "Loss: 2.377 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 42\n",
            "Loss: 379.679 | Acc: 11.820% (1182/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 43\n",
            "Loss: 2.454 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 43\n",
            "Loss: 339.213 | Acc: 11.640% (1164/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 44\n",
            "Loss: 2.397 | Acc: 19.000% (19/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 44\n",
            "Loss: 305.540 | Acc: 11.610% (1161/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 45\n",
            "Loss: 2.433 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 45\n",
            "Loss: 388.041 | Acc: 11.690% (1169/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 46\n",
            "Loss: 2.400 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 46\n",
            "Loss: 723.377 | Acc: 12.120% (1212/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 47\n",
            "Loss: 2.350 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 47\n",
            "Loss: 652.576 | Acc: 12.130% (1213/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 48\n",
            "Loss: 2.325 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 48\n",
            "Loss: 396.075 | Acc: 11.450% (1145/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 49\n",
            "Loss: 2.455 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 49\n",
            "Loss: 437.663 | Acc: 11.510% (1151/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iwk-6UWnmI4",
        "colab_type": "code",
        "outputId": "64d66c40-b5e0-4a94-d2e6-c0b8f53e7c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2897
        }
      },
      "source": [
        "lr = 0.0001\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "start_epoch = 50\n",
        "epochs = 20\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Train===\n",
            "Epoch: 50\n",
            "Loss: 2.292 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 50\n",
            "Loss: 2609.190 | Acc: 12.440% (1244/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 51\n",
            "Loss: 2.241 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 51\n",
            "Loss: 2071.229 | Acc: 12.350% (1235/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 52\n",
            "Loss: 2.348 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 52\n",
            "Loss: 1288.794 | Acc: 12.070% (1207/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 53\n",
            "Loss: 2.343 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 53\n",
            "Loss: 1136.236 | Acc: 12.040% (1204/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 54\n",
            "Loss: 2.218 | Acc: 18.000% (18/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 54\n",
            "Loss: 2062.610 | Acc: 12.380% (1238/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 55\n",
            "Loss: 2.308 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 55\n",
            "Loss: 1014.241 | Acc: 11.910% (1191/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 56\n",
            "Loss: 2.391 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 56\n",
            "Loss: 819.034 | Acc: 11.790% (1179/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 57\n",
            "Loss: 2.316 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 57\n",
            "Loss: 889.049 | Acc: 11.810% (1181/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 58\n",
            "Loss: 2.274 | Acc: 18.000% (18/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 58\n",
            "Loss: 899.852 | Acc: 11.820% (1182/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 59\n",
            "Loss: 2.312 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 59\n",
            "Loss: 811.129 | Acc: 11.690% (1169/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 60\n",
            "Loss: 2.372 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 60\n",
            "Loss: 511.468 | Acc: 11.560% (1156/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 61\n",
            "Loss: 2.353 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 61\n",
            "Loss: 763.643 | Acc: 11.650% (1165/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 62\n",
            "Loss: 2.303 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 62\n",
            "Loss: 871.895 | Acc: 11.760% (1176/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 63\n",
            "Loss: 2.278 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 63\n",
            "Loss: 825.632 | Acc: 11.810% (1181/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 64\n",
            "Loss: 2.288 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 64\n",
            "Loss: 834.258 | Acc: 11.750% (1175/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 65\n",
            "Loss: 2.336 | Acc: 17.000% (17/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 65\n",
            "Loss: 1136.938 | Acc: 11.900% (1190/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 66\n",
            "Loss: 2.354 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 66\n",
            "Loss: 1360.215 | Acc: 11.990% (1199/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 67\n",
            "Loss: 2.279 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 67\n",
            "Loss: 1301.563 | Acc: 11.970% (1197/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 68\n",
            "Loss: 2.304 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 68\n",
            "Loss: 2027.646 | Acc: 12.220% (1222/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 69\n",
            "Loss: 2.262 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 69\n",
            "Loss: 2126.674 | Acc: 12.350% (1235/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZgSxaktUEN1",
        "colab_type": "text"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywNoNjYjUEN2",
        "colab_type": "text"
      },
      "source": [
        "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neSrmOTRUEN3",
        "colab_type": "text"
      },
      "source": [
        "## ImageNet features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyRAh3_UEN3",
        "colab_type": "text"
      },
      "source": [
        "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
        "\n",
        "__Question 5:__ Pick a model from the list above, adapt it to CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy.\n",
        "\n",
        "\n",
        "__Answer__: I adapted the code provided in here: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html. I used DensNet architecture pretrained on ImageNet, and i trained the whole network (and not only same layers). The obtained results are the following:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on all the dataset | source\n",
        "|------|------|------|------|------|------|\n",
        "|   DenseNet  | 60 | 97 % | 37 %|\t95.04%| https://github.com/kuangliu/pytorch-cifar|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKVAXqpsUEN4",
        "colab_type": "code",
        "outputId": "471434c6-b651-46b5-efb9-229a08796298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12023
        }
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 8\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# Number of epochs to train for\n",
        "num_epochs = 60\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history\n",
        "\n",
        "dataloaders = {'train': trainloader, 'val':testloader}\n",
        "\n",
        "# Train hole model or just some layers\n",
        "feature_extract = False\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "## Define model\n",
        "model_ft = models.densenet121(pretrained=True)\n",
        "set_parameter_requires_grad(model_ft, feature_extract)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t features.conv0.weight\n",
            "\t features.norm0.weight\n",
            "\t features.norm0.bias\n",
            "\t features.denseblock1.denselayer1.norm1.weight\n",
            "\t features.denseblock1.denselayer1.norm1.bias\n",
            "\t features.denseblock1.denselayer1.conv1.weight\n",
            "\t features.denseblock1.denselayer1.norm2.weight\n",
            "\t features.denseblock1.denselayer1.norm2.bias\n",
            "\t features.denseblock1.denselayer1.conv2.weight\n",
            "\t features.denseblock1.denselayer2.norm1.weight\n",
            "\t features.denseblock1.denselayer2.norm1.bias\n",
            "\t features.denseblock1.denselayer2.conv1.weight\n",
            "\t features.denseblock1.denselayer2.norm2.weight\n",
            "\t features.denseblock1.denselayer2.norm2.bias\n",
            "\t features.denseblock1.denselayer2.conv2.weight\n",
            "\t features.denseblock1.denselayer3.norm1.weight\n",
            "\t features.denseblock1.denselayer3.norm1.bias\n",
            "\t features.denseblock1.denselayer3.conv1.weight\n",
            "\t features.denseblock1.denselayer3.norm2.weight\n",
            "\t features.denseblock1.denselayer3.norm2.bias\n",
            "\t features.denseblock1.denselayer3.conv2.weight\n",
            "\t features.denseblock1.denselayer4.norm1.weight\n",
            "\t features.denseblock1.denselayer4.norm1.bias\n",
            "\t features.denseblock1.denselayer4.conv1.weight\n",
            "\t features.denseblock1.denselayer4.norm2.weight\n",
            "\t features.denseblock1.denselayer4.norm2.bias\n",
            "\t features.denseblock1.denselayer4.conv2.weight\n",
            "\t features.denseblock1.denselayer5.norm1.weight\n",
            "\t features.denseblock1.denselayer5.norm1.bias\n",
            "\t features.denseblock1.denselayer5.conv1.weight\n",
            "\t features.denseblock1.denselayer5.norm2.weight\n",
            "\t features.denseblock1.denselayer5.norm2.bias\n",
            "\t features.denseblock1.denselayer5.conv2.weight\n",
            "\t features.denseblock1.denselayer6.norm1.weight\n",
            "\t features.denseblock1.denselayer6.norm1.bias\n",
            "\t features.denseblock1.denselayer6.conv1.weight\n",
            "\t features.denseblock1.denselayer6.norm2.weight\n",
            "\t features.denseblock1.denselayer6.norm2.bias\n",
            "\t features.denseblock1.denselayer6.conv2.weight\n",
            "\t features.transition1.norm.weight\n",
            "\t features.transition1.norm.bias\n",
            "\t features.transition1.conv.weight\n",
            "\t features.denseblock2.denselayer1.norm1.weight\n",
            "\t features.denseblock2.denselayer1.norm1.bias\n",
            "\t features.denseblock2.denselayer1.conv1.weight\n",
            "\t features.denseblock2.denselayer1.norm2.weight\n",
            "\t features.denseblock2.denselayer1.norm2.bias\n",
            "\t features.denseblock2.denselayer1.conv2.weight\n",
            "\t features.denseblock2.denselayer2.norm1.weight\n",
            "\t features.denseblock2.denselayer2.norm1.bias\n",
            "\t features.denseblock2.denselayer2.conv1.weight\n",
            "\t features.denseblock2.denselayer2.norm2.weight\n",
            "\t features.denseblock2.denselayer2.norm2.bias\n",
            "\t features.denseblock2.denselayer2.conv2.weight\n",
            "\t features.denseblock2.denselayer3.norm1.weight\n",
            "\t features.denseblock2.denselayer3.norm1.bias\n",
            "\t features.denseblock2.denselayer3.conv1.weight\n",
            "\t features.denseblock2.denselayer3.norm2.weight\n",
            "\t features.denseblock2.denselayer3.norm2.bias\n",
            "\t features.denseblock2.denselayer3.conv2.weight\n",
            "\t features.denseblock2.denselayer4.norm1.weight\n",
            "\t features.denseblock2.denselayer4.norm1.bias\n",
            "\t features.denseblock2.denselayer4.conv1.weight\n",
            "\t features.denseblock2.denselayer4.norm2.weight\n",
            "\t features.denseblock2.denselayer4.norm2.bias\n",
            "\t features.denseblock2.denselayer4.conv2.weight\n",
            "\t features.denseblock2.denselayer5.norm1.weight\n",
            "\t features.denseblock2.denselayer5.norm1.bias\n",
            "\t features.denseblock2.denselayer5.conv1.weight\n",
            "\t features.denseblock2.denselayer5.norm2.weight\n",
            "\t features.denseblock2.denselayer5.norm2.bias\n",
            "\t features.denseblock2.denselayer5.conv2.weight\n",
            "\t features.denseblock2.denselayer6.norm1.weight\n",
            "\t features.denseblock2.denselayer6.norm1.bias\n",
            "\t features.denseblock2.denselayer6.conv1.weight\n",
            "\t features.denseblock2.denselayer6.norm2.weight\n",
            "\t features.denseblock2.denselayer6.norm2.bias\n",
            "\t features.denseblock2.denselayer6.conv2.weight\n",
            "\t features.denseblock2.denselayer7.norm1.weight\n",
            "\t features.denseblock2.denselayer7.norm1.bias\n",
            "\t features.denseblock2.denselayer7.conv1.weight\n",
            "\t features.denseblock2.denselayer7.norm2.weight\n",
            "\t features.denseblock2.denselayer7.norm2.bias\n",
            "\t features.denseblock2.denselayer7.conv2.weight\n",
            "\t features.denseblock2.denselayer8.norm1.weight\n",
            "\t features.denseblock2.denselayer8.norm1.bias\n",
            "\t features.denseblock2.denselayer8.conv1.weight\n",
            "\t features.denseblock2.denselayer8.norm2.weight\n",
            "\t features.denseblock2.denselayer8.norm2.bias\n",
            "\t features.denseblock2.denselayer8.conv2.weight\n",
            "\t features.denseblock2.denselayer9.norm1.weight\n",
            "\t features.denseblock2.denselayer9.norm1.bias\n",
            "\t features.denseblock2.denselayer9.conv1.weight\n",
            "\t features.denseblock2.denselayer9.norm2.weight\n",
            "\t features.denseblock2.denselayer9.norm2.bias\n",
            "\t features.denseblock2.denselayer9.conv2.weight\n",
            "\t features.denseblock2.denselayer10.norm1.weight\n",
            "\t features.denseblock2.denselayer10.norm1.bias\n",
            "\t features.denseblock2.denselayer10.conv1.weight\n",
            "\t features.denseblock2.denselayer10.norm2.weight\n",
            "\t features.denseblock2.denselayer10.norm2.bias\n",
            "\t features.denseblock2.denselayer10.conv2.weight\n",
            "\t features.denseblock2.denselayer11.norm1.weight\n",
            "\t features.denseblock2.denselayer11.norm1.bias\n",
            "\t features.denseblock2.denselayer11.conv1.weight\n",
            "\t features.denseblock2.denselayer11.norm2.weight\n",
            "\t features.denseblock2.denselayer11.norm2.bias\n",
            "\t features.denseblock2.denselayer11.conv2.weight\n",
            "\t features.denseblock2.denselayer12.norm1.weight\n",
            "\t features.denseblock2.denselayer12.norm1.bias\n",
            "\t features.denseblock2.denselayer12.conv1.weight\n",
            "\t features.denseblock2.denselayer12.norm2.weight\n",
            "\t features.denseblock2.denselayer12.norm2.bias\n",
            "\t features.denseblock2.denselayer12.conv2.weight\n",
            "\t features.transition2.norm.weight\n",
            "\t features.transition2.norm.bias\n",
            "\t features.transition2.conv.weight\n",
            "\t features.denseblock3.denselayer1.norm1.weight\n",
            "\t features.denseblock3.denselayer1.norm1.bias\n",
            "\t features.denseblock3.denselayer1.conv1.weight\n",
            "\t features.denseblock3.denselayer1.norm2.weight\n",
            "\t features.denseblock3.denselayer1.norm2.bias\n",
            "\t features.denseblock3.denselayer1.conv2.weight\n",
            "\t features.denseblock3.denselayer2.norm1.weight\n",
            "\t features.denseblock3.denselayer2.norm1.bias\n",
            "\t features.denseblock3.denselayer2.conv1.weight\n",
            "\t features.denseblock3.denselayer2.norm2.weight\n",
            "\t features.denseblock3.denselayer2.norm2.bias\n",
            "\t features.denseblock3.denselayer2.conv2.weight\n",
            "\t features.denseblock3.denselayer3.norm1.weight\n",
            "\t features.denseblock3.denselayer3.norm1.bias\n",
            "\t features.denseblock3.denselayer3.conv1.weight\n",
            "\t features.denseblock3.denselayer3.norm2.weight\n",
            "\t features.denseblock3.denselayer3.norm2.bias\n",
            "\t features.denseblock3.denselayer3.conv2.weight\n",
            "\t features.denseblock3.denselayer4.norm1.weight\n",
            "\t features.denseblock3.denselayer4.norm1.bias\n",
            "\t features.denseblock3.denselayer4.conv1.weight\n",
            "\t features.denseblock3.denselayer4.norm2.weight\n",
            "\t features.denseblock3.denselayer4.norm2.bias\n",
            "\t features.denseblock3.denselayer4.conv2.weight\n",
            "\t features.denseblock3.denselayer5.norm1.weight\n",
            "\t features.denseblock3.denselayer5.norm1.bias\n",
            "\t features.denseblock3.denselayer5.conv1.weight\n",
            "\t features.denseblock3.denselayer5.norm2.weight\n",
            "\t features.denseblock3.denselayer5.norm2.bias\n",
            "\t features.denseblock3.denselayer5.conv2.weight\n",
            "\t features.denseblock3.denselayer6.norm1.weight\n",
            "\t features.denseblock3.denselayer6.norm1.bias\n",
            "\t features.denseblock3.denselayer6.conv1.weight\n",
            "\t features.denseblock3.denselayer6.norm2.weight\n",
            "\t features.denseblock3.denselayer6.norm2.bias\n",
            "\t features.denseblock3.denselayer6.conv2.weight\n",
            "\t features.denseblock3.denselayer7.norm1.weight\n",
            "\t features.denseblock3.denselayer7.norm1.bias\n",
            "\t features.denseblock3.denselayer7.conv1.weight\n",
            "\t features.denseblock3.denselayer7.norm2.weight\n",
            "\t features.denseblock3.denselayer7.norm2.bias\n",
            "\t features.denseblock3.denselayer7.conv2.weight\n",
            "\t features.denseblock3.denselayer8.norm1.weight\n",
            "\t features.denseblock3.denselayer8.norm1.bias\n",
            "\t features.denseblock3.denselayer8.conv1.weight\n",
            "\t features.denseblock3.denselayer8.norm2.weight\n",
            "\t features.denseblock3.denselayer8.norm2.bias\n",
            "\t features.denseblock3.denselayer8.conv2.weight\n",
            "\t features.denseblock3.denselayer9.norm1.weight\n",
            "\t features.denseblock3.denselayer9.norm1.bias\n",
            "\t features.denseblock3.denselayer9.conv1.weight\n",
            "\t features.denseblock3.denselayer9.norm2.weight\n",
            "\t features.denseblock3.denselayer9.norm2.bias\n",
            "\t features.denseblock3.denselayer9.conv2.weight\n",
            "\t features.denseblock3.denselayer10.norm1.weight\n",
            "\t features.denseblock3.denselayer10.norm1.bias\n",
            "\t features.denseblock3.denselayer10.conv1.weight\n",
            "\t features.denseblock3.denselayer10.norm2.weight\n",
            "\t features.denseblock3.denselayer10.norm2.bias\n",
            "\t features.denseblock3.denselayer10.conv2.weight\n",
            "\t features.denseblock3.denselayer11.norm1.weight\n",
            "\t features.denseblock3.denselayer11.norm1.bias\n",
            "\t features.denseblock3.denselayer11.conv1.weight\n",
            "\t features.denseblock3.denselayer11.norm2.weight\n",
            "\t features.denseblock3.denselayer11.norm2.bias\n",
            "\t features.denseblock3.denselayer11.conv2.weight\n",
            "\t features.denseblock3.denselayer12.norm1.weight\n",
            "\t features.denseblock3.denselayer12.norm1.bias\n",
            "\t features.denseblock3.denselayer12.conv1.weight\n",
            "\t features.denseblock3.denselayer12.norm2.weight\n",
            "\t features.denseblock3.denselayer12.norm2.bias\n",
            "\t features.denseblock3.denselayer12.conv2.weight\n",
            "\t features.denseblock3.denselayer13.norm1.weight\n",
            "\t features.denseblock3.denselayer13.norm1.bias\n",
            "\t features.denseblock3.denselayer13.conv1.weight\n",
            "\t features.denseblock3.denselayer13.norm2.weight\n",
            "\t features.denseblock3.denselayer13.norm2.bias\n",
            "\t features.denseblock3.denselayer13.conv2.weight\n",
            "\t features.denseblock3.denselayer14.norm1.weight\n",
            "\t features.denseblock3.denselayer14.norm1.bias\n",
            "\t features.denseblock3.denselayer14.conv1.weight\n",
            "\t features.denseblock3.denselayer14.norm2.weight\n",
            "\t features.denseblock3.denselayer14.norm2.bias\n",
            "\t features.denseblock3.denselayer14.conv2.weight\n",
            "\t features.denseblock3.denselayer15.norm1.weight\n",
            "\t features.denseblock3.denselayer15.norm1.bias\n",
            "\t features.denseblock3.denselayer15.conv1.weight\n",
            "\t features.denseblock3.denselayer15.norm2.weight\n",
            "\t features.denseblock3.denselayer15.norm2.bias\n",
            "\t features.denseblock3.denselayer15.conv2.weight\n",
            "\t features.denseblock3.denselayer16.norm1.weight\n",
            "\t features.denseblock3.denselayer16.norm1.bias\n",
            "\t features.denseblock3.denselayer16.conv1.weight\n",
            "\t features.denseblock3.denselayer16.norm2.weight\n",
            "\t features.denseblock3.denselayer16.norm2.bias\n",
            "\t features.denseblock3.denselayer16.conv2.weight\n",
            "\t features.denseblock3.denselayer17.norm1.weight\n",
            "\t features.denseblock3.denselayer17.norm1.bias\n",
            "\t features.denseblock3.denselayer17.conv1.weight\n",
            "\t features.denseblock3.denselayer17.norm2.weight\n",
            "\t features.denseblock3.denselayer17.norm2.bias\n",
            "\t features.denseblock3.denselayer17.conv2.weight\n",
            "\t features.denseblock3.denselayer18.norm1.weight\n",
            "\t features.denseblock3.denselayer18.norm1.bias\n",
            "\t features.denseblock3.denselayer18.conv1.weight\n",
            "\t features.denseblock3.denselayer18.norm2.weight\n",
            "\t features.denseblock3.denselayer18.norm2.bias\n",
            "\t features.denseblock3.denselayer18.conv2.weight\n",
            "\t features.denseblock3.denselayer19.norm1.weight\n",
            "\t features.denseblock3.denselayer19.norm1.bias\n",
            "\t features.denseblock3.denselayer19.conv1.weight\n",
            "\t features.denseblock3.denselayer19.norm2.weight\n",
            "\t features.denseblock3.denselayer19.norm2.bias\n",
            "\t features.denseblock3.denselayer19.conv2.weight\n",
            "\t features.denseblock3.denselayer20.norm1.weight\n",
            "\t features.denseblock3.denselayer20.norm1.bias\n",
            "\t features.denseblock3.denselayer20.conv1.weight\n",
            "\t features.denseblock3.denselayer20.norm2.weight\n",
            "\t features.denseblock3.denselayer20.norm2.bias\n",
            "\t features.denseblock3.denselayer20.conv2.weight\n",
            "\t features.denseblock3.denselayer21.norm1.weight\n",
            "\t features.denseblock3.denselayer21.norm1.bias\n",
            "\t features.denseblock3.denselayer21.conv1.weight\n",
            "\t features.denseblock3.denselayer21.norm2.weight\n",
            "\t features.denseblock3.denselayer21.norm2.bias\n",
            "\t features.denseblock3.denselayer21.conv2.weight\n",
            "\t features.denseblock3.denselayer22.norm1.weight\n",
            "\t features.denseblock3.denselayer22.norm1.bias\n",
            "\t features.denseblock3.denselayer22.conv1.weight\n",
            "\t features.denseblock3.denselayer22.norm2.weight\n",
            "\t features.denseblock3.denselayer22.norm2.bias\n",
            "\t features.denseblock3.denselayer22.conv2.weight\n",
            "\t features.denseblock3.denselayer23.norm1.weight\n",
            "\t features.denseblock3.denselayer23.norm1.bias\n",
            "\t features.denseblock3.denselayer23.conv1.weight\n",
            "\t features.denseblock3.denselayer23.norm2.weight\n",
            "\t features.denseblock3.denselayer23.norm2.bias\n",
            "\t features.denseblock3.denselayer23.conv2.weight\n",
            "\t features.denseblock3.denselayer24.norm1.weight\n",
            "\t features.denseblock3.denselayer24.norm1.bias\n",
            "\t features.denseblock3.denselayer24.conv1.weight\n",
            "\t features.denseblock3.denselayer24.norm2.weight\n",
            "\t features.denseblock3.denselayer24.norm2.bias\n",
            "\t features.denseblock3.denselayer24.conv2.weight\n",
            "\t features.transition3.norm.weight\n",
            "\t features.transition3.norm.bias\n",
            "\t features.transition3.conv.weight\n",
            "\t features.denseblock4.denselayer1.norm1.weight\n",
            "\t features.denseblock4.denselayer1.norm1.bias\n",
            "\t features.denseblock4.denselayer1.conv1.weight\n",
            "\t features.denseblock4.denselayer1.norm2.weight\n",
            "\t features.denseblock4.denselayer1.norm2.bias\n",
            "\t features.denseblock4.denselayer1.conv2.weight\n",
            "\t features.denseblock4.denselayer2.norm1.weight\n",
            "\t features.denseblock4.denselayer2.norm1.bias\n",
            "\t features.denseblock4.denselayer2.conv1.weight\n",
            "\t features.denseblock4.denselayer2.norm2.weight\n",
            "\t features.denseblock4.denselayer2.norm2.bias\n",
            "\t features.denseblock4.denselayer2.conv2.weight\n",
            "\t features.denseblock4.denselayer3.norm1.weight\n",
            "\t features.denseblock4.denselayer3.norm1.bias\n",
            "\t features.denseblock4.denselayer3.conv1.weight\n",
            "\t features.denseblock4.denselayer3.norm2.weight\n",
            "\t features.denseblock4.denselayer3.norm2.bias\n",
            "\t features.denseblock4.denselayer3.conv2.weight\n",
            "\t features.denseblock4.denselayer4.norm1.weight\n",
            "\t features.denseblock4.denselayer4.norm1.bias\n",
            "\t features.denseblock4.denselayer4.conv1.weight\n",
            "\t features.denseblock4.denselayer4.norm2.weight\n",
            "\t features.denseblock4.denselayer4.norm2.bias\n",
            "\t features.denseblock4.denselayer4.conv2.weight\n",
            "\t features.denseblock4.denselayer5.norm1.weight\n",
            "\t features.denseblock4.denselayer5.norm1.bias\n",
            "\t features.denseblock4.denselayer5.conv1.weight\n",
            "\t features.denseblock4.denselayer5.norm2.weight\n",
            "\t features.denseblock4.denselayer5.norm2.bias\n",
            "\t features.denseblock4.denselayer5.conv2.weight\n",
            "\t features.denseblock4.denselayer6.norm1.weight\n",
            "\t features.denseblock4.denselayer6.norm1.bias\n",
            "\t features.denseblock4.denselayer6.conv1.weight\n",
            "\t features.denseblock4.denselayer6.norm2.weight\n",
            "\t features.denseblock4.denselayer6.norm2.bias\n",
            "\t features.denseblock4.denselayer6.conv2.weight\n",
            "\t features.denseblock4.denselayer7.norm1.weight\n",
            "\t features.denseblock4.denselayer7.norm1.bias\n",
            "\t features.denseblock4.denselayer7.conv1.weight\n",
            "\t features.denseblock4.denselayer7.norm2.weight\n",
            "\t features.denseblock4.denselayer7.norm2.bias\n",
            "\t features.denseblock4.denselayer7.conv2.weight\n",
            "\t features.denseblock4.denselayer8.norm1.weight\n",
            "\t features.denseblock4.denselayer8.norm1.bias\n",
            "\t features.denseblock4.denselayer8.conv1.weight\n",
            "\t features.denseblock4.denselayer8.norm2.weight\n",
            "\t features.denseblock4.denselayer8.norm2.bias\n",
            "\t features.denseblock4.denselayer8.conv2.weight\n",
            "\t features.denseblock4.denselayer9.norm1.weight\n",
            "\t features.denseblock4.denselayer9.norm1.bias\n",
            "\t features.denseblock4.denselayer9.conv1.weight\n",
            "\t features.denseblock4.denselayer9.norm2.weight\n",
            "\t features.denseblock4.denselayer9.norm2.bias\n",
            "\t features.denseblock4.denselayer9.conv2.weight\n",
            "\t features.denseblock4.denselayer10.norm1.weight\n",
            "\t features.denseblock4.denselayer10.norm1.bias\n",
            "\t features.denseblock4.denselayer10.conv1.weight\n",
            "\t features.denseblock4.denselayer10.norm2.weight\n",
            "\t features.denseblock4.denselayer10.norm2.bias\n",
            "\t features.denseblock4.denselayer10.conv2.weight\n",
            "\t features.denseblock4.denselayer11.norm1.weight\n",
            "\t features.denseblock4.denselayer11.norm1.bias\n",
            "\t features.denseblock4.denselayer11.conv1.weight\n",
            "\t features.denseblock4.denselayer11.norm2.weight\n",
            "\t features.denseblock4.denselayer11.norm2.bias\n",
            "\t features.denseblock4.denselayer11.conv2.weight\n",
            "\t features.denseblock4.denselayer12.norm1.weight\n",
            "\t features.denseblock4.denselayer12.norm1.bias\n",
            "\t features.denseblock4.denselayer12.conv1.weight\n",
            "\t features.denseblock4.denselayer12.norm2.weight\n",
            "\t features.denseblock4.denselayer12.norm2.bias\n",
            "\t features.denseblock4.denselayer12.conv2.weight\n",
            "\t features.denseblock4.denselayer13.norm1.weight\n",
            "\t features.denseblock4.denselayer13.norm1.bias\n",
            "\t features.denseblock4.denselayer13.conv1.weight\n",
            "\t features.denseblock4.denselayer13.norm2.weight\n",
            "\t features.denseblock4.denselayer13.norm2.bias\n",
            "\t features.denseblock4.denselayer13.conv2.weight\n",
            "\t features.denseblock4.denselayer14.norm1.weight\n",
            "\t features.denseblock4.denselayer14.norm1.bias\n",
            "\t features.denseblock4.denselayer14.conv1.weight\n",
            "\t features.denseblock4.denselayer14.norm2.weight\n",
            "\t features.denseblock4.denselayer14.norm2.bias\n",
            "\t features.denseblock4.denselayer14.conv2.weight\n",
            "\t features.denseblock4.denselayer15.norm1.weight\n",
            "\t features.denseblock4.denselayer15.norm1.bias\n",
            "\t features.denseblock4.denselayer15.conv1.weight\n",
            "\t features.denseblock4.denselayer15.norm2.weight\n",
            "\t features.denseblock4.denselayer15.norm2.bias\n",
            "\t features.denseblock4.denselayer15.conv2.weight\n",
            "\t features.denseblock4.denselayer16.norm1.weight\n",
            "\t features.denseblock4.denselayer16.norm1.bias\n",
            "\t features.denseblock4.denselayer16.conv1.weight\n",
            "\t features.denseblock4.denselayer16.norm2.weight\n",
            "\t features.denseblock4.denselayer16.norm2.bias\n",
            "\t features.denseblock4.denselayer16.conv2.weight\n",
            "\t features.norm5.weight\n",
            "\t features.norm5.bias\n",
            "\t classifier.weight\n",
            "\t classifier.bias\n",
            "Epoch 0/59\n",
            "----------\n",
            "train Loss: 2.5988 Acc: 0.0900\n",
            "val Loss: 2.4137 Acc: 0.0863\n",
            "\n",
            "Epoch 1/59\n",
            "----------\n",
            "train Loss: 2.4719 Acc: 0.1000\n",
            "val Loss: 2.3954 Acc: 0.1068\n",
            "\n",
            "Epoch 2/59\n",
            "----------\n",
            "train Loss: 2.3574 Acc: 0.1300\n",
            "val Loss: 2.4618 Acc: 0.1188\n",
            "\n",
            "Epoch 3/59\n",
            "----------\n",
            "train Loss: 2.2578 Acc: 0.1600\n",
            "val Loss: 2.5412 Acc: 0.1297\n",
            "\n",
            "Epoch 4/59\n",
            "----------\n",
            "train Loss: 2.2274 Acc: 0.1600\n",
            "val Loss: 2.5141 Acc: 0.1354\n",
            "\n",
            "Epoch 5/59\n",
            "----------\n",
            "train Loss: 2.1977 Acc: 0.2200\n",
            "val Loss: 2.5368 Acc: 0.1434\n",
            "\n",
            "Epoch 6/59\n",
            "----------\n",
            "train Loss: 2.1112 Acc: 0.2000\n",
            "val Loss: 2.4878 Acc: 0.1504\n",
            "\n",
            "Epoch 7/59\n",
            "----------\n",
            "train Loss: 2.0182 Acc: 0.3400\n",
            "val Loss: 2.4824 Acc: 0.1629\n",
            "\n",
            "Epoch 8/59\n",
            "----------\n",
            "train Loss: 2.0727 Acc: 0.2900\n",
            "val Loss: 2.4532 Acc: 0.1718\n",
            "\n",
            "Epoch 9/59\n",
            "----------\n",
            "train Loss: 1.8873 Acc: 0.4300\n",
            "val Loss: 2.4445 Acc: 0.1819\n",
            "\n",
            "Epoch 10/59\n",
            "----------\n",
            "train Loss: 2.0315 Acc: 0.3100\n",
            "val Loss: 2.4380 Acc: 0.1844\n",
            "\n",
            "Epoch 11/59\n",
            "----------\n",
            "train Loss: 1.8176 Acc: 0.4400\n",
            "val Loss: 2.4498 Acc: 0.1914\n",
            "\n",
            "Epoch 12/59\n",
            "----------\n",
            "train Loss: 1.8544 Acc: 0.4100\n",
            "val Loss: 2.3729 Acc: 0.2050\n",
            "\n",
            "Epoch 13/59\n",
            "----------\n",
            "train Loss: 1.8813 Acc: 0.3600\n",
            "val Loss: 2.3483 Acc: 0.2139\n",
            "\n",
            "Epoch 14/59\n",
            "----------\n",
            "train Loss: 1.8295 Acc: 0.4300\n",
            "val Loss: 2.3548 Acc: 0.2183\n",
            "\n",
            "Epoch 15/59\n",
            "----------\n",
            "train Loss: 1.8493 Acc: 0.4000\n",
            "val Loss: 2.3322 Acc: 0.2218\n",
            "\n",
            "Epoch 16/59\n",
            "----------\n",
            "train Loss: 1.7198 Acc: 0.5000\n",
            "val Loss: 2.3539 Acc: 0.2216\n",
            "\n",
            "Epoch 17/59\n",
            "----------\n",
            "train Loss: 1.7381 Acc: 0.4500\n",
            "val Loss: 2.3041 Acc: 0.2290\n",
            "\n",
            "Epoch 18/59\n",
            "----------\n",
            "train Loss: 1.7155 Acc: 0.4500\n",
            "val Loss: 2.2960 Acc: 0.2375\n",
            "\n",
            "Epoch 19/59\n",
            "----------\n",
            "train Loss: 1.6577 Acc: 0.4800\n",
            "val Loss: 2.2883 Acc: 0.2430\n",
            "\n",
            "Epoch 20/59\n",
            "----------\n",
            "train Loss: 1.6033 Acc: 0.4900\n",
            "val Loss: 2.2926 Acc: 0.2423\n",
            "\n",
            "Epoch 21/59\n",
            "----------\n",
            "train Loss: 1.6500 Acc: 0.4600\n",
            "val Loss: 2.2715 Acc: 0.2481\n",
            "\n",
            "Epoch 22/59\n",
            "----------\n",
            "train Loss: 1.5144 Acc: 0.5700\n",
            "val Loss: 2.2614 Acc: 0.2572\n",
            "\n",
            "Epoch 23/59\n",
            "----------\n",
            "train Loss: 1.5795 Acc: 0.5000\n",
            "val Loss: 2.2215 Acc: 0.2618\n",
            "\n",
            "Epoch 24/59\n",
            "----------\n",
            "train Loss: 1.5040 Acc: 0.5400\n",
            "val Loss: 2.1932 Acc: 0.2735\n",
            "\n",
            "Epoch 25/59\n",
            "----------\n",
            "train Loss: 1.5212 Acc: 0.5500\n",
            "val Loss: 2.2360 Acc: 0.2706\n",
            "\n",
            "Epoch 26/59\n",
            "----------\n",
            "train Loss: 1.4803 Acc: 0.5500\n",
            "val Loss: 2.2400 Acc: 0.2720\n",
            "\n",
            "Epoch 27/59\n",
            "----------\n",
            "train Loss: 1.4018 Acc: 0.6100\n",
            "val Loss: 2.1958 Acc: 0.2862\n",
            "\n",
            "Epoch 28/59\n",
            "----------\n",
            "train Loss: 1.3699 Acc: 0.6300\n",
            "val Loss: 2.1831 Acc: 0.2813\n",
            "\n",
            "Epoch 29/59\n",
            "----------\n",
            "train Loss: 1.3926 Acc: 0.5800\n",
            "val Loss: 2.2099 Acc: 0.2801\n",
            "\n",
            "Epoch 30/59\n",
            "----------\n",
            "train Loss: 1.3412 Acc: 0.6300\n",
            "val Loss: 2.1840 Acc: 0.2896\n",
            "\n",
            "Epoch 31/59\n",
            "----------\n",
            "train Loss: 1.3088 Acc: 0.6100\n",
            "val Loss: 2.1573 Acc: 0.2900\n",
            "\n",
            "Epoch 32/59\n",
            "----------\n",
            "train Loss: 1.2482 Acc: 0.6300\n",
            "val Loss: 2.1839 Acc: 0.2956\n",
            "\n",
            "Epoch 33/59\n",
            "----------\n",
            "train Loss: 1.1428 Acc: 0.7200\n",
            "val Loss: 2.1908 Acc: 0.2979\n",
            "\n",
            "Epoch 34/59\n",
            "----------\n",
            "train Loss: 1.1736 Acc: 0.6600\n",
            "val Loss: 2.1608 Acc: 0.3019\n",
            "\n",
            "Epoch 35/59\n",
            "----------\n",
            "train Loss: 1.2249 Acc: 0.6200\n",
            "val Loss: 2.1650 Acc: 0.3001\n",
            "\n",
            "Epoch 36/59\n",
            "----------\n",
            "train Loss: 1.1708 Acc: 0.6600\n",
            "val Loss: 2.1447 Acc: 0.3024\n",
            "\n",
            "Epoch 37/59\n",
            "----------\n",
            "train Loss: 1.1956 Acc: 0.6700\n",
            "val Loss: 2.1698 Acc: 0.3014\n",
            "\n",
            "Epoch 38/59\n",
            "----------\n",
            "train Loss: 1.2829 Acc: 0.6000\n",
            "val Loss: 2.1816 Acc: 0.3043\n",
            "\n",
            "Epoch 39/59\n",
            "----------\n",
            "train Loss: 1.1772 Acc: 0.6300\n",
            "val Loss: 2.1464 Acc: 0.3149\n",
            "\n",
            "Epoch 40/59\n",
            "----------\n",
            "train Loss: 1.1729 Acc: 0.6300\n",
            "val Loss: 2.0939 Acc: 0.3176\n",
            "\n",
            "Epoch 41/59\n",
            "----------\n",
            "train Loss: 1.1118 Acc: 0.6900\n",
            "val Loss: 2.1428 Acc: 0.3110\n",
            "\n",
            "Epoch 42/59\n",
            "----------\n",
            "train Loss: 1.1220 Acc: 0.6200\n",
            "val Loss: 2.1362 Acc: 0.3170\n",
            "\n",
            "Epoch 43/59\n",
            "----------\n",
            "train Loss: 1.1653 Acc: 0.6500\n",
            "val Loss: 2.1457 Acc: 0.3204\n",
            "\n",
            "Epoch 44/59\n",
            "----------\n",
            "train Loss: 1.0365 Acc: 0.7200\n",
            "val Loss: 2.1129 Acc: 0.3194\n",
            "\n",
            "Epoch 45/59\n",
            "----------\n",
            "train Loss: 1.0583 Acc: 0.7500\n",
            "val Loss: 2.1197 Acc: 0.3253\n",
            "\n",
            "Epoch 46/59\n",
            "----------\n",
            "train Loss: 0.9048 Acc: 0.7500\n",
            "val Loss: 2.1476 Acc: 0.3190\n",
            "\n",
            "Epoch 47/59\n",
            "----------\n",
            "train Loss: 0.9873 Acc: 0.7200\n",
            "val Loss: 2.1158 Acc: 0.3253\n",
            "\n",
            "Epoch 48/59\n",
            "----------\n",
            "train Loss: 1.0200 Acc: 0.6900\n",
            "val Loss: 2.1468 Acc: 0.3235\n",
            "\n",
            "Epoch 49/59\n",
            "----------\n",
            "train Loss: 0.9041 Acc: 0.8000\n",
            "val Loss: 2.1257 Acc: 0.3268\n",
            "\n",
            "Epoch 50/59\n",
            "----------\n",
            "train Loss: 0.9900 Acc: 0.7200\n",
            "val Loss: 2.1637 Acc: 0.3292\n",
            "\n",
            "Epoch 51/59\n",
            "----------\n",
            "train Loss: 0.9617 Acc: 0.7400\n",
            "val Loss: 2.1498 Acc: 0.3322\n",
            "\n",
            "Epoch 52/59\n",
            "----------\n",
            "train Loss: 0.8467 Acc: 0.7300\n",
            "val Loss: 2.1281 Acc: 0.3371\n",
            "\n",
            "Epoch 53/59\n",
            "----------\n",
            "train Loss: 0.9983 Acc: 0.6300\n",
            "val Loss: 2.1655 Acc: 0.3377\n",
            "\n",
            "Epoch 54/59\n",
            "----------\n",
            "train Loss: 0.8394 Acc: 0.7900\n",
            "val Loss: 2.1028 Acc: 0.3422\n",
            "\n",
            "Epoch 55/59\n",
            "----------\n",
            "train Loss: 0.8871 Acc: 0.7600\n",
            "val Loss: 2.1606 Acc: 0.3278\n",
            "\n",
            "Epoch 56/59\n",
            "----------\n",
            "train Loss: 0.9649 Acc: 0.6900\n",
            "val Loss: 2.1607 Acc: 0.3327\n",
            "\n",
            "Epoch 57/59\n",
            "----------\n",
            "train Loss: 0.8784 Acc: 0.7700\n",
            "val Loss: 2.1568 Acc: 0.3371\n",
            "\n",
            "Epoch 58/59\n",
            "----------\n",
            "train Loss: 0.7505 Acc: 0.8300\n",
            "val Loss: 2.1131 Acc: 0.3434\n",
            "\n",
            "Epoch 59/59\n",
            "----------\n",
            "train Loss: 0.7515 Acc: 0.8500\n",
            "val Loss: 2.1208 Acc: 0.3433\n",
            "\n",
            "Training complete in 8m 57s\n",
            "Best val Acc: 0.343400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KWej3xlUEN6",
        "colab_type": "text"
      },
      "source": [
        "## DCGan features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPPeFKl9UEN7",
        "colab_type": "text"
      },
      "source": [
        "GANs correspond to an unsupervised technique for generating images. In https://arxiv.org/pdf/1511.06434.pdf, Sec. 5.1 shows that the representation obtained from the Discriminator has some nice generalization properties on CIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rudJIaEKUEN8",
        "colab_type": "text"
      },
      "source": [
        "__Question 6:__  Using for instance a pretrained model from https://github.com/soumith/dcgan.torch combined with https://github.com/pytorch/examples/tree/master/dcgan, propose a model to train on $\\mathcal{X}_{\\text{train}}$. Train it and report its accuracy.\n",
        "\n",
        "*Hint:* You can use the library: https://github.com/bshillingford/python-torchfile to load the weights of a model from torch(Lua) to pytorch(python).\n",
        "\n",
        "__Answer__: I used the architecture of the descriminator provided in the DCGAN paper (https://github.com/pytorch/examples/blob/master/dcgan/main.py#L182) and the pretrained weights of this github repo (https://github.com/donand/GAN_pytorch) trained on celebA (I didnt't find any repo with pretrained discriminator on ImageNet). I followed the same feature extraction method combined with a SVM classifier as described in Sec. 5.1. I obtaied an accuracy = **28.38 %** (after a cross validation for hyperparameters tunning). I think this score can be improved as the weights for the descriminator are not perfect (the used network is trained for 10 epochs only)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pB-C2i7UEN9",
        "colab_type": "code",
        "outputId": "0382ae0f-29cc-4074-81f2-3c8822957f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Define model\n",
        "\n",
        "image_size = (1, 64, 64)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels=3, nf=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.flattened_size = 64 * \\\n",
        "            (image_size[1]//2//2//2) * (image_size[2]//2//2//2)\n",
        "        self.conv_block = nn.Sequential(\n",
        "            # input is (3, 32, 32)\n",
        "            nn.Conv2d(input_channels, nf, 4, padding=1, stride=2),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            # input is (nf, 16, 16)\n",
        "            nn.Conv2d(nf, nf * 2, 4, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(nf * 2),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            # input is (nf*2, 8, 8)\n",
        "            nn.Conv2d(nf * 2, nf * 4, 4, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(nf * 4),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(nf * 4, nf * 8, 4, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(nf * 8),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            # input is (nf*4, 4, 4)\n",
        "            nn.Conv2d(nf * 8, 1, 4, padding=0, stride=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        return x.view(-1, 1)\n",
        "    \n",
        "    def get_features(self, input):\n",
        "      \n",
        "        self.conv1 = nn.Sequential(\n",
        "            self.conv_block._modules['0'],\n",
        "            self.conv_block._modules['1'],\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "              self.conv_block._modules['2'],\n",
        "              self.conv_block._modules['3'],\n",
        "              self.conv_block._modules['4'],\n",
        "          )\n",
        "        self.conv3 = nn.Sequential(\n",
        "              self.conv_block._modules['5'],\n",
        "              self.conv_block._modules['6'],\n",
        "              self.conv_block._modules['7'],\n",
        "          )\n",
        "        self.conv4 = nn.Sequential(\n",
        "              self.conv_block._modules['8'],\n",
        "              self.conv_block._modules['9'],\n",
        "              self.conv_block._modules['10'],\n",
        "          )\n",
        "        self.conv5 = nn.Sequential(\n",
        "              self.conv_block._modules['11'],\n",
        "              self.conv_block._modules['12']\n",
        "          )\n",
        "      \n",
        "      \n",
        "        out_conv1 = self.conv1(input)\n",
        "        out_conv2 = self.conv2(out_conv1)\n",
        "        out_conv3 = self.conv3(out_conv2)\n",
        "        out_conv4 = self.conv4(out_conv3)\n",
        "        out_conv5 = self.conv5(out_conv4)\n",
        "\n",
        "        max_pool1 = nn.MaxPool2d(int(out_conv1.size(2) / 4))\n",
        "        max_pool2 = nn.MaxPool2d(int(out_conv2.size(2) / 4))\n",
        "        max_pool3 = nn.MaxPool2d(int(out_conv3.size(2) / 4))\n",
        "        # max_pool4 = nn.MaxPool2d(int(out_conv4.size(2) / 4))\n",
        "\n",
        "        vector1 = max_pool1(out_conv1).view(input.size(0), -1).squeeze(1)\n",
        "        vector2 = max_pool2(out_conv2).view(input.size(0), -1).squeeze(1)\n",
        "        vector3 = max_pool3(out_conv3).view(input.size(0), -1).squeeze(1)\n",
        "        # vector4 = max_pool4(out_conv4).view(input.size(0), -1).squeeze(1)\n",
        "\n",
        "        return torch.cat((vector1, vector2, vector3), 1)\n",
        "\n",
        "# Load model\n",
        "netD = Discriminator().to(device)\n",
        "netD.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# Load train\n",
        "transform = transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.CenterCrop(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               ])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./root', train=True, download=True, transform=transform)\n",
        "# Keep 100 examples\n",
        "p = np.random.permutation(len(trainset.data))\n",
        "trainset.data = trainset.data[p][:100,:,:,:]\n",
        "trainset.targets = np.array(trainset.targets)[p][:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load test\n",
        "testset = torchvision.datasets.CIFAR10(root='./root', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Extract Features\n",
        "input = torch.FloatTensor(100, 3, 64, 64).cuda()\n",
        "features = np.array([])\n",
        "labels = np.array([])\n",
        "\n",
        "for i, data in enumerate(trainloader):\n",
        "    imgs, label = data\n",
        "    imgs = imgs.cuda()\n",
        "    input.resize_as_(imgs).copy_(imgs)\n",
        "    input_v = Variable(input)\n",
        "\n",
        "    feature = netD.get_features(input_v)\n",
        "    feature = feature.data.cpu().numpy()\n",
        "    feature = feature.astype(np.float16)\n",
        "    if features.size == 0:\n",
        "        features = feature\n",
        "        labels = label\n",
        "    else:\n",
        "        features = np.concatenate((features, feature), axis=0)\n",
        "        labels = np.concatenate((labels, label), axis=0)\n",
        "        \n",
        "# Extract test features\n",
        "tfeatures = np.array([])\n",
        "tlabels = np.array([])\n",
        "for i, data in enumerate(testloader):\n",
        "    imgs, label = data\n",
        "    imgs = imgs.cuda()\n",
        "    input.resize_as_(imgs).copy_(imgs)\n",
        "    input_v = Variable(input)\n",
        "\n",
        "    feature = netD.get_features(input_v)\n",
        "    feature = feature.data.cpu().numpy()\n",
        "    feature = feature.astype(np.float16)\n",
        "    if tfeatures.size == 0:\n",
        "        tfeatures = feature\n",
        "        tlabels = label\n",
        "    else:\n",
        "        tfeatures = np.concatenate((tfeatures, feature), axis=0)\n",
        "        tlabels = np.concatenate((tlabels, label), axis=0)\n",
        "        \n",
        "print(\"Size of test\")\n",
        "print('number of samples ', tlabels.shape)\n",
        "print(tfeatures.shape)\n",
        "\n",
        "# Training\n",
        "print(\"Size of training\")\n",
        "print('number of samples ', labels.shape)\n",
        "print(features.shape)\n",
        "\n",
        "clf = svm.SVC(C=8, decision_function_shape='ovo', gamma='scale')\n",
        "clf.fit(features, labels)\n",
        "\n",
        "# Predict\n",
        "print('predict svm')\n",
        "val_labels = tlabels.squeeze()\n",
        "predicted_labels = clf.predict(tfeatures)\n",
        "print(len(val_labels), len(predicted_labels))\n",
        "a = predicted_labels == val_labels\n",
        "print(np.sum(a))\n",
        "accuracy = np.sum(predicted_labels == val_labels) / len(val_labels)\n",
        "\n",
        "print('svm results: {} % accuracy'.format(accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Size of test\n",
            "number of samples  (10000,)\n",
            "(10000, 14336)\n",
            "Size of training\n",
            "number of samples  torch.Size([100])\n",
            "(100, 14336)\n",
            "predict svm\n",
            "10000 10000\n",
            "2838\n",
            "svm results: 28.38 % accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p04jxVYUEN-",
        "colab_type": "text"
      },
      "source": [
        "# Incorporating *a priori*\n",
        "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
        "\n",
        "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
        "\n",
        "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
        "\n",
        "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
        "\n",
        "Otherwise, one has to handle several boundary effects.\n",
        "\n",
        "__Question 7:__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC-R-9ZAUEN_",
        "colab_type": "text"
      },
      "source": [
        "__Answer__: The problem with small resolution images is that the transformation may destroy the features that classify the image and make it difficult to classify even for a human. For example, scaling with a large factor can zoom in the middle of the image, and since we are dealing with a low resolution image, the middle pixels are not representative of the class. To remedy this, the transformations must be performed by limiting their effect, for example, if it is a translation, a very small translation step must be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYVPuxkUUEN_",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrWt4zQtUEOA",
        "colab_type": "text"
      },
      "source": [
        "__Question 8:__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ and __Question 4__ with them and report the accuracies.\n",
        "\n",
        "__Answer__: Some transformations that could be used are: Rotation, RandomCrop, ColorJitter and RandomGrayScale. After training the models from Question 3 and 4 using this transforms and tunning the huperparameters, we find:\n",
        "\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on all the dataset | source\n",
        "|------|------|------|------|------|------|\n",
        "|   Resnet18  | 60 | 82 % | 27 %|93.02 % | https://github.com/kuangliu/pytorch-cifar|\n",
        "|   Resnet50  | 70 | 95 % | 19 %|93.62 % | https://github.com/kuangliu/pytorch-cifar|\n",
        "|   VGG16  | 90 | 32 % | 15 %|\t92.64%| https://github.com/kuangliu/pytorch-cifar|\n",
        "|   VGG19  | 100 | 16 % | 11.6 %|91.52 % | https://www.researchgate.net/publication/324584468_MaxGain_Regularisation_of_Neural_Networks_by_Constraining_Activation_Magnitudes/figures?lo=1|\n",
        "\n",
        "We see that in some cases the results are improved, but only with a small percentage compared to the results found in questions 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T21XXAYUUEOA",
        "colab_type": "code",
        "outputId": "700fe032-4652-421e-a004-0a417ba141a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7253
        }
      },
      "source": [
        "# hyperparameters\n",
        "lr = 0.01\n",
        "batch_size = 64\n",
        "\n",
        "# Data augumentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# Load train\n",
        "trainset = torchvision.datasets.CIFAR10(root='./root', train=True, download=True, transform=transform_train)\n",
        "# Keep 100 examples\n",
        "p = np.random.permutation(len(trainset.data))\n",
        "trainset.data = trainset.data[p][:100,:,:,:]\n",
        "trainset.targets = np.array(trainset.targets)[p][:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load test\n",
        "testset = torchvision.datasets.CIFAR10(root='./root', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Load architecture: Resnet / VGG\n",
        "net = vgg19()\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Train network\n",
        "def train(epoch):\n",
        "  \n",
        "    print('\\n===Train===\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print( 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n===Test===\\nEpoch: %d' % epoch)\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "start_epoch = 0\n",
        "epochs = 50\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "===Train===\n",
            "Epoch: 0\n",
            "Loss: 6.899 | Acc: 0.000% (0/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 0\n",
            "Loss: 6.884 | Acc: 9.950% (995/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 1\n",
            "Loss: 6.870 | Acc: 3.000% (3/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 1\n",
            "Loss: 6.834 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 2\n",
            "Loss: 6.787 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 2\n",
            "Loss: 6.676 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 3\n",
            "Loss: 6.473 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 3\n",
            "Loss: 5.084 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 4\n",
            "Loss: 9.622 | Acc: 19.000% (19/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 4\n",
            "Loss: 6.699 | Acc: 9.980% (998/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 5\n",
            "Loss: 6.790 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 5\n",
            "Loss: 6.633 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 6\n",
            "Loss: 6.770 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 6\n",
            "Loss: 6.273 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 7\n",
            "Loss: 9.717 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 7\n",
            "Loss: 6.772 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 8\n",
            "Loss: 6.927 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 8\n",
            "Loss: 6.757 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 9\n",
            "Loss: 6.838 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 9\n",
            "Loss: 6.747 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 10\n",
            "Loss: 6.805 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 10\n",
            "Loss: 6.747 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 11\n",
            "Loss: 6.768 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 11\n",
            "Loss: 6.737 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 12\n",
            "Loss: 6.750 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 12\n",
            "Loss: 6.717 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 13\n",
            "Loss: 6.736 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 13\n",
            "Loss: 6.693 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 14\n",
            "Loss: 6.711 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 14\n",
            "Loss: 6.671 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 15\n",
            "Loss: 6.683 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 15\n",
            "Loss: 6.651 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 16\n",
            "Loss: 6.662 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 16\n",
            "Loss: 6.624 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 17\n",
            "Loss: 6.630 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 17\n",
            "Loss: 6.597 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 18\n",
            "Loss: 6.605 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 18\n",
            "Loss: 6.555 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 19\n",
            "Loss: 6.555 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 19\n",
            "Loss: 6.486 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 20\n",
            "Loss: 6.602 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 20\n",
            "Loss: 6.362 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 21\n",
            "Loss: 6.689 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 21\n",
            "Loss: 6.338 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 22\n",
            "Loss: 6.471 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 22\n",
            "Loss: 6.390 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 23\n",
            "Loss: 6.463 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 23\n",
            "Loss: 6.393 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 24\n",
            "Loss: 6.420 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 24\n",
            "Loss: 6.284 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 25\n",
            "Loss: 6.303 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 25\n",
            "Loss: 5.979 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 26\n",
            "Loss: 5.934 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 26\n",
            "Loss: 6.034 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 27\n",
            "Loss: 6.918 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 27\n",
            "Loss: 6.258 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 28\n",
            "Loss: 6.347 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 28\n",
            "Loss: 6.206 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 29\n",
            "Loss: 6.244 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 29\n",
            "Loss: 6.144 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 30\n",
            "Loss: 6.178 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 30\n",
            "Loss: 6.015 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 31\n",
            "Loss: 6.194 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 31\n",
            "Loss: 5.813 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 32\n",
            "Loss: 5.931 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 32\n",
            "Loss: 5.584 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 33\n",
            "Loss: 5.825 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 33\n",
            "Loss: 5.183 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 34\n",
            "Loss: 5.382 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 34\n",
            "Loss: 4.925 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 35\n",
            "Loss: 5.591 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 35\n",
            "Loss: 4.820 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 36\n",
            "Loss: 5.235 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 36\n",
            "Loss: 4.649 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 37\n",
            "Loss: 5.282 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 37\n",
            "Loss: 4.369 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 38\n",
            "Loss: 4.822 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 38\n",
            "Loss: 4.154 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 39\n",
            "Loss: 4.537 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 39\n",
            "Loss: 3.946 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 40\n",
            "Loss: 4.180 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 40\n",
            "Loss: 3.694 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 41\n",
            "Loss: 4.227 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 41\n",
            "Loss: 3.443 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 42\n",
            "Loss: 3.672 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 42\n",
            "Loss: 3.194 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 43\n",
            "Loss: 3.381 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 43\n",
            "Loss: 2.885 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 44\n",
            "Loss: 2.977 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 44\n",
            "Loss: 2.531 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 45\n",
            "Loss: 2.556 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 45\n",
            "Loss: 2.653 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 46\n",
            "Loss: 2.525 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 46\n",
            "Loss: 2.997 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 47\n",
            "Loss: 2.579 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 47\n",
            "Loss: 2.367 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 48\n",
            "Loss: 2.572 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 48\n",
            "Loss: 2.412 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 49\n",
            "Loss: 2.642 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 49\n",
            "Loss: 2.418 | Acc: 10.000% (1000/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy-Q_BjjVcI4",
        "colab_type": "code",
        "outputId": "b38f52e7-183d-411b-a190-aa6f8c2a2579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4337
        }
      },
      "source": [
        "lr = 0.00001\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "start_epoch = 50\n",
        "epochs = 30\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Train===\n",
            "Epoch: 50\n",
            "Loss: 2940548956553216.000 | Acc: 18.000% (18/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 50\n",
            "Loss: 5076202298330317.000 | Acc: 10.410% (1041/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 51\n",
            "Loss: 3212624934207488.000 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 51\n",
            "Loss: 5051243267610378.000 | Acc: 10.640% (1064/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 52\n",
            "Loss: 3640866191179776.000 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 52\n",
            "Loss: 4999143938578186.000 | Acc: 10.610% (1061/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 53\n",
            "Loss: 3306740955217920.000 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 53\n",
            "Loss: 4960697694520607.000 | Acc: 10.700% (1070/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 54\n",
            "Loss: 3251283263750144.000 | Acc: 20.000% (20/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 54\n",
            "Loss: 4953975090912952.000 | Acc: 10.610% (1061/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 55\n",
            "Loss: 3469870121353216.000 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 55\n",
            "Loss: 4943260845363692.000 | Acc: 10.500% (1050/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 56\n",
            "Loss: 3226721956397056.000 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 56\n",
            "Loss: 4931775796797768.000 | Acc: 10.570% (1057/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 57\n",
            "Loss: 2993692734390272.000 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 57\n",
            "Loss: 4917637146979860.000 | Acc: 10.530% (1053/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 58\n",
            "Loss: 2914109137879040.000 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 58\n",
            "Loss: 4898319095436411.000 | Acc: 10.570% (1057/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 59\n",
            "Loss: 3462990925922304.000 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 59\n",
            "Loss: 4874107764835615.000 | Acc: 10.530% (1053/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 60\n",
            "Loss: 2546609590108160.000 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 60\n",
            "Loss: 4850093494045245.000 | Acc: 10.560% (1056/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 61\n",
            "Loss: 2805585112203264.000 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 61\n",
            "Loss: 4828339069958226.000 | Acc: 10.560% (1056/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 62\n",
            "Loss: 2788410611728384.000 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 62\n",
            "Loss: 4811782734731018.000 | Acc: 10.480% (1048/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 63\n",
            "Loss: 2847858998902784.000 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 63\n",
            "Loss: 4799998730939924.000 | Acc: 10.510% (1051/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 64\n",
            "Loss: 2677292627132416.000 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 64\n",
            "Loss: 4783023029737226.000 | Acc: 10.480% (1048/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 65\n",
            "Loss: 2487874905702400.000 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 65\n",
            "Loss: 4763281749432074.000 | Acc: 10.480% (1048/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 66\n",
            "Loss: 3705887969509376.000 | Acc: 8.000% (8/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 66\n",
            "Loss: 4738442257047224.000 | Acc: 10.560% (1056/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 67\n",
            "Loss: 4034148696064000.000 | Acc: 6.000% (6/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 67\n",
            "Loss: 4741715771061699.000 | Acc: 10.530% (1053/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 68\n",
            "Loss: 3135769715671040.000 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 68\n",
            "Loss: 4750732614665503.000 | Acc: 10.480% (1048/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 69\n",
            "Loss: 3250656198524928.000 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 69\n",
            "Loss: 4812106485339259.000 | Acc: 10.510% (1051/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 70\n",
            "Loss: 2542113497546752.000 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 70\n",
            "Loss: 4854997246111908.000 | Acc: 10.570% (1057/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 71\n",
            "Loss: 2843174665977856.000 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 71\n",
            "Loss: 4892307813574902.000 | Acc: 10.510% (1051/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 72\n",
            "Loss: 3008254552571904.000 | Acc: 7.000% (7/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 72\n",
            "Loss: 4924322769576919.000 | Acc: 10.520% (1052/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 73\n",
            "Loss: 2757334241640448.000 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 73\n",
            "Loss: 4942436983394796.000 | Acc: 10.560% (1056/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 74\n",
            "Loss: 2820836071309312.000 | Acc: 7.000% (7/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 74\n",
            "Loss: 4954207859349914.000 | Acc: 10.540% (1054/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 75\n",
            "Loss: 2887279819358208.000 | Acc: 13.000% (13/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 75\n",
            "Loss: 4961386877052518.000 | Acc: 10.520% (1052/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 76\n",
            "Loss: 2998185270181888.000 | Acc: 10.000% (10/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 76\n",
            "Loss: 4942461868703744.000 | Acc: 10.530% (1053/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 77\n",
            "Loss: 3592251724791808.000 | Acc: 6.000% (6/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 77\n",
            "Loss: 4897058939952169.000 | Acc: 10.550% (1055/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 78\n",
            "Loss: 2762908370993152.000 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 78\n",
            "Loss: 4850713021602857.000 | Acc: 10.610% (1061/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 79\n",
            "Loss: 3118272287342592.000 | Acc: 5.000% (5/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 79\n",
            "Loss: 4815514508334203.000 | Acc: 10.680% (1068/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9b3ESAtUEOC",
        "colab_type": "text"
      },
      "source": [
        "## Wavelets\n",
        "\n",
        "__Question 9:__ Use a Scattering Transform as an input to a ResNet-like architecture. You can find a baseline here: https://arxiv.org/pdf/1703.08961.pdf.\n",
        "\n",
        "*Hint:* You can use the following package: https://www.kymat.io/\n",
        "\n",
        "\n",
        "__Answer__: In this question, I was inspired by this repo (https://github.com/edouardoyallon/scalingscattering),  I used the same architecture as theirs, and `pyscatwave` for scattering, as I had some problems with Kymatio dependencies. I tried to use the same metodology and hyperparameters (learing rate, decay, batch size, data augumentation, etc) as https://arxiv.org/pdf/1703.08961.pdf in Sec. 4., and I find:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy in the cited paper | source\n",
        "|------|------|------|------|------|------|\n",
        "|   WideResnet  | 70 | 84 % | 30.7 %|38.9 % | https://arxiv.org/pdf/1703.08961|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY1CkcKfbc_I",
        "colab_type": "code",
        "outputId": "f7c994cf-15a9-4df3-bdf3-b4b5773069e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7651
        }
      },
      "source": [
        "# define model\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block,J=2,N=32, k=1,n=2,num_classes=10,use_all=False):\n",
        "        self.inplanes = 16*k\n",
        "        self.ichannels = 16*k\n",
        "        super(ResNet, self).__init__()\n",
        "        self.nspace = N // (2 ** J)\n",
        "        self.nfscat = int(1 + 8 * J + 8 * 8 * J * (J - 1) / 2)\n",
        "        self.bn0 = nn.BatchNorm2d(self.nfscat*3,eps=1e-5,affine=False)\n",
        "        self.conv1 = nn.Conv2d(self.nfscat*3,self.ichannels, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.ichannels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "            \n",
        "        self.layer2 = self._make_layer(block, 32*k, n)\n",
        "        self.layer3 = self._make_layer(block, 64*k, n)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64*k, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                if m.affine:\n",
        "                    m.weight.data.fill_(1)\n",
        "                    m.bias.data.zero_()\n",
        "        self.scat = Scattering(M=32, N=32, J=3, pre_pad=False).cuda()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.scat(x)\n",
        "        x = x.view(x.size(0), 3*self.nfscat, self.nspace, self.nspace)\n",
        "        x = self.bn0(x)\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "            \n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "lr = 0.1\n",
        "batch_size = 32\n",
        "\n",
        "# Data augumentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# Load train\n",
        "trainset = torchvision.datasets.CIFAR10(root='./root', train=True, download=True, transform=transform_train)\n",
        "# Keep 100 examples\n",
        "p = np.random.permutation(len(trainset.data))\n",
        "trainset.data = trainset.data[p][:100,:,:,:]\n",
        "trainset.targets = np.array(trainset.targets)[p][:100]\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load test\n",
        "testset = torchvision.datasets.CIFAR10(root='./root', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load model\n",
        "net = ResNet(BasicBlock,3,32,16,2)\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Train network\n",
        "def train(epoch):\n",
        "  \n",
        "    print('\\n===Train===\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print( 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n===Test===\\nEpoch: %d' % epoch)\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "start_epoch = 0\n",
        "epochs = 50\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/fftpack/basic.py:160: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  z[index] = x\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Train===\n",
            "Epoch: 0\n",
            "0-96-8-48-48-periodize.cu\n",
            "modulus.cu\n",
            "0-96-2-48-48-periodize.cu\n",
            "0-96-4-24-24-periodize.cu\n",
            "0-96-4-48-48-periodize.cu\n",
            "0-96-2-12-12-periodize.cu\n",
            "0-96-2-24-24-periodize.cu\n",
            "0-12-8-48-48-periodize.cu\n",
            "0-12-2-48-48-periodize.cu\n",
            "0-12-4-24-24-periodize.cu\n",
            "0-12-4-48-48-periodize.cu\n",
            "0-12-2-12-12-periodize.cu\n",
            "0-12-2-24-24-periodize.cu\n",
            "Loss: 10.407 | Acc: 6.000% (6/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 0\n",
            "0-300-8-48-48-periodize.cu\n",
            "0-300-2-48-48-periodize.cu\n",
            "0-300-4-24-24-periodize.cu\n",
            "0-300-4-48-48-periodize.cu\n",
            "0-300-2-12-12-periodize.cu\n",
            "0-300-2-24-24-periodize.cu\n",
            "Loss: 8.062 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 1\n",
            "Loss: 34.892 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 1\n",
            "Loss: 13.021 | Acc: 11.050% (1105/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 2\n",
            "Loss: 16.729 | Acc: 9.000% (9/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 2\n",
            "Loss: 9.596 | Acc: 10.330% (1033/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 3\n",
            "Loss: 25.830 | Acc: 16.000% (16/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 3\n",
            "Loss: 4.580 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 4\n",
            "Loss: 5.454 | Acc: 14.000% (14/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 4\n",
            "Loss: 2.335 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 5\n",
            "Loss: 3.309 | Acc: 7.000% (7/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 5\n",
            "Loss: 2.336 | Acc: 13.840% (1384/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 6\n",
            "Loss: 2.226 | Acc: 11.000% (11/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 6\n",
            "Loss: 2.347 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 7\n",
            "Loss: 2.987 | Acc: 19.000% (19/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 7\n",
            "Loss: 2.391 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 8\n",
            "Loss: 2.194 | Acc: 21.000% (21/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 8\n",
            "Loss: 2.362 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 9\n",
            "Loss: 2.195 | Acc: 12.000% (12/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 9\n",
            "Loss: 2.416 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 10\n",
            "Loss: 2.274 | Acc: 20.000% (20/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 10\n",
            "Loss: 2.425 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 11\n",
            "Loss: 2.228 | Acc: 24.000% (24/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 11\n",
            "Loss: 2.350 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 12\n",
            "Loss: 2.110 | Acc: 22.000% (22/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 12\n",
            "Loss: 2.359 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 13\n",
            "Loss: 2.224 | Acc: 15.000% (15/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 13\n",
            "Loss: 2.475 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 14\n",
            "Loss: 2.090 | Acc: 23.000% (23/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 14\n",
            "Loss: 2.478 | Acc: 10.000% (1000/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 15\n",
            "Loss: 2.082 | Acc: 26.000% (26/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 15\n",
            "Loss: 2.456 | Acc: 9.990% (999/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 16\n",
            "Loss: 1.975 | Acc: 29.000% (29/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 16\n",
            "Loss: 2.460 | Acc: 9.950% (995/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 17\n",
            "Loss: 2.149 | Acc: 21.000% (21/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 17\n",
            "Loss: 2.485 | Acc: 9.990% (999/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 18\n",
            "Loss: 2.271 | Acc: 29.000% (29/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 18\n",
            "Loss: 2.349 | Acc: 12.180% (1218/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 19\n",
            "Loss: 2.131 | Acc: 21.000% (21/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 19\n",
            "Loss: 2.319 | Acc: 15.290% (1529/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 20\n",
            "Loss: 2.131 | Acc: 18.000% (18/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 20\n",
            "Loss: 2.287 | Acc: 17.140% (1714/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 21\n",
            "Loss: 1.983 | Acc: 19.000% (19/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 21\n",
            "Loss: 2.339 | Acc: 12.690% (1269/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 22\n",
            "Loss: 1.946 | Acc: 23.000% (23/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 22\n",
            "Loss: 2.437 | Acc: 13.680% (1368/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 23\n",
            "Loss: 2.001 | Acc: 24.000% (24/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 23\n",
            "Loss: 2.464 | Acc: 14.250% (1425/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 24\n",
            "Loss: 2.093 | Acc: 32.000% (32/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 24\n",
            "Loss: 2.355 | Acc: 13.710% (1371/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 25\n",
            "Loss: 2.003 | Acc: 28.000% (28/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 25\n",
            "Loss: 2.353 | Acc: 14.060% (1406/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 26\n",
            "Loss: 2.210 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 26\n",
            "Loss: 2.434 | Acc: 13.700% (1370/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 27\n",
            "Loss: 2.036 | Acc: 26.000% (26/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 27\n",
            "Loss: 2.495 | Acc: 14.760% (1476/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 28\n",
            "Loss: 2.204 | Acc: 26.000% (26/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 28\n",
            "Loss: 2.472 | Acc: 15.200% (1520/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 29\n",
            "Loss: 2.044 | Acc: 29.000% (29/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 29\n",
            "Loss: 2.438 | Acc: 16.160% (1616/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 30\n",
            "Loss: 2.081 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 30\n",
            "Loss: 2.394 | Acc: 17.260% (1726/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 31\n",
            "Loss: 2.114 | Acc: 27.000% (27/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 31\n",
            "Loss: 2.404 | Acc: 17.130% (1713/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 32\n",
            "Loss: 2.052 | Acc: 24.000% (24/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 32\n",
            "Loss: 2.397 | Acc: 14.870% (1487/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 33\n",
            "Loss: 1.887 | Acc: 31.000% (31/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 33\n",
            "Loss: 2.408 | Acc: 15.010% (1501/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 34\n",
            "Loss: 2.043 | Acc: 31.000% (31/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 34\n",
            "Loss: 2.453 | Acc: 14.410% (1441/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 35\n",
            "Loss: 2.037 | Acc: 23.000% (23/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 35\n",
            "Loss: 2.456 | Acc: 15.160% (1516/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 36\n",
            "Loss: 1.932 | Acc: 27.000% (27/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 36\n",
            "Loss: 2.392 | Acc: 17.310% (1731/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 37\n",
            "Loss: 2.012 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 37\n",
            "Loss: 2.447 | Acc: 16.240% (1624/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 38\n",
            "Loss: 2.115 | Acc: 28.000% (28/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 38\n",
            "Loss: 2.485 | Acc: 15.910% (1591/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 39\n",
            "Loss: 2.397 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 39\n",
            "Loss: 2.450 | Acc: 16.090% (1609/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 40\n",
            "Loss: 2.055 | Acc: 31.000% (31/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 40\n",
            "Loss: 2.212 | Acc: 17.240% (1724/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 41\n",
            "Loss: 2.056 | Acc: 26.000% (26/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 41\n",
            "Loss: 2.338 | Acc: 15.460% (1546/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 42\n",
            "Loss: 1.836 | Acc: 28.000% (28/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 42\n",
            "Loss: 2.780 | Acc: 13.790% (1379/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 43\n",
            "Loss: 2.461 | Acc: 26.000% (26/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 43\n",
            "Loss: 2.533 | Acc: 13.800% (1380/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 44\n",
            "Loss: 1.917 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 44\n",
            "Loss: 2.234 | Acc: 17.310% (1731/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 45\n",
            "Loss: 1.873 | Acc: 31.000% (31/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 45\n",
            "Loss: 2.258 | Acc: 17.100% (1710/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 46\n",
            "Loss: 2.154 | Acc: 29.000% (29/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 46\n",
            "Loss: 2.343 | Acc: 17.000% (1700/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 47\n",
            "Loss: 2.146 | Acc: 25.000% (25/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 47\n",
            "Loss: 2.385 | Acc: 16.900% (1690/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 48\n",
            "Loss: 2.022 | Acc: 30.000% (30/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 48\n",
            "Loss: 2.258 | Acc: 19.010% (1901/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 49\n",
            "Loss: 1.914 | Acc: 27.000% (27/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 49\n",
            "Loss: 2.236 | Acc: 19.510% (1951/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQCDysaTyW1T",
        "colab_type": "code",
        "outputId": "53e18325-e740-4e3c-b200-3227e3e2ac91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2897
        }
      },
      "source": [
        "lr = 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "start_epoch = 50\n",
        "epochs = 20\n",
        "for epoch in range(start_epoch, start_epoch+epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Train===\n",
            "Epoch: 50\n",
            "Loss: 0.742 | Acc: 83.000% (83/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 50\n",
            "Loss: 3.488 | Acc: 30.320% (3032/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 51\n",
            "Loss: 0.515 | Acc: 82.000% (82/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 51\n",
            "Loss: 3.293 | Acc: 30.710% (3071/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 52\n",
            "Loss: 0.814 | Acc: 81.000% (81/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 52\n",
            "Loss: 3.446 | Acc: 30.300% (3030/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 53\n",
            "Loss: 1.103 | Acc: 80.000% (80/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 53\n",
            "Loss: 3.466 | Acc: 30.110% (3011/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 54\n",
            "Loss: 0.656 | Acc: 87.000% (87/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 54\n",
            "Loss: 3.433 | Acc: 29.930% (2993/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 55\n",
            "Loss: 0.553 | Acc: 87.000% (87/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 55\n",
            "Loss: 3.496 | Acc: 29.860% (2986/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 56\n",
            "Loss: 0.957 | Acc: 82.000% (82/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 56\n",
            "Loss: 3.663 | Acc: 29.590% (2959/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 57\n",
            "Loss: 0.712 | Acc: 84.000% (84/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 57\n",
            "Loss: 3.690 | Acc: 29.720% (2972/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 58\n",
            "Loss: 0.568 | Acc: 87.000% (87/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 58\n",
            "Loss: 3.527 | Acc: 29.990% (2999/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 59\n",
            "Loss: 0.495 | Acc: 88.000% (88/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 59\n",
            "Loss: 3.598 | Acc: 30.250% (3025/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 60\n",
            "Loss: 0.766 | Acc: 85.000% (85/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 60\n",
            "Loss: 3.649 | Acc: 29.680% (2968/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 61\n",
            "Loss: 0.583 | Acc: 81.000% (81/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 61\n",
            "Loss: 3.701 | Acc: 29.500% (2950/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 62\n",
            "Loss: 0.545 | Acc: 88.000% (88/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 62\n",
            "Loss: 3.733 | Acc: 29.830% (2983/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 63\n",
            "Loss: 0.651 | Acc: 84.000% (84/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 63\n",
            "Loss: 3.466 | Acc: 30.120% (3012/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 64\n",
            "Loss: 0.426 | Acc: 88.000% (88/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 64\n",
            "Loss: 3.537 | Acc: 30.160% (3016/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 65\n",
            "Loss: 0.684 | Acc: 83.000% (83/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 65\n",
            "Loss: 3.501 | Acc: 30.070% (3007/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 66\n",
            "Loss: 0.726 | Acc: 80.000% (80/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 66\n",
            "Loss: 3.476 | Acc: 29.760% (2976/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 67\n",
            "Loss: 0.889 | Acc: 78.000% (78/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 67\n",
            "Loss: 3.501 | Acc: 29.560% (2956/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 68\n",
            "Loss: 1.044 | Acc: 82.000% (82/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 68\n",
            "Loss: 3.782 | Acc: 29.570% (2957/10000)\n",
            "\n",
            "===Train===\n",
            "Epoch: 69\n",
            "Loss: 0.664 | Acc: 85.000% (85/100)\n",
            "\n",
            "===Test===\n",
            "Epoch: 69\n",
            "Loss: 3.805 | Acc: 30.110% (3011/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOoFxe8GUEOD",
        "colab_type": "text"
      },
      "source": [
        "# Weak supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t6yU2AgUEOD",
        "colab_type": "text"
      },
      "source": [
        "Weakly supervised techniques permit to tackle the issue of labeled data. An introduction to those techniques can be found here: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html.\n",
        "\n",
        "__(Open) Question 10:__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort.\n",
        "\n",
        "__Answer__: I tried in this question to combine the labelled dataset, with the predictions of the transfer learning network on the unlabelled dataset, and train a network with that using the method mentioned in this paper https://arxiv.org/pdf/1706.09451.pdf, but I couldn't finish my approach until the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKvgDmdQUEOE",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVQPCBPUEOE",
        "colab_type": "text"
      },
      "source": [
        "__Question 11:__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8zFJ36pUEOF",
        "colab_type": "text"
      },
      "source": [
        "A summary of the results obtained is shown in the following figure (see below).\n",
        "\n",
        "We can see that the transfer learning gave the best results (except maybe weak supervision). This result is not surprising, for the reasons that will be mentionning later. We're now going to discuss the pros and cons of each method.\n",
        "\n",
        "\n",
        "\n",
        "* **Training state of the art classification networks from scratch ** :  In this method, several state of the art networks  in the field of image classification have been trained from scratch. ResNet and VGG were particularly tested. With the exception of ResNet18, the other architectures gave poor results. This result is quite normal, since we want to train models with millions of parameters with only 100 images, which will inevitably lead to overfitting. The superiority of ResNet18 can be explained by the fact that its architecture is better suited for image classification than VGG and it has fewer parameters to train than ResNet50. Personally, I don't see any pros of this method.\n",
        "* **Data augmentation**: In this method, we used the same architectures as the previous methods, but with more data augmentation. This method has given better results than the previous one, which is expected, because the network sees a lot of image, which is considered as a form of regularization, and therefore it allows to minimize the overfeating effect. This method is to be tried when we can't use the transfer learning or DCGAN features, i. e. when we don't have a pre-trainined network in a domain close to ours.\n",
        "\n",
        "* **Transfert learning**: This method gave the best result, similar to the result obtained by using scattering in https://arxiv.org/pdf/1703.08961. This result is predictable because the transfer learning framework is specially designed for the case of small data, and we have been able to prove that using it is always better than training from scratch even if we have a large amount of labelled data. Indeed, if the network used is pre-trained on images similar to ours (for example CIFAR and ImageNet), then its different layers are able to extract features very relevant to our task, and it is enough to adapt a little the last layers to classify our images, as the first layers extract low level features, which do not change much from one dataset to another.  This method is the first to be considered if we have a pre-trainined network on images similar to ours. \n",
        "\n",
        "* **DCGan features**: This method has given the second best results, and we believe that we can have a better result if we have a better trained discriminator, and it is trained to discriminate images similar to ours (for example ImageNet instead of CelebA). The downsides of this method, as for transfer learning, is that we must have a pre-trained network on a dataset similar to ours, but the pros are that when the dicriminator is well trained, the representation obtained is very discriminate, and we can use classic classifiers that do not require a large training dataset.\n",
        "\n",
        "* **Wavelets**: In the paper mentioned above, this method gave a score equal to 38%. The pros of this method is that it allows to use state of the art architectures, but without the first layers which are replaced by wavelets adapted to the extraction of low level features, which reduces the number of parameters to estimate, and subsequently the size of the dataset needed. I think combining this method with transfer learning, i.e. using a pre-trained network, and replacing the first layers with wavelets can give a better result than both methods, but I couldn't test that.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vkeH2lWEOCr",
        "colab_type": "code",
        "outputId": "4087d473-0206-4c72-c41d-bd578c14a23d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "names = [\"ResNet18 scatch\", \"ResNet18 augmentation\", \"ResNet50 scatch\", \"ResNet50 augmentation\", \"VGG16 scatch\", \"VGG16 augmentation\", \"VGG19 scatch\", \"VGG19 augmentation\", \"Transfert Learning\", \"DCGan features\", \"Wavelets\"]\n",
        "scores = [27, 27, 14, 19, 10, 15, 10, 11.6, 37, 28.38,30.7]\n",
        "\n",
        "y_pos = np.arange(len(names))\n",
        " \n",
        "plt.barh(y_pos, scores, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, names)\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Comparaison of different methods for small data classification on CIFAR10 dataset')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAEWCAYAAAB47K3ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncknfPwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2WcKQiQGhH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}